--
-- PostgreSQL database cluster dump
--

-- Started on 2025-10-28 05:44:41

\restrict srdVKAAU8pnj5ta4GZ1SzGmWBrS2HRRKTahWXxY2uu3NmUoxMYi2HLniOvXkV0k

SET default_transaction_read_only = off;

SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;

--
-- Roles
--

CREATE ROLE ericbo;
ALTER ROLE ericbo WITH SUPERUSER INHERIT CREATEROLE CREATEDB LOGIN REPLICATION BYPASSRLS PASSWORD 'SCRAM-SHA-256$4096:/4cIIE69yACu4hfkLWgbeg==$7/BwPDiBI5yBoKdwJVXAH1Mu42TAV+W2IhvbpHPvajI=:qnQppCQVEK7qS0xLi4at/00sRQFNC7NWFA5Fg6MHb5I=' VALID UNTIL '2026-04-30 00:00:00-06';
CREATE ROLE postgres;
ALTER ROLE postgres WITH SUPERUSER INHERIT CREATEROLE CREATEDB LOGIN REPLICATION BYPASSRLS PASSWORD 'SCRAM-SHA-256$4096:dy+gcGa+AtI91KQVTI0vyQ==$lz8sVyGsd1SXD7UfaI/DinYQR/nJhyM3UER+/e9RUpg=:SOZKMj4RdTi2vDvgtrteT+W6Oo59POVQis4dC5Apx9k=';

--
-- User Configurations
--


--
-- Role memberships
--

GRANT ericbo TO pg_create_subscription WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;
GRANT ericbo TO pg_execute_server_program WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;
GRANT ericbo TO pg_maintain WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;
GRANT ericbo TO pg_read_all_data WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;
GRANT ericbo TO pg_write_all_data WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;
GRANT postgres TO ericbo WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;
GRANT postgres TO pg_execute_server_program WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;
GRANT postgres TO pg_read_all_data WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;
GRANT postgres TO pg_write_all_data WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;
GRANT postgres TO pg_write_server_files WITH ADMIN OPTION, INHERIT TRUE GRANTED BY postgres;






\unrestrict srdVKAAU8pnj5ta4GZ1SzGmWBrS2HRRKTahWXxY2uu3NmUoxMYi2HLniOvXkV0k

--
-- Databases
--

--
-- Database "template1" dump
--

\connect template1

--
-- PostgreSQL database dump
--

\restrict Yuyhb1obKtJHFN7AgulMqlXy2nTEEUwqzJr9aDf3d6Jx5vynLoVqvAhclAcFepj

-- Dumped from database version 18.0
-- Dumped by pg_dump version 18.0

-- Started on 2025-10-28 05:44:41

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET transaction_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

-- Completed on 2025-10-28 05:44:41

--
-- PostgreSQL database dump complete
--

\unrestrict Yuyhb1obKtJHFN7AgulMqlXy2nTEEUwqzJr9aDf3d6Jx5vynLoVqvAhclAcFepj

--
-- Database "postgres" dump
--

\connect postgres

--
-- PostgreSQL database dump
--

\restrict xcjnimaxyjShnqYYGRuKudhxf3MbLq7fJYfi9a2FWqfQ66mSAMQhYndvdWmpHtD

-- Dumped from database version 18.0
-- Dumped by pg_dump version 18.0

-- Started on 2025-10-28 05:44:41

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET transaction_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- TOC entry 6 (class 2615 OID 24576)
-- Name: prepper; Type: SCHEMA; Schema: -; Owner: postgres
--

CREATE SCHEMA prepper;


ALTER SCHEMA prepper OWNER TO postgres;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- TOC entry 224 (class 1259 OID 24625)
-- Name: aws_certified_architect_associate_questions; Type: TABLE; Schema: prepper; Owner: ericbo
--

CREATE TABLE prepper.aws_certified_architect_associate_questions (
    id integer NOT NULL,
    question_id integer CONSTRAINT aws_certified_architect_associate_question_question_id_not_null NOT NULL,
    question_number integer CONSTRAINT aws_certified_architect_associate_ques_question_number_not_null NOT NULL,
    category text,
    difficulty text,
    domain text,
    question_text text CONSTRAINT aws_certified_architect_associate_questi_question_text_not_null NOT NULL,
    options jsonb NOT NULL,
    correct_answer text CONSTRAINT aws_certified_architect_associate_quest_correct_answer_not_null NOT NULL,
    explanation text,
    explanation_details jsonb,
    multiple_answers boolean
);


ALTER TABLE prepper.aws_certified_architect_associate_questions OWNER TO ericbo;

--
-- TOC entry 227 (class 1259 OID 40963)
-- Name: aws_certified_architect_associate_questions_id_seq; Type: SEQUENCE; Schema: prepper; Owner: ericbo
--

ALTER TABLE prepper.aws_certified_architect_associate_questions ALTER COLUMN id ADD GENERATED ALWAYS AS IDENTITY (
    SEQUENCE NAME prepper.aws_certified_architect_associate_questions_id_seq
    START WITH 95
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1
);


--
-- TOC entry 225 (class 1259 OID 40961)
-- Name: aws_question_id_seq; Type: SEQUENCE; Schema: prepper; Owner: ericbo
--

CREATE SEQUENCE prepper.aws_question_id_seq
    START WITH 194
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE prepper.aws_question_id_seq OWNER TO ericbo;

--
-- TOC entry 226 (class 1259 OID 40962)
-- Name: aws_question_number_seq; Type: SEQUENCE; Schema: prepper; Owner: ericbo
--

CREATE SEQUENCE prepper.aws_question_number_seq
    START WITH 94
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE prepper.aws_question_number_seq OWNER TO ericbo;

--
-- TOC entry 221 (class 1259 OID 24591)
-- Name: comptia_cloud_plus_questions; Type: TABLE; Schema: prepper; Owner: ericbo
--

CREATE TABLE prepper.comptia_cloud_plus_questions (
    id integer NOT NULL,
    question_id integer NOT NULL,
    question_number integer NOT NULL,
    category text,
    difficulty text,
    domain text,
    question_text text NOT NULL,
    options jsonb NOT NULL,
    correct_answer text NOT NULL,
    explanation text,
    explanation_details jsonb,
    multiple_answers bit(1),
    correct_answers text[]
);


ALTER TABLE prepper.comptia_cloud_plus_questions OWNER TO ericbo;

--
-- TOC entry 220 (class 1259 OID 24590)
-- Name: id_seq; Type: SEQUENCE; Schema: prepper; Owner: ericbo
--

ALTER TABLE prepper.comptia_cloud_plus_questions ALTER COLUMN id ADD GENERATED ALWAYS AS IDENTITY (
    SEQUENCE NAME prepper.id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1
);


--
-- TOC entry 222 (class 1259 OID 24612)
-- Name: question_id_seq; Type: SEQUENCE; Schema: prepper; Owner: ericbo
--

CREATE SEQUENCE prepper.question_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    MAXVALUE 2147483647
    CACHE 1;


ALTER SEQUENCE prepper.question_id_seq OWNER TO ericbo;

--
-- TOC entry 223 (class 1259 OID 24616)
-- Name: question_number_seq; Type: SEQUENCE; Schema: prepper; Owner: postgres
--

CREATE SEQUENCE prepper.question_number_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    MAXVALUE 2147483647
    CACHE 1;


ALTER SEQUENCE prepper.question_number_seq OWNER TO postgres;

--
-- TOC entry 5024 (class 0 OID 24625)
-- Dependencies: 224
-- Data for Name: aws_certified_architect_associate_questions; Type: TABLE DATA; Schema: prepper; Owner: ericbo
--

COPY prepper.aws_certified_architect_associate_questions (id, question_id, question_number, category, difficulty, domain, question_text, options, correct_answer, explanation, explanation_details, multiple_answers) FROM stdin;
72	172	72	Performance - Storage Optimization	Advanced	Design High-Performing Architectures	A high-frequency trading application requires consistent sub-millisecond latency for database operations. The application processes millions of transactions per second with frequent random read/write patterns. Which storage solution provides optimal performance?	[{"text": "EBS gp3 volumes with baseline performance", "isCorrect": false}, {"text": "EBS io2 volumes with provisioned IOPS", "isCorrect": true}, {"text": "EFS with General Purpose performance mode", "isCorrect": false}, {"text": "S3 with Transfer Acceleration", "isCorrect": false}]	EBS io2 volumes with provisioned IOPS	EBS io2 provides consistent high IOPS performance with sub-millisecond latency, ideal for high-frequency trading applications requiring predictable performance.	{"summary": "High-performance storage requirements:", "breakdown": ["io2: Up to 64,000 IOPS per volume", "Consistent performance: no performance variability", "Sub-millisecond latency: ideal for trading systems", "Provisioned IOPS: guaranteed performance levels"], "otherOptions": "gp3 has variable performance and IOPS limits\\nEFS has higher latency than block storage\\nS3 is object storage, not suitable for databases"}	\N
8	108	8	AWS Security - IAM Policies	Expert	Design Secure Applications	A company has multiple AWS accounts and needs to grant developers access to specific S3 buckets based on their team assignment. Developers should only access buckets prefixed with their team name (e.g., team-alpha-*). Which approach provides the MOST secure and scalable solution?	[{"text": "Create individual IAM users in each account with specific S3 bucket policies", "isCorrect": false}, {"text": "Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions", "isCorrect": true}, {"text": "Create shared IAM users with different access keys for each team", "isCorrect": false}, {"text": "Use S3 bucket policies with IAM user conditions for each team", "isCorrect": false}]	Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions	Organizations with SCPs provide account-level governance, while cross-account roles with dynamic conditions enable secure, scalable team-based access.	{"summary": "Multi-account secure access strategy:", "breakdown": ["SCPs provide organization-wide security boundaries", "Cross-account roles eliminate need for multiple IAM users", "Dynamic conditions based on user attributes (team tags)", "Centralized access management across accounts"], "otherOptions": "Individual users don't scale across multiple accounts\\nShared users violate security best practices\\nBucket policies alone can't handle multi-account scenarios efficiently"}	\N
9	109	9	AWS Security - Data Encryption	Application	Design Secure Applications	A healthcare company must encrypt all data at rest and in transit. They need to maintain full control over encryption keys and meet HIPAA compliance requirements. The solution must integrate with RDS, S3, and EBS. Which key management approach is MOST appropriate?	[{"text": "Use AWS managed keys (SSE-S3, default RDS encryption)", "isCorrect": false}, {"text": "Use AWS KMS customer managed keys with CloudTrail logging", "isCorrect": true}, {"text": "Use client-side encryption with application-managed keys", "isCorrect": false}, {"text": "Use AWS KMS AWS managed keys with detailed monitoring", "isCorrect": false}]	Use AWS KMS customer managed keys with CloudTrail logging	Customer managed KMS keys provide full control over key policies, rotation, and access, while CloudTrail provides complete audit trails required for HIPAA compliance.	{"summary": "HIPAA-compliant key management requirements:", "breakdown": ["Customer managed keys: Full control over key policies and access", "CloudTrail: Complete audit trail of key usage", "Cross-service integration: Works with RDS, S3, EBS", "HIPAA compliance: Meets regulatory requirements for key control"], "otherOptions": "AWS managed keys don't provide sufficient control for HIPAA\\nClient-side encryption complex and doesn't integrate well\\nAWS managed keys still don't give customer full control"}	\N
11	111	11	AWS Security - Advanced Threats	Expert	Design Secure Applications	A company's web application experiences DDoS attacks and SQL injection attempts. They need comprehensive protection that automatically adapts to new attack patterns. The solution must integrate with CloudFront and ALB. Which combination provides the MOST comprehensive protection?	[{"text": "AWS Shield Standard + CloudFront + Security Groups", "isCorrect": false}, {"text": "AWS Shield Advanced + AWS WAF + AWS Firewall Manager", "isCorrect": true}, {"text": "AWS Shield Standard + AWS WAF + VPC Flow Logs", "isCorrect": false}, {"text": "Network ACLs + AWS Inspector + CloudWatch", "isCorrect": false}]	AWS Shield Advanced + AWS WAF + AWS Firewall Manager	Shield Advanced provides enhanced DDoS protection, WAF blocks application-layer attacks, and Firewall Manager centrally manages security policies across resources.	{"summary": "Comprehensive web application protection stack:", "breakdown": ["Shield Advanced: Enhanced DDoS protection with attack response team", "WAF: Blocks SQL injection, XSS, and other application attacks", "Firewall Manager: Centralized security policy management", "Auto-adaptation: Machine learning capabilities for new threats"], "otherOptions": "Shield Standard insufficient for advanced DDoS\\nVPC Flow Logs don't provide active protection\\nInspector scans instances, doesn't provide runtime protection"}	\N
18	118	18	AWS Security - Secrets Management	Application	Design Secure Applications	A microservices application needs to securely store and rotate database passwords, API keys, and certificates. The solution must integrate with RDS automatic rotation and provide audit trails. Which service combination is MOST appropriate?	[{"text": "Store secrets in S3 with bucket encryption and versioning", "isCorrect": false}, {"text": "Use AWS Secrets Manager with CloudTrail logging", "isCorrect": true}, {"text": "Store secrets in Systems Manager Parameter Store with SecureString", "isCorrect": false}, {"text": "Use AWS KMS to encrypt secrets stored in DynamoDB", "isCorrect": false}]	Use AWS Secrets Manager with CloudTrail logging	Secrets Manager provides automatic rotation for RDS, integration with AWS services, and complete audit trails through CloudTrail.	{"summary": "Secrets Manager comprehensive benefits:", "breakdown": ["Automatic rotation for RDS, Redshift, DocumentDB", "Native integration with AWS services", "Complete audit trail through CloudTrail", "Secure retrieval with fine-grained IAM permissions"], "otherOptions": "S3 not designed for secrets management\\nParameter Store lacks automatic rotation for databases\\nCustom solution requires building rotation logic"}	\N
24	124	24	AWS Security - ML Data Protection	Application	Design Secure Applications	A healthcare company uses Amazon Comprehend Medical to analyze patient records and Amazon Textract for insurance form processing. They must ensure HIPAA compliance and data isolation. Which security configuration is MOST appropriate?	[{"text": "Use default service encryption and enable CloudTrail logging", "isCorrect": false}, {"text": "Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service", "isCorrect": true}, {"text": "Process all data in a single private subnet with network ACLs", "isCorrect": false}, {"text": "Use AWS managed keys and restrict access with S3 bucket policies", "isCorrect": false}]	Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service	VPC endpoints ensure private connectivity, customer-managed KMS keys provide encryption control, and separate IAM roles enable least privilege access for HIPAA compliance.	{"summary": "HIPAA-compliant ML service configuration:", "breakdown": ["VPC endpoints: Keep data traffic within AWS network", "Customer-managed KMS: Full control over encryption keys", "Separate IAM roles: Least privilege per service", "Audit trail: CloudTrail logs all API calls for compliance"], "otherOptions": "Default encryption insufficient for HIPAA requirements\\nNetwork ACLs don't provide service-level isolation\\nAWS managed keys don't provide sufficient control"}	\N
25	125	25	AWS Security - Container Security	Expert	Design Secure Applications	A fintech application runs on EKS with sensitive payment processing workloads. The security team requires pod-level network isolation, secrets rotation every 30 days, and runtime threat detection. Which solution meets ALL requirements?	[{"text": "Calico network policies, AWS Secrets Manager, and Amazon Inspector", "isCorrect": false}, {"text": "AWS App Mesh, Systems Manager Parameter Store, and CloudWatch Container Insights", "isCorrect": false}, {"text": "EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection", "isCorrect": true}, {"text": "Network ACLs, HashiCorp Vault, and AWS Config rules", "isCorrect": false}]	EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection	EKS security groups provide pod-level isolation, Secrets Manager handles automatic rotation, and GuardDuty EKS protection offers runtime threat detection.	{"summary": "Comprehensive EKS security architecture:", "breakdown": ["Security groups for pods: Fine-grained network isolation at pod level", "Secrets Manager: Automatic 30-day rotation with EKS integration", "GuardDuty EKS: Runtime threat detection and anomaly detection", "Native AWS integration: Simplified management and compliance"], "otherOptions": "Inspector scans vulnerabilities, not runtime threats\\nApp Mesh is service mesh, not security focused\\nNetwork ACLs too coarse for pod-level control"}	\N
33	133	33	AWS ML Services Integration	Application	Design Secure Applications	A social media platform needs to detect inappropriate content in user posts (text and images) in real-time. They process 1 million posts daily and must comply with COPPA regulations for users under 13. Which combination of AWS services provides comprehensive content moderation with compliance controls?	[{"text": "Amazon Comprehend for text and custom SageMaker model for images", "isCorrect": false}, {"text": "Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic", "isCorrect": true}, {"text": "Amazon Textract for text extraction and Amazon Personalize for content filtering", "isCorrect": false}, {"text": "AWS Glue for data preparation and Amazon Forecast for predicting inappropriate content", "isCorrect": false}]	Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic	Rekognition provides pre-trained image moderation, Comprehend detects toxic text, and Lambda implements age-appropriate filtering logic for COPPA compliance.	{"summary": "Content moderation architecture components:", "breakdown": ["Rekognition: Detects inappropriate images (violence, adult content)", "Comprehend: Identifies toxic text, PII, and sentiment", "Lambda age-gating: Applies stricter rules for users under 13", "Real-time processing: Sub-second moderation decisions"], "otherOptions": "Custom models require training data and maintenance\\nTextract is for document processing, not moderation\\nForecast predicts time-series data, not content issues"}	\N
73	173	73	Performance - Global Content Delivery	Intermediate	Design High-Performing Architectures	A media streaming service serves video content globally but experiences slow loading for users in Asia-Pacific. Content is stored in S3 in us-east-1. Which solution provides the BEST performance improvement?	[{"text": "Enable S3 Transfer Acceleration globally", "isCorrect": false}, {"text": "Deploy CloudFront distribution with regional edge caches", "isCorrect": true}, {"text": "Replicate S3 buckets to all AWS regions", "isCorrect": false}, {"text": "Use ElastiCache clusters in each target region", "isCorrect": false}]	Deploy CloudFront distribution with regional edge caches	CloudFront edge locations provide low-latency content delivery globally, with regional edge caches reducing origin load for popular content.	{"summary": "Global content delivery optimization:", "breakdown": ["Edge locations: 400+ global points of presence", "Regional edge caches: reduce origin load", "Automatic content caching: faster subsequent requests", "Optimized for media streaming workloads"], "otherOptions": "Transfer Acceleration helps uploads, not downloads\\nMulti-region replication is complex and expensive\\nElastiCache doesn't help with media streaming"}	\N
37	137	37	AWS Security - Zero Trust Architecture	Expert	Design Secure Applications	A financial services company needs to implement zero-trust access to their AWS environment for 500 developers across 50 teams. Requirements include: no long-lived credentials, audit trail of all actions, team-based access boundaries, and integration with existing Okta SSO. Which solution provides the most comprehensive zero-trust implementation?	[{"text": "IAM users with MFA and IP restrictions for each developer", "isCorrect": false}, {"text": "AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules", "isCorrect": true}, {"text": "Cognito user pools with custom authorizers", "isCorrect": false}, {"text": "IAM roles with external ID and session policies", "isCorrect": false}]	AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules	AWS SSO provides temporary credentials, Okta integration enables SAML-based authentication, Permission Sets with session tags enforce team boundaries, while CloudTrail ensures complete audit trails.	{"summary": "Zero-trust implementation components:", "breakdown": ["AWS SSO: No long-lived credentials, automatic rotation", "Permission Sets: Team-based access with session tags", "Okta SAML: Leverages existing identity provider", "CloudTrail + EventBridge: Real-time audit and alerting"], "otherOptions": "IAM users require long-lived access keys\\nCognito designed for application users, not AWS access\\nMissing centralized management for 500 developers"}	\N
41	141	41	AWS Storage Concepts	Knowledge	Design High-Performing Architectures	A company needs to choose between different AWS storage types for various workloads. Which statement BEST describes the appropriate use cases for each storage type?	[{"text": "Use object storage for databases, block storage for file shares, file storage for web content", "isCorrect": false}, {"text": "Use block storage for databases, file storage for shared access, object storage for web content", "isCorrect": true}, {"text": "Use file storage for databases, object storage for shared access, block storage for web content", "isCorrect": false}, {"text": "All storage types can be used interchangeably for any workload", "isCorrect": false}]	Use block storage for databases, file storage for shared access, object storage for web content	Block storage (EBS) provides high IOPS for databases, file storage (EFS) enables shared access, object storage (S3) scales for web content.	{"summary": "AWS storage type characteristics:", "breakdown": ["Block storage (EBS): High IOPS, low latency, ideal for databases and boot volumes", "File storage (EFS): POSIX-compliant, shared access, good for content repositories", "Object storage (S3): Auto-scaling, REST API, perfect for web content and backups", "Each type optimized for specific access patterns and performance requirements"], "otherOptions": "Object storage not suitable for database IOPS requirements\\nFile storage lacks the IOPS performance needed for databases\\nEach storage type has specific use cases and limitations"}	\N
42	142	42	AWS Storage Concepts	Application	Design Resilient Architectures	An enterprise is migrating from on-premises and needs to maintain compatibility with existing storage protocols. They use NFS for Unix systems, SMB for Windows, and iSCSI for SAN storage. Which AWS services provide the BEST protocol compatibility?	[{"text": "S3 for all protocols using third-party gateways", "isCorrect": false}, {"text": "EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI", "isCorrect": true}, {"text": "EBS for all protocols using EC2 instance configuration", "isCorrect": false}, {"text": "DataSync for protocol translation across all storage types", "isCorrect": false}]	EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI	AWS provides native protocol support: EFS (NFS), FSx for Windows (SMB), Storage Gateway (iSCSI) for seamless migration.	{"summary": "Native AWS protocol support:", "breakdown": ["EFS: Native NFS v4.1, POSIX-compliant, Linux/Unix compatibility", "FSx for Windows: Native SMB, Active Directory integration", "Storage Gateway: iSCSI interface, hybrid cloud connectivity", "No application changes required for protocol compatibility"], "otherOptions": "S3 doesn't natively support NFS/SMB/iSCSI protocols\\nEBS provides block storage but not protocol-level compatibility\\nDataSync is for data transfer, not protocol translation"}	\N
46	146	46	AWS EBS Deep Dive	Application	Design High-Performing Architectures	A database application requires consistent 20,000 IOPS with low latency for transaction processing. The database size is 8TB and growing. Which EBS volume type and configuration provides the BEST performance while supporting future growth?	[{"text": "gp3 volumes with 20,000 provisioned IOPS", "isCorrect": false}, {"text": "io2 Block Express volumes with 20,000 provisioned IOPS", "isCorrect": true}, {"text": "gp2 volumes in RAID 0 configuration", "isCorrect": false}, {"text": "st1 throughput optimized volumes for cost savings", "isCorrect": false}]	io2 Block Express volumes with 20,000 provisioned IOPS	io2 Block Express provides consistent IOPS up to 256,000, supports volumes up to 64TB, and offers sub-millisecond latency for databases.	{"summary": "io2 Block Express advantages for databases:", "breakdown": ["Consistent IOPS: 20,000 IOPS guaranteed regardless of volume size", "Sub-millisecond latency: Optimal for transaction processing", "Scalability: Supports up to 64TB volumes for future growth", "Durability: 99.999% annual durability vs 99.9% for gp3"], "otherOptions": "gp3 IOPS can vary with volume size and burst credits\\nRAID 0 increases failure risk and management complexity\\nst1 optimized for throughput, not IOPS-intensive workloads"}	\N
43	143	43	AWS S3 Deep Dive	Application	Design Resilient Architectures	A media company requires 99.999999999% (11 9s) data durability for their video assets. They need to understand the difference between S3 durability and availability for their SLA reporting. Which statement is CORRECT about S3 durability vs availability?	[{"text": "Durability and availability are identical metrics across all S3 storage classes", "isCorrect": false}, {"text": "Durability (11 9s) represents data loss protection, while availability varies by storage class", "isCorrect": true}, {"text": "Availability is always higher than durability in S3", "isCorrect": false}, {"text": "Only S3 Standard provides 11 9s durability", "isCorrect": false}]	Durability (11 9s) represents data loss protection, while availability varies by storage class	S3 provides 11 9s durability (data loss protection) across ALL storage classes, but availability (uptime) varies by class.	{"summary": "S3 durability vs availability:", "breakdown": ["Durability (11 9s): Probability of NOT losing data over a year", "Availability varies: Standard (99.99%), IA (99.9%), Glacier (variable)", "All storage classes: Same durability guarantee", "Durability: Data integrity protection through redundancy"], "otherOptions": "Different metrics with different SLA percentages\\nAvailability percentages are typically lower than durability\\nAll S3 storage classes maintain 11 9s durability"}	\N
49	149	49	AWS File Systems Deep Dive	Application	Design High-Performing Architectures	A video editing company needs shared storage accessible from 50 EC2 instances across multiple AZs for collaborative editing projects. Files range from 1GB to 50GB, and editors require low-latency access. Performance needs vary from 1 GB/s during off-hours to 10 GB/s during production. Which file system solution provides the BEST performance and scalability?	[{"text": "Amazon EFS with General Purpose performance mode", "isCorrect": false}, {"text": "Amazon EFS with Max I/O performance mode and Provisioned Throughput", "isCorrect": true}, {"text": "Amazon FSx for Lustre with scratch file system", "isCorrect": false}, {"text": "Multiple EBS volumes shared using NFS on EC2", "isCorrect": false}]	Amazon EFS with Max I/O performance mode and Provisioned Throughput	EFS Max I/O mode supports higher IOPS for concurrent access, while Provisioned Throughput ensures consistent 10 GB/s performance regardless of file system size.	{"summary": "EFS configuration for video editing workloads:", "breakdown": ["Max I/O mode: Higher IOPS limit for 50 concurrent clients", "Provisioned Throughput: Guarantees 10 GB/s during peak production", "Multi-AZ access: Native support across availability zones", "POSIX compliance: Standard file system semantics for editing software"], "otherOptions": "General Purpose mode has lower IOPS limits for concurrent access\\nLustre scratch file system designed for HPC, not collaborative editing\\nEBS volumes can't be natively shared between instances"}	\N
1	101	1	AWS Architecture - High Availability	Application	Design Resilient Architectures	A company runs a critical e-commerce application on a single EC2 instance in us-east-1a. The application must achieve 99.99% uptime and automatically recover from AZ failures. Which solution provides the MOST cost-effective approach to meet these requirements?	[{"text": "Deploy a larger EC2 instance type in the same AZ", "isCorrect": false}, {"text": "Create an AMI and manually launch instances in other AZs when needed", "isCorrect": false}, {"text": "Use Auto Scaling Group with instances across multiple AZs behind an ALB", "isCorrect": true}, {"text": "Use Elastic Beanstalk with a single instance", "isCorrect": false}]	Use Auto Scaling Group with instances across multiple AZs behind an ALB	Auto Scaling Groups with multiple AZs provide automatic failure detection and recovery, while ALB distributes traffic and handles health checks.	{"summary": "Multi-AZ Auto Scaling provides automated resilience:", "breakdown": ["Auto Scaling Group automatically replaces failed instances", "Multiple AZs protect against entire AZ failures", "ALB provides health checks and traffic distribution", "Most cost-effective as it only runs minimum required instances"], "otherOptions": "Larger instance doesn't solve AZ failure\\nManual process doesn't meet automation requirement\\nSingle instance still has single point of failure"}	\N
48	148	48	AWS EBS Deep Dive	Expert	Design Secure Architectures	A financial application stores sensitive data on EBS volumes and requires encryption in transit and at rest. The application accesses data across multiple AZs and needs to ensure encryption keys remain under customer control with automatic rotation. Which configuration provides comprehensive encryption coverage?	[{"text": "EBS encryption with AWS managed keys and HTTPS application protocols", "isCorrect": false}, {"text": "EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS", "isCorrect": true}, {"text": "Instance store volumes with client-side encryption", "isCorrect": false}, {"text": "EBS encryption with customer-provided keys (SSE-C)", "isCorrect": false}]	EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS	Customer-managed KMS keys provide control with automatic rotation, EBS encryption handles data at rest, and TLS provides encryption in transit.	{"summary": "Comprehensive EBS encryption strategy:", "breakdown": ["Customer-managed KMS keys: Full control over encryption keys", "Automatic rotation: Annual key rotation without service interruption", "EBS encryption: All data at rest encrypted including snapshots", "Application TLS: Encrypts data in transit between instances"], "otherOptions": "AWS managed keys don't provide customer control\\nInstance store data is temporary, not suitable for persistent financial data\\nCustomer-provided keys require manual key management"}	\N
52	152	52	AWS Hybrid Storage Migration	Expert	Design High-Performing Architectures	A media company operates a hybrid architecture where video editors work on-premises but need seamless access to cloud storage for rendering jobs. Local performance must remain unaffected while providing transparent cloud storage scalability. Frequently accessed files should be available locally. Which hybrid storage solution provides the BEST user experience?	[{"text": "AWS Storage Gateway File Gateway with local cache", "isCorrect": true}, {"text": "AWS DataSync scheduled sync between on-premises and S3", "isCorrect": false}, {"text": "Direct Connect with EFS mounted on workstations", "isCorrect": false}, {"text": "S3 bucket mounted using third-party NFS gateway", "isCorrect": false}]	AWS Storage Gateway File Gateway with local cache	Storage Gateway File Gateway provides NFS interface with intelligent local caching, transparent S3 integration, and optimal performance for frequently accessed files.	{"summary": "File Gateway hybrid benefits:", "breakdown": ["Local cache: Frequently accessed files available at local speeds", "Transparent scaling: Files automatically stored in S3", "NFS interface: Standard file system access for editing applications", "Intelligent caching: Recently and frequently used files cached locally"], "otherOptions": "DataSync requires scheduled sync, not transparent access\\nEFS over Direct Connect has latency for large video files\\nThird-party solutions lack AWS service integration and support"}	\N
2	102	2	AWS Architecture - Data Resilience	Application	Design Resilient Architectures	A financial services company stores critical transaction data in RDS MySQL. They need to ensure data can be recovered with RPO of 5 minutes and RTO of 15 minutes in case of primary database failure. Which combination of solutions meets these requirements? (Choose TWO)	[{"text": "Enable automated backups with 5-minute backup frequency", "isCorrect": false}, {"text": "Configure RDS Multi-AZ deployment", "isCorrect": true}, {"text": "Create hourly manual snapshots", "isCorrect": false}, {"text": "Enable point-in-time recovery", "isCorrect": true}]	Configure RDS Multi-AZ deployment	Multi-AZ provides automatic failover within minutes, while point-in-time recovery enables restoration to any second within the backup retention period.	{"summary": "Meeting RPO/RTO requirements:", "breakdown": ["Multi-AZ: Automatic failover typically completes in 60-120 seconds", "Point-in-time recovery: Can restore to any second (meets 5-min RPO)", "Automated backups: Only daily, can't meet 5-minute RPO", "Manual snapshots: Too infrequent for RPO requirement"], "otherOptions": "Automated backups are daily, not every 5 minutes\\nHourly snapshots exceed 5-minute RPO requirement"}	\N
60	160	60	AWS Storage Resiliency Scalability	Application	Design Resilient Architectures	An e-commerce platform experiences seasonal traffic patterns with 50x increase during holiday sales. Their product catalog and user-generated content storage must scale automatically without performance degradation. Current EBS-based storage architecture requires manual intervention during peaks. Which storage architecture provides automatic scalability?	[{"text": "Auto Scaling EBS volumes with CloudWatch metrics", "isCorrect": false}, {"text": "Amazon S3 with CloudFront for content delivery and Lambda for processing", "isCorrect": true}, {"text": "Amazon EFS with Provisioned Throughput mode", "isCorrect": false}, {"text": "Multiple EBS volumes in RAID configuration", "isCorrect": false}]	Amazon S3 with CloudFront for content delivery and Lambda for processing	S3 provides unlimited automatic scaling, CloudFront handles traffic spikes through edge caching, and Lambda scales processing automatically with demand.	{"summary": "Auto-scaling storage architecture:", "breakdown": ["S3 unlimited scaling: Handles any amount of data and requests", "CloudFront edge caching: Reduces origin load during traffic spikes", "Lambda auto-scaling: Processing scales from 0 to thousands of concurrent executions", "No manual intervention: All components scale automatically"], "otherOptions": "EBS volumes cannot auto-scale capacity\\nEFS requires pre-provisioned throughput planning\\nRAID configuration still requires manual capacity planning"}	\N
65	165	65	AWS Storage Comprehensive	Expert	Design High-Performing Architectures	A large enterprise needs a comprehensive storage strategy for their cloud migration. Requirements include: 500TB database storage with 50,000 IOPS, 2PB file shares for 1000 users, 10PB object storage with global access, and hybrid connectivity to on-premises. Which architecture provides optimal performance, cost, and integration?	[{"text": "io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway", "isCorrect": true}, {"text": "gp3 EBS, FSx for Lustre, S3 Intelligent-Tiering, and Direct Connect", "isCorrect": false}, {"text": "Aurora storage, EFS General Purpose, S3 Standard, and VPN connections", "isCorrect": false}, {"text": "Instance store, FSx for Windows, S3 Glacier, and AWS Outposts", "isCorrect": false}]	io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway	io2 Block Express provides required IOPS, EFS Max I/O handles concurrent users, S3+CloudFront scales globally, Storage Gateway enables hybrid integration.	{"summary": "Enterprise storage architecture components:", "breakdown": ["io2 Block Express: 50,000 IOPS with sub-millisecond latency for databases", "EFS Max I/O: Scales to 1000+ concurrent users with higher IOPS", "S3 + CloudFront: Unlimited object storage with global low-latency access", "Storage Gateway: Seamless hybrid connectivity with caching"], "otherOptions": "gp3 may not provide consistent 50,000 IOPS, FSx Lustre not ideal for general file shares\\nAurora storage limits database choice, VPN insufficient for 500TB+ workloads\\nInstance store temporary, FSx Windows not mentioned as requirement, Glacier too slow"}	\N
66	166	66	AWS Storage Comprehensive	Expert	Design Secure Architectures	A financial services firm requires end-to-end security for their storage infrastructure handling sensitive customer data across S3, EBS, and EFS. Requirements include: customer-controlled encryption, automated compliance monitoring, data classification, and audit trails meeting regulatory standards. Which comprehensive security architecture meets all requirements?	[{"text": "AWS managed encryption, GuardDuty monitoring, and CloudTrail logging", "isCorrect": false}, {"text": "Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events", "isCorrect": true}, {"text": "CloudHSM encryption, Security Hub, and VPC Flow Logs", "isCorrect": false}, {"text": "Client-side encryption, Inspector scanning, and AWS Config rules", "isCorrect": false}]	Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events	Customer-managed KMS provides encryption control, Macie classifies sensitive data, Conformance Packs ensure compliance, CloudTrail data events provide complete audit trails.	{"summary": "Comprehensive security architecture for financial services:", "breakdown": ["Customer-managed KMS: Full control over encryption keys across all storage services", "Amazon Macie: Automated discovery and classification of sensitive financial data", "Config Conformance Packs: Continuous compliance monitoring for financial regulations", "CloudTrail data events: Complete audit trail of all data access activities"], "otherOptions": "AWS managed encryption lacks customer control required for financial services\\nCloudHSM overkill, Security Hub doesn't provide data classification\\nClient-side encryption complex to manage, Inspector focuses on vulnerabilities not data classification"}	\N
3	103	3	AWS Architecture - Cross-Region Resilience	Expert	Design Resilient Architectures	A media company streams video content globally and needs to ensure service availability even if an entire AWS region becomes unavailable. The application uses ALB, EC2 Auto Scaling, and RDS MySQL. Which design provides the MOST comprehensive disaster recovery solution?	[{"text": "Create read replicas in multiple regions with manual failover", "isCorrect": false}, {"text": "Use Route 53 health checks with cross-region ALBs and RDS cross-region automated backups", "isCorrect": false}, {"text": "Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability", "isCorrect": true}, {"text": "Use CloudFormation to deploy infrastructure in multiple regions manually when needed", "isCorrect": false}]	Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability	Full cross-region deployment with automated DNS failover and database replica promotion provides the fastest recovery from regional failures.	{"summary": "Comprehensive multi-region DR strategy:", "breakdown": ["Active-passive setup in multiple regions", "Route 53 automatic failover based on health checks", "RDS read replicas can be promoted to standalone databases", "Infrastructure already running in secondary region"], "otherOptions": "Manual failover too slow for video streaming\\nCross-region backups take too long to restore\\nManual deployment during disaster is too slow"}	\N
4	104	4	AWS Storage - Backup Strategy	Application	Design Resilient Architectures	A company needs to backup 50TB of data from on-premises to AWS. The data must be available within 4 hours during a disaster, and they want to minimize ongoing costs. The initial backup can take up to 2 weeks. Which solution is MOST appropriate?	[{"text": "AWS Storage Gateway with Tape Gateway to S3 Glacier", "isCorrect": false}, {"text": "AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering", "isCorrect": true}, {"text": "Direct upload to S3 Standard over internet connection", "isCorrect": false}, {"text": "AWS DataSync to S3 Glacier Deep Archive", "isCorrect": false}]	AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering	Snowball Edge handles the initial 50TB transfer efficiently, while S3 Intelligent-Tiering automatically optimizes costs and meets the 4-hour recovery requirement.	{"summary": "Large data backup strategy considerations:", "breakdown": ["Snowball Edge: Cost-effective for 50TB initial transfer", "S3 Intelligent-Tiering: Automatic cost optimization", "Standard tier availability: Immediate access for 4-hour RTO", "2-week initial timeline: Perfect for Snowball shipping"], "otherOptions": "Tape Gateway too slow for 4-hour recovery\\n50TB over internet too slow and expensive\\nGlacier Deep Archive takes 12+ hours to retrieve"}	\N
5	105	5	AWS Performance - Database Optimization	Application	Design High-Performing Architectures	An application experiences slow database performance during peak hours. The RDS MySQL instance shows 95% CPU utilization, and 80% of queries are read operations. The application cannot tolerate any downtime. Which solution provides the BEST performance improvement with minimal operational overhead?	[{"text": "Create a larger RDS instance and schedule maintenance window for upgrade", "isCorrect": false}, {"text": "Enable RDS Performance Insights to identify slow queries", "isCorrect": false}, {"text": "Create RDS read replicas and modify application to use them for read queries", "isCorrect": true}, {"text": "Migrate to Amazon Aurora MySQL with automatic scaling", "isCorrect": false}]	Create RDS read replicas and modify application to use them for read queries	Read replicas directly address the 80% read workload without downtime, providing immediate performance relief for read-heavy applications.	{"summary": "Read replica benefits for read-heavy workloads:", "breakdown": ["80% read queries can be offloaded to replicas", "No downtime required for setup", "Immediate performance improvement", "Cost-effective solution for read scaling"], "otherOptions": "Instance upgrade requires downtime\\nPerformance Insights only identifies issues, doesn't solve them\\nMigration requires significant effort and testing"}	\N
6	106	6	AWS Performance - Caching Strategy	Expert	Design High-Performing Architectures	A social media application serves user profiles stored in DynamoDB. The same profiles are accessed frequently by millions of users, causing high read costs and occasional throttling. The application requires sub-millisecond response times. Which caching strategy provides the BEST performance and cost optimization?	[{"text": "Implement application-level caching with Redis ElastiCache", "isCorrect": false}, {"text": "Enable DynamoDB DAX (DynamoDB Accelerator)", "isCorrect": true}, {"text": "Use CloudFront to cache DynamoDB API responses", "isCorrect": false}, {"text": "Increase DynamoDB provisioned read capacity", "isCorrect": false}]	Enable DynamoDB DAX (DynamoDB Accelerator)	DAX provides microsecond-level response times specifically designed for DynamoDB, with automatic cache management and no application changes required.	{"summary": "DAX advantages for DynamoDB caching:", "breakdown": ["Microsecond response times (sub-millisecond requirement)", "Transparent to application (no code changes)", "Automatic cache invalidation and management", "Reduces DynamoDB read costs significantly"], "otherOptions": "Redis requires application code changes and cache management\\nCloudFront can't cache DynamoDB API calls\\nIncreasing capacity doesn't solve latency, only costs more"}	\N
7	107	7	AWS Performance - Content Delivery	Comprehension	Design High-Performing Architectures	A global news website serves static content (images, videos, articles) to users worldwide. Users in Asia report slow page load times, while users in the US experience good performance. The origin servers are located in us-east-1. Which solution provides the MOST effective performance improvement for Asian users?	[{"text": "Deploy additional EC2 instances in ap-southeast-1", "isCorrect": false}, {"text": "Implement CloudFront distribution with edge locations in Asia", "isCorrect": true}, {"text": "Use Transfer Acceleration for S3 uploads", "isCorrect": false}, {"text": "Create Route 53 latency-based routing to multiple regions", "isCorrect": false}]	Implement CloudFront distribution with edge locations in Asia	CloudFront edge locations cache static content closer to Asian users, dramatically reducing latency for static content delivery.	{"summary": "CloudFront benefits for global content delivery:", "breakdown": ["Edge locations in Asia cache content locally", "Reduces latency from us-east-1 to Asia", "Handles static content (images, videos, articles) efficiently", "Automatic cache management and optimization"], "otherOptions": "Additional instances help dynamic content, not static\\nTransfer Acceleration for uploads, not downloads\\nLatency routing helps with dynamic content, not static"}	\N
12	112	12	AWS Cost Optimization - Instance Selection	Application	Design Cost-Optimized Architectures	A batch processing workload runs predictably every night from 2 AM to 6 AM for data analytics. The workload can tolerate interruptions and can resume from checkpoints. Current costs using On-Demand instances are $500/month. Which pricing model combination provides the MAXIMUM cost savings?	[{"text": "Reserved Instances for consistent baseline capacity", "isCorrect": false}, {"text": "Spot Instances with Auto Scaling groups across multiple AZs", "isCorrect": true}, {"text": "Savings Plans for compute usage commitment", "isCorrect": false}, {"text": "Dedicated Hosts for workload isolation", "isCorrect": false}]	Spot Instances with Auto Scaling groups across multiple AZs	Spot Instances can provide up to 90% savings for fault-tolerant batch workloads, and multiple AZs reduce interruption risk.	{"summary": "Spot Instances optimal for batch processing:", "breakdown": ["Up to 90% cost savings vs On-Demand", "Workload tolerates interruptions (checkpoint resume)", "Predictable 4-hour window reduces interruption risk", "Multiple AZs provide capacity diversification"], "otherOptions": "Reserved Instances only ~75% savings, overkill for 4-hour workload\\nSavings Plans better for consistent 24/7 usage\\nDedicated Hosts most expensive option"}	\N
13	113	13	AWS Cost Optimization - Storage Lifecycle	Application	Design Cost-Optimized Architectures	A company stores application logs in S3. Logs are actively analyzed for 30 days, occasionally accessed for 90 days, and must be retained for 7 years for compliance. Current storage costs are $1000/month in S3 Standard. Which lifecycle policy provides the BEST cost optimization?	[{"text": "Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year", "isCorrect": true}, {"text": "Keep all data in S3 Standard for quick access when needed", "isCorrect": false}, {"text": "Move everything to S3 Glacier immediately after upload", "isCorrect": false}, {"text": "Use S3 Intelligent-Tiering for automatic optimization", "isCorrect": false}]	Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year	Lifecycle policy matching access patterns: S3 Standard for active use, S3 IA for occasional access, Glacier for long-term retention, Deep Archive for compliance.	{"summary": "Optimal lifecycle transitions for log data:", "breakdown": ["Days 1-30: S3 Standard for active analysis", "Days 31-90: S3 IA for occasional access (50% savings)", "Days 91-365: S3 Glacier for backup (68% savings)", "Years 1-7: Deep Archive for compliance (77% savings)"], "otherOptions": "Standard storage most expensive for long-term retention\\nGlacier immediate transition prevents active analysis\\nIntelligent-Tiering adds overhead for predictable patterns"}	\N
14	114	14	AWS Cost Optimization - Database Costs	Expert	Design Cost-Optimized Architectures	A startup's MySQL database experiences unpredictable traffic patterns ranging from 5% to 95% CPU utilization throughout the day. Current RDS costs are $800/month with frequent over-provisioning during low-traffic periods. Which solution provides the BEST cost optimization while maintaining performance?	[{"text": "Migrate to Aurora Serverless v2 with automatic scaling", "isCorrect": true}, {"text": "Use RDS with scheduled Auto Scaling based on time patterns", "isCorrect": false}, {"text": "Switch to smaller RDS instance and add read replicas", "isCorrect": false}, {"text": "Migrate to DynamoDB with on-demand billing", "isCorrect": false}]	Migrate to Aurora Serverless v2 with automatic scaling	Aurora Serverless v2 automatically scales compute capacity based on actual demand, eliminating over-provisioning costs during low-traffic periods.	{"summary": "Aurora Serverless v2 benefits for variable workloads:", "breakdown": ["Automatic scaling from 0.5 to 128 ACUs based on demand", "Pay only for actual compute used (per-second billing)", "No over-provisioning during low-traffic periods", "MySQL compatibility maintains application compatibility"], "otherOptions": "Scheduled scaling can't handle unpredictable patterns\\nSmaller instance may cause performance issues during peaks\\nDynamoDB requires application rewrite from MySQL"}	\N
15	115	15	AWS Cost Optimization - Data Transfer	Application	Design Cost-Optimized Architectures	A company transfers 100TB of data monthly from EC2 instances to users worldwide. Current data transfer costs are $9,000/month. The data consists of software downloads that rarely change. Which solution provides the MOST significant cost reduction?	[{"text": "Use VPC endpoints to reduce data transfer charges", "isCorrect": false}, {"text": "Implement CloudFront CDN with S3 origin", "isCorrect": true}, {"text": "Move EC2 instances to regions with lower data transfer costs", "isCorrect": false}, {"text": "Compress all data before transmission", "isCorrect": false}]	Implement CloudFront CDN with S3 origin	CloudFront eliminates most internet data transfer charges and caches static content globally, dramatically reducing costs for software downloads.	{"summary": "CloudFront cost optimization for static content:", "breakdown": ["No data transfer charges between S3 and CloudFront", "Reduced CloudFront to internet rates vs EC2 to internet", "Caching reduces origin data transfer for repeated downloads", "Global edge locations improve performance and reduce costs"], "otherOptions": "VPC endpoints help AWS service costs, not internet transfer\\nRegional pricing differences minimal for internet transfer\\nCompression helps but doesn't address core transfer costs"}	\N
16	116	16	AWS Architecture - Decoupling	Application	Design Resilient Architectures	A photo processing application receives images via API, processes them, and stores results. During peak times, the processing queue becomes backlogged and API requests start failing. Which decoupling strategy provides the BEST resilience?	[{"text": "Increase EC2 instance size for faster processing", "isCorrect": false}, {"text": "Use SQS queue between API and processing with Auto Scaling based on queue depth", "isCorrect": true}, {"text": "Add more API Gateway throttling to reduce incoming requests", "isCorrect": false}, {"text": "Use Lambda functions instead of EC2 for processing", "isCorrect": false}]	Use SQS queue between API and processing with Auto Scaling based on queue depth	SQS decouples components allowing the API to accept requests even when processing is slow, with Auto Scaling adding capacity based on queue backlog.	{"summary": "SQS decoupling benefits:", "breakdown": ["API remains responsive during processing delays", "Queue acts as buffer for variable processing times", "Auto Scaling responds to actual demand (queue depth)", "No lost requests during traffic spikes"], "otherOptions": "Larger instances don't solve architecture coupling\\nThrottling causes request failures\\nLambda has 15-minute timeout limit for processing"}	\N
17	117	17	AWS Performance - Auto Scaling	Expert	Design High-Performing Architectures	An application experiences sudden traffic spikes that cause Auto Scaling to launch instances, but by the time instances are ready, traffic has decreased. This causes unnecessary costs. Which scaling strategy provides the BEST optimization?	[{"text": "Use predictive scaling with machine learning to anticipate traffic patterns", "isCorrect": true}, {"text": "Decrease Auto Scaling cooldown periods for faster scaling", "isCorrect": false}, {"text": "Use larger instance types that can handle traffic spikes", "isCorrect": false}, {"text": "Implement step scaling with smaller scaling increments", "isCorrect": false}]	Use predictive scaling with machine learning to anticipate traffic patterns	Predictive scaling uses machine learning to forecast traffic and pre-scale capacity, avoiding the lag time of reactive scaling.	{"summary": "Predictive scaling advantages:", "breakdown": ["ML forecasts traffic patterns to pre-scale capacity", "Reduces lag time between traffic spike and available capacity", "Prevents over-scaling by understanding traffic duration", "Works with historical patterns and scheduled events"], "otherOptions": "Faster scaling still reactive, doesn't solve core timing issue\\nOver-provisioning increases costs unnecessarily\\nStep scaling still reactive to traffic changes"}	\N
19	119	19	AWS Architecture - Service Limits	Expert	Design High-Performing Architectures	An application needs to upload files up to 500GB to S3. Users report upload failures and timeouts. The application currently uses single PUT requests. Which solution addresses the root cause of the upload failures?	[{"text": "Enable S3 Transfer Acceleration for faster uploads", "isCorrect": false}, {"text": "Implement S3 multipart upload for large files", "isCorrect": true}, {"text": "Use CloudFront for upload optimization", "isCorrect": false}, {"text": "Increase application timeout settings", "isCorrect": false}]	Implement S3 multipart upload for large files	S3 has a 5GB limit for single PUT operations. Files larger than 5GB require multipart upload, which is recommended for files over 100MB.	{"summary": "S3 upload limits and best practices:", "breakdown": ["Single PUT limit: 5GB maximum file size", "Multipart upload: Required for files >5GB, recommended >100MB", "500GB files: Must use multipart upload", "Improves reliability with retry capability per part"], "otherOptions": "Transfer Acceleration helps speed, doesn't solve size limit\\nCloudFront not designed for large file uploads\\nTimeouts don't solve the 5GB PUT limit"}	\N
20	120	20	AWS Architecture - Lambda Limitations	Expert	Design Resilient Architectures	A data processing function needs to run for 20 minutes to process large datasets. The current Lambda function times out after 15 minutes. Which solution provides the BEST architecture for this requirement?	[{"text": "Increase Lambda timeout to maximum 15 minutes and optimize code", "isCorrect": false}, {"text": "Break processing into smaller Lambda functions using Step Functions", "isCorrect": true}, {"text": "Use Lambda with SQS to process data in smaller chunks", "isCorrect": false}, {"text": "Migrate to ECS Fargate for longer-running tasks", "isCorrect": false}]	Break processing into smaller Lambda functions using Step Functions	Lambda has a hard 15-minute timeout limit. Step Functions can orchestrate multiple Lambda functions to handle longer workflows.	{"summary": "Lambda timeout limits and solutions:", "breakdown": ["Lambda maximum timeout: 15 minutes (hard limit)", "Step Functions: Can orchestrate multi-step workflows", "Break down processing: Multiple Lambda functions in sequence", "State management: Step Functions handles workflow state"], "otherOptions": "15 minutes is the maximum timeout\\nSQS helps with async processing but doesn't solve timeout\\nECS Fargate valid but Step Functions more serverless-native"}	\N
88	187	87	Performance - Database Read Scaling	Advanced	Design High-Performing Architectures	An analytics application runs complex read-heavy queries on a 10TB database. Read traffic varies from 1,000 to 50,000 queries per minute. Write traffic is minimal but requires immediate consistency. Which solution provides optimal read scaling?	[{"text": "Aurora with Auto Scaling read replicas and reader endpoint", "isCorrect": true}, {"text": "RDS with manual read replica provisioning", "isCorrect": false}, {"text": "DynamoDB with Global Secondary Indexes", "isCorrect": false}, {"text": "ElastiCache Redis cluster with read-through caching", "isCorrect": false}]	Aurora with Auto Scaling read replicas and reader endpoint	Aurora Auto Scaling automatically adjusts read replica count based on CPU utilization, reader endpoint distributes read traffic, and maintains strong consistency for writes.	{"summary": "Aurora read scaling advantages:", "breakdown": ["Auto Scaling replicas: Automatically adjusts from 1-15 replicas based on demand", "Reader endpoint: Automatic load balancing across read replicas", "Strong consistency: Immediate consistency for writes", "10TB capacity: Aurora scales storage automatically"], "otherOptions": "Manual scaling can't respond to 50x traffic variation efficiently\\nDynamoDB requires data model changes and lacks SQL compatibility\\nCaching adds complexity and potential consistency issues"}	\N
89	188	88	Performance - Edge Computing	Expert	Design High-Performing Architectures	A gaming platform needs sub-10ms response times for real-time multiplayer games globally. Traditional CloudFront caching cannot cache dynamic game state. Which solution provides the LOWEST latency for dynamic content?	[{"text": "Lambda@Edge with DynamoDB Global Tables", "isCorrect": true}, {"text": "CloudFront with origin shield and multiple origins", "isCorrect": false}, {"text": "Global Accelerator with multiple regional endpoints", "isCorrect": false}, {"text": "Route 53 latency-based routing with regional ALBs", "isCorrect": false}]	Lambda@Edge with DynamoDB Global Tables	Lambda@Edge runs code at CloudFront edge locations (400+ globally), DynamoDB Global Tables provide millisecond latency data access, enabling sub-10ms response times for dynamic content.	{"summary": "Edge computing for ultra-low latency:", "breakdown": ["Lambda@Edge: Code execution at 400+ edge locations", "DynamoDB Global Tables: Single-digit millisecond data access", "Dynamic content: Real-time game state processing at edge", "Global coverage: Reduces round-trip time to nearest edge"], "otherOptions": "Origin shield doesn't solve dynamic content latency\\nGlobal Accelerator improves network path but doesn't cache dynamic content\\nRoute 53 routing still requires round-trip to regional data centers"}	\N
90	189	89	Performance - Auto Scaling Optimization	Advanced	Design High-Performing Architectures	A video processing application experiences unpredictable spikes where demand can increase 20x within 5 minutes. Current Auto Scaling cannot respond fast enough, causing user timeouts. Which optimization provides the FASTEST scaling response?	[{"text": "Predictive scaling with custom CloudWatch metrics", "isCorrect": false}, {"text": "Pre-warm instances with warm pools and lifecycle hooks", "isCorrect": true}, {"text": "Larger instance types to handle peak load", "isCorrect": false}, {"text": "Scheduled scaling based on historical patterns", "isCorrect": false}]	Pre-warm instances with warm pools and lifecycle hooks	Warm pools maintain pre-initialized instances in stopped state, lifecycle hooks customize instance preparation, enabling sub-minute scaling response compared to cold instance launches.	{"summary": "Warm pool scaling optimization:", "breakdown": ["Warm pools: Pre-initialized instances ready in 30-60 seconds", "Lifecycle hooks: Custom preparation scripts for faster service startup", "Cost effective: Stopped instances have minimal EBS charges", "Unpredictable spikes: Always-ready capacity for immediate scaling"], "otherOptions": "Predictive scaling can't predict truly unpredictable spikes\\nOver-provisioning wastes cost during normal periods\\nScheduled scaling doesn't handle unpredictable demand"}	\N
91	190	90	Performance - Database Query Optimization	Expert	Design High-Performing Architectures	A data warehouse runs complex analytical queries that take 30+ minutes to complete. The queries scan 100TB+ datasets but only return aggregated results. Users need interactive query performance (<5 seconds). Which solution provides the BEST query acceleration?	[{"text": "Amazon Redshift with materialized views and result caching", "isCorrect": true}, {"text": "Athena with partitioned data and columnar formats", "isCorrect": false}, {"text": "Aurora with query plan caching and read replicas", "isCorrect": false}, {"text": "DynamoDB with pre-computed aggregation tables", "isCorrect": false}]	Amazon Redshift with materialized views and result caching	Redshift materialized views pre-compute complex aggregations, result caching returns identical queries instantly, and columnar storage with compression optimizes scan performance.	{"summary": "Data warehouse query acceleration:", "breakdown": ["Materialized views: Pre-computed aggregations refresh automatically", "Result caching: Identical queries return in milliseconds", "Columnar storage: Optimized for analytical query patterns", "Redshift Spectrum: Can query 100TB+ datasets efficiently"], "otherOptions": "Athena still requires scanning large datasets for complex aggregations\\nAurora optimized for OLTP, not analytical workloads\\nDynamoDB requires complete data model restructuring"}	\N
68	168	68	Security - Data Encryption Strategy	Advanced	Design Secure Architectures	A healthcare company processes PHI data and requires encryption at rest and in transit with full key management control. Which combination meets HIPAA compliance requirements? (Choose TWO)	[{"text": "S3 with SSE-S3 default encryption", "isCorrect": false}, {"text": "Customer-managed KMS keys with key rotation", "isCorrect": true}, {"text": "EBS volumes with AWS-managed encryption", "isCorrect": false}, {"text": "VPC endpoints for service communication", "isCorrect": true}, {"text": "Application Load Balancer with ACM certificates", "isCorrect": false}]	Customer-managed KMS keys with key rotation, VPC endpoints for service communication	HIPAA requires customer control over encryption keys (customer-managed KMS) and secure data transmission (VPC endpoints keep traffic off public internet). SSE-S3 and AWS-managed keys do not provide sufficient control.	{"summary": "HIPAA compliance requirements:", "breakdown": ["Customer-managed KMS: Full control over key policies and access", "VPC endpoints: Keep data traffic within AWS network", "Key rotation: Automatic rotation for compliance", "Audit trails: CloudTrail logs all key usage"], "otherOptions": "SSE-S3 doesn't provide customer key control\\nAWS-managed keys insufficient for HIPAA\\nACM helps but doesn't address data at rest"}	\N
77	177	77	Security - IAM Policy Conditions	Expert	Design Secure Architectures	A company needs developers to access S3 buckets only during business hours (9 AM - 5 PM) and only from corporate IP addresses. The policy must automatically deny access outside these conditions. Which IAM policy condition combination provides the MOST secure implementation?	[{"text": "aws:CurrentTime and aws:SourceIp conditions with explicit deny", "isCorrect": true}, {"text": "aws:RequestedTime and aws:VpcSourceIp with allow statements", "isCorrect": false}, {"text": "aws:TokenIssueTime and aws:userid conditions only", "isCorrect": false}, {"text": "aws:SecureTransport and aws:MultiFactorAuthAge conditions", "isCorrect": false}]	aws:CurrentTime and aws:SourceIp conditions with explicit deny	aws:CurrentTime restricts access to business hours, aws:SourceIp limits to corporate IPs, and explicit deny statements ensure security even if other policies allow access.	{"summary": "IAM conditional access controls:", "breakdown": ["aws:CurrentTime: Time-based access control (9 AM - 5 PM)", "aws:SourceIp: IP-based access restriction", "Explicit deny: Cannot be overridden by other allow policies", "Defense in depth: Multiple condition types for comprehensive security"], "otherOptions": "aws:RequestedTime is not a valid condition key\\nMissing time and IP restrictions required by business\\nAddresses different security concerns, not time/location"}	\N
79	179	79	Security - Data Loss Prevention	Expert	Design Secure Architectures	A healthcare company needs to prevent accidental exposure of PHI in S3 buckets across 100+ AWS accounts. They require automated remediation, real-time alerts, and compliance reporting. Which comprehensive solution provides the BEST data protection?	[{"text": "Amazon Macie + EventBridge + Config Remediation + Security Hub", "isCorrect": true}, {"text": "AWS Config + CloudWatch + SNS + Manual remediation", "isCorrect": false}, {"text": "GuardDuty + CloudTrail + Lambda + Custom dashboard", "isCorrect": false}, {"text": "Inspector + Systems Manager + CloudFormation + Cost Explorer", "isCorrect": false}]	Amazon Macie + EventBridge + Config Remediation + Security Hub	Macie detects PHI and exposure risks, EventBridge provides real-time alerting, Config Remediation automatically fixes violations, Security Hub centralizes compliance reporting.	{"summary": "Comprehensive data protection architecture:", "breakdown": ["Macie: ML-powered PHI detection and exposure analysis", "EventBridge: Real-time alerts for immediate response", "Config Remediation: Automatic policy enforcement", "Security Hub: Centralized compliance dashboard across 100+ accounts"], "otherOptions": "Lacks automated PHI detection and remediation\\nGuardDuty focuses on threats, not data classification\\nInspector and Cost Explorer irrelevant for data protection"}	\N
21	121	21	AWS Cost Optimization - Compute Savings	Expert	Design Cost-Optimized Architectures	A company runs a web application with the following pattern: 2 instances minimum 24/7, scales to 10 instances during business hours (9 AM-5 PM weekdays), and spikes to 50 instances during monthly sales (first 3 days). Current monthly On-Demand costs are $15,000. Which purchasing strategy provides MAXIMUM cost savings while maintaining performance?	[{"text": "All-Upfront Reserved Instances for 10 instances, On-Demand for the rest", "isCorrect": false}, {"text": "Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes", "isCorrect": true}, {"text": "Compute Savings Plans for all capacity needs", "isCorrect": false}, {"text": "Scheduled Reserved Instances for business hours, On-Demand for spikes", "isCorrect": false}]	Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes	Layered purchasing strategy: RIs for baseline (70% savings), Savings Plans for predictable scaling (66% savings), Spot for spikes (up to 90% savings).	{"summary": "Optimal cost strategy for variable workloads:", "breakdown": ["Reserved Instances: 2 baseline instances (24/7) - 70% savings", "Savings Plans: 8 additional instances for business hours - 66% savings", "Spot Instances: 40 instances for monthly sales - up to 90% savings", "Total savings: Approximately 75% reduction from $15,000"], "otherOptions": "Over-provisioning RIs for 10 instances wastes money nights/weekends\\nSavings Plans less discount than RIs for baseline\\nScheduled RIs discontinued, wouldn't cover spikes"}	\N
22	122	22	AWS Cost Optimization - Data Processing	Application	Design Cost-Optimized Architectures	A data analytics company processes 10TB of log files daily using EMR clusters. Processing takes 4 hours and runs once daily at 2 AM. The current approach uses On-Demand instances costing $8,000/month. Which architecture change provides the BEST cost optimization?	[{"text": "Migrate to AWS Glue for serverless ETL processing", "isCorrect": false}, {"text": "Use EMR on Spot Instances with diverse instance types across multiple AZs", "isCorrect": true}, {"text": "Pre-process data with Lambda before EMR to reduce cluster size", "isCorrect": false}, {"text": "Use smaller EMR cluster running 24/7 with Reserved Instances", "isCorrect": false}]	Use EMR on Spot Instances with diverse instance types across multiple AZs	EMR on Spot Instances can provide 50-80% cost savings for batch processing workloads that can tolerate interruptions.	{"summary": "EMR Spot Instance optimization:", "breakdown": ["Spot savings: 50-80% reduction from On-Demand pricing", "Instance diversity: Reduces interruption risk", "Multiple AZs: Ensures capacity availability", "4-hour processing window: Perfect for Spot reliability"], "otherOptions": "Glue more expensive for 10TB daily processing\\nLambda timeout and cost limitations for TB-scale data\\n24/7 cluster wasteful for 4-hour daily job"}	\N
23	123	23	AWS Cost Optimization - Storage Optimization	Expert	Design Cost-Optimized Architectures	A media company stores 500TB of video content in S3 Standard. Analytics show: 10% accessed daily (hot), 30% accessed weekly (warm), 60% accessed rarely but unpredictably. Retrieval must complete within 12 hours. Current cost is $11,500/month. Which storage strategy provides optimal cost reduction?	[{"text": "S3 Intelligent-Tiering for all content with archive configuration", "isCorrect": true}, {"text": "S3 Standard for hot, S3 IA for warm, S3 Glacier for cold data", "isCorrect": false}, {"text": "S3 One Zone-IA for all content to reduce costs", "isCorrect": false}, {"text": "S3 Standard for hot, S3 Glacier Instant Retrieval for all other content", "isCorrect": false}]	S3 Intelligent-Tiering for all content with archive configuration	S3 Intelligent-Tiering automatically optimizes storage costs for unpredictable access patterns without retrieval fees, perfect for this use case.	{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic tiering: No manual lifecycle policies needed", "No retrieval fees: Critical for unpredictable access", "Archive configuration: Moves rarely accessed objects to archive tiers", "Cost savings: Up to 70% reduction while maintaining performance"], "otherOptions": "Manual lifecycle rules don't handle unpredictable access well\\nOne Zone-IA risky for 500TB of valuable content\\nGlacier Instant Retrieval has retrieval fees for 60% of content"}	\N
26	126	26	AWS Architecture - Event-Driven Resilience	Expert	Design Resilient Architectures	An e-commerce platform must process orders from multiple channels (web, mobile, partners) with different formats. The system must handle 50,000 orders/hour during sales, ensure no order loss, and enable new channel integration without affecting existing ones. Which architecture provides the BEST resilience and scalability?	[{"text": "API Gateway with Lambda functions writing directly to DynamoDB", "isCorrect": false}, {"text": "Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors", "isCorrect": true}, {"text": "Kinesis Data Streams with Lambda consumers for each channel", "isCorrect": false}, {"text": "Application Load Balancer with Auto Scaling EC2 instances", "isCorrect": false}]	Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors	EventBridge provides event routing with schema registry, SQS ensures message durability, and containerized processors enable independent scaling per channel.	{"summary": "Event-driven architecture benefits:", "breakdown": ["EventBridge: Central event router with content-based filtering", "Schema Registry: Validates events and enables discovery", "SQS queues: Guaranteed message delivery and buffering", "Container isolation: Each channel processor scales independently"], "otherOptions": "Direct writes risk data loss during high load\\nKinesis overkill for transactional data, complex scaling\\nMonolithic approach, difficult to add new channels"}	\N
27	127	27	AWS Architecture - Disaster Recovery	Application	Design Resilient Architectures	A financial services application requires RPO of 1 hour and RTO of 4 hours. The application uses Aurora PostgreSQL, EC2 instances with custom AMIs, and stores documents in S3. The DR solution must be cost-effective. Which approach meets these requirements?	[{"text": "Aurora Global Database with standby EC2 instances in secondary region", "isCorrect": false}, {"text": "Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup", "isCorrect": true}, {"text": "Multi-region active-active deployment with Route 53 failover", "isCorrect": false}, {"text": "Database snapshots and EC2 snapshots copied hourly to secondary region", "isCorrect": false}]	Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup	Backup-based DR meets RPO/RTO requirements cost-effectively: Aurora automated backups (continuous), AMIs ready for quick launch, S3 CRR for documents.	{"summary": "Cost-effective DR strategy components:", "breakdown": ["Aurora backups: Point-in-time recovery within 5 minutes (meets 1-hour RPO)", "Cross-region AMI copies: Ready for quick EC2 launch", "S3 CRR: Near real-time replication for documents", "AWS Backup: Centralized backup management and compliance"], "otherOptions": "Global Database expensive for 4-hour RTO requirement\\nActive-active overkill and complex for these requirements\\nManual snapshots miss continuous data changes"}	\N
28	128	28	AWS Performance - API Optimization	Application	Design High-Performing Architectures	A mobile app backend serves personalized content to 10 million users globally. The API Gateway shows high latency for data aggregation from multiple microservices. Each request makes 5-7 service calls with 200ms average latency per call. Which solution provides the BEST performance improvement?	[{"text": "Implement API Gateway caching for all endpoints", "isCorrect": false}, {"text": "Use AWS AppSync with GraphQL to batch and parallelize service calls", "isCorrect": true}, {"text": "Add ElastiCache in front of each microservice", "isCorrect": false}, {"text": "Increase Lambda memory allocation for faster processing", "isCorrect": false}]	Use AWS AppSync with GraphQL to batch and parallelize service calls	AppSync with GraphQL reduces API calls through query batching and parallel resolution, dramatically reducing overall latency for aggregated data.	{"summary": "AppSync optimization benefits:", "breakdown": ["Query batching: Single request instead of 5-7 calls", "Parallel resolution: Service calls execute simultaneously", "Reduced latency: From 1000-1400ms to 200-300ms total", "Caching built-in: Further performance improvements"], "otherOptions": "Caching doesn't help with personalized content\\nStill requires serial service calls\\nLambda memory doesn't solve aggregation latency"}	\N
29	129	29	AWS Performance - ML Inference	Expert	Design High-Performing Architectures	A video streaming platform needs real-time content moderation using computer vision ML models. They process 1,000 concurrent streams with <100ms inference latency requirement. Current SageMaker endpoint shows 300ms latency. Which optimization provides the required performance?	[{"text": "Use larger SageMaker instance types with more GPUs", "isCorrect": false}, {"text": "Deploy models to Lambda with Provisioned Concurrency", "isCorrect": false}, {"text": "Implement SageMaker multi-model endpoints with edge deployment using AWS IoT Greengrass", "isCorrect": false}, {"text": "Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference", "isCorrect": true}]	Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference	EC2 Inf1 instances with AWS Inferentia chips provide the lowest latency and highest throughput for real-time ML inference at scale.	{"summary": "Inf1 instance optimization benefits:", "breakdown": ["AWS Inferentia chips: Purpose-built for ML inference", "Sub-100ms latency: Achievable with optimized models", "High throughput: Handle 1,000 concurrent streams", "Cost-effective: Up to 70% lower cost than GPU instances"], "otherOptions": "Larger instances don't guarantee latency reduction\\nLambda cold starts and limits prevent consistent <100ms\\nEdge deployment adds complexity without meeting core requirement"}	\N
30	130	30	AWS Cost Optimization - Hybrid Architecture	Expert	Design Cost-Optimized Architectures	A company wants to modernize their on-premises data warehouse (50Tto AWS. They need to maintain some on-premises reporting tools while enabling cloud analytics. Budget is limited to $5,000/month. Query patterns: 20% hot data (daily), 30% warm (weekly), 50% cold (monthly). Which architecture provides the BEST cost optimization?	[{"text": "Migrate everything to Redshift with Reserved Instances", "isCorrect": false}, {"text": "Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect", "isCorrect": true}, {"text": "Keep all data on-premises and use Athena federated queries", "isCorrect": false}, {"text": "Use RDS PostgreSQL with read replicas for all data", "isCorrect": false}]	Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect	Tiered approach with Redshift for hot data and Spectrum for S3-based warm/cold data provides optimal price-performance within budget constraints.	{"summary": "Hybrid data warehouse optimization:", "breakdown": ["Redshift (10TB): ~$2,500/month for hot data performance", "S3 + Spectrum (40TB): ~$1,000/month for warm/cold storage", "Direct Connect: ~$1,500/month for reliable hybrid connectivity", "Total: ~$5,000/month within budget with optimal performance"], "otherOptions": "Full Redshift for 50TB exceeds budget significantly\\nAthena federated queries too slow for hot data needs\\nRDS not optimized for analytics workloads, would exceed budget"}	\N
31	131	31	AWS Cost Optimization - Serverless Economics	Expert	Design Cost-Optimized Architectures	A news aggregation service processes 50 million articles monthly with unpredictable spikes during breaking news (up to 10x normal load). Current EC2-based architecture costs $12,000/month and struggles with spike handling. Articles average 5KB, require text analysis, and must be searchable within 1 minute. Which serverless architecture provides the best cost optimization while meeting requirements?	[{"text": "API Gateway  Lambda  DynamoDB with ElasticSearch", "isCorrect": false}, {"text": "EventBridge  SQS  Lambda with concurrent execution limits  S3  Athena with Glue crawlers", "isCorrect": true}, {"text": "Kinesis Data Streams  Kinesis Analytics  RDS", "isCorrect": false}, {"text": "Step Functions orchestrating multiple Lambda functions  Aurora Serverless", "isCorrect": false}]	EventBridge  SQS  Lambda with concurrent execution limits  S3  Athena with Glue crawlers	EventBridge with SQS provides event buffering for spikes, Lambda with concurrency limits controls costs, S3 offers cheap storage, and Athena enables SQL searches without database costs.	{"summary": "Serverless architecture cost optimization:", "breakdown": ["EventBridge + SQS: Handles 10x spikes without over-provisioning", "Lambda concurrency limits: Prevents runaway costs during spikes", "S3 storage: $0.023/GB vs database storage at $0.10/GB", "Athena queries: Pay-per-query ($5/TB scanned) vs always-on database"], "otherOptions": "ElasticSearch cluster costs $3000+/month minimum\\nKinesis Analytics expensive for sporadic spikes\\nAurora Serverless still has minimum capacity costs"}	\N
32	132	32	AWS Cost Optimization - Container Right-Sizing	Application	Design Cost-Optimized Architectures	A microservices application runs 40 containers on ECS with varying resource needs: 10 containers need 4 vCPU/8GB (API servers), 20 need 1 vCPU/2GB (workers), and 10 need 0.5 vCPU/1GB (sidecars). Current costs using m5.4xlarge instances are $8,000/month with 35% CPU utilization. Which optimization strategy provides maximum cost savings?	[{"text": "Switch to larger instances for better CPU:memory ratio", "isCorrect": false}, {"text": "Use ECS capacity providers with mixed instance types and Spot for workers", "isCorrect": true}, {"text": "Migrate all containers to Fargate with right-sized task definitions", "isCorrect": false}, {"text": "Implement cluster auto-scaling based on CPU utilization", "isCorrect": false}]	Use ECS capacity providers with mixed instance types and Spot for workers	Capacity providers enable mixing instance types optimized for different workloads, while Spot instances for stateless workers can reduce costs by 70-90%.	{"summary": "Container cost optimization strategy:", "breakdown": ["Mixed instances: c5 for CPU-intensive, r5 for memory-intensive", "Spot for workers: 70% savings on 20 containers (stateless)", "Better bin packing: Increases utilization from 35% to 75%", "Total savings: Approximately 60% reduction from $8,000"], "otherOptions": "Larger instances worsen utilization problems\\nFargate more expensive for predictable workloads\\nAuto-scaling doesn't address instance type mismatch"}	\N
34	134	34	AWS Architecture - IoT and Edge	Expert	Design Resilient Architectures	A manufacturing company operates 500 factories with 100 IoT sensors each, generating 1KB messages every second. They need local anomaly detection with sub-second response, cloud-based analytics, and must operate during internet outages. Which architecture provides the required edge computing capabilities?	[{"text": "Direct IoT Core ingestion with Kinesis Analytics for anomaly detection", "isCorrect": false}, {"text": "AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync", "isCorrect": true}, {"text": "EC2 instances in each factory with VPN connections to cloud", "isCorrect": false}, {"text": "AWS Outposts in each factory for local processing", "isCorrect": false}]	AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync	IoT Greengrass enables local Lambda execution for sub-second anomaly detection, continues operating offline, and synchronizes with cloud when connected.	{"summary": "Edge computing with IoT Greengrass benefits:", "breakdown": ["Local Lambda: Sub-second anomaly detection at edge", "Offline operation: Continues processing during outages", "Efficient sync: Batches data for cloud analytics when connected", "Cost-effective: No need for servers in 500 locations"], "otherOptions": "Requires constant internet, no local processing\\nManaging 500 EC2 instances operationally complex\\nOutposts overkill and expensive for IoT workloads"}	\N
70	170	70	Resilience - Disaster Recovery Strategy	Intermediate	Design Resilient Architectures	A financial application requires RTO of 4 hours and RPO of 1 hour. The application uses EC2, RDS MySQL, and S3. Which DR strategy meets these requirements MOST cost-effectively?	[{"text": "Pilot light with automated failover scripts", "isCorrect": true}, {"text": "Warm standby with Multi-AZ RDS in secondary region", "isCorrect": false}, {"text": "Hot standby with real-time replication", "isCorrect": false}, {"text": "Backup and restore with scheduled snapshots", "isCorrect": false}]	Pilot light with automated failover scripts	Pilot light maintains core infrastructure (like RDS read replica) in secondary region, can meet 4-hour RTO with automation, and is more cost-effective than warm/hot standby while exceeding backup/restore capabilities.	{"summary": "Pilot light DR strategy:", "breakdown": ["Maintains core infrastructure (database replicas)", "Automated deployment scripts for rapid scaling", "Meets 4-hour RTO with proper automation", "More cost-effective than hot standby"], "otherOptions": "Warm standby is expensive and exceeds requirements\\nHot standby is expensive and exceeds requirements\\nBackup/restore may not meet 4-hour RTO"}	\N
35	135	35	AWS Cost Optimization - Streaming Data	Application	Design Cost-Optimized Architectures	A ride-sharing app ingests 100,000 location updates per second during peak hours (6-9 AM, 5-8 PM) but only 5,000 per second during off-peak. Current Kinesis Data Streams with provisioned shards costs $15,000/month. Real-time processing is required only during peak hours; batch processing is acceptable off-peak. Which architecture reduces costs while meeting requirements?	[{"text": "Kinesis Data Streams with auto-scaling shards based on traffic", "isCorrect": false}, {"text": "Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch", "isCorrect": true}, {"text": "Replace with SQS queues and Lambda functions", "isCorrect": false}, {"text": "DynamoDB Streams with Lambda triggers", "isCorrect": false}]	Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch	Kinesis On-Demand eliminates over-provisioning costs while Firehose to S3 provides cost-effective batch processing during off-peak hours.	{"summary": "Streaming cost optimization strategy:", "breakdown": ["Kinesis On-Demand: Pay only for actual throughput", "Peak hours: Real-time processing as required", "Off-peak Firehose: 90% cheaper than real-time streams", "Estimated savings: 65% reduction from $15,000/month"], "otherOptions": "Auto-scaling still requires minimum shards\\nSQS lacks streaming semantics for real-time\\nDynamoDB Streams tied to table operations"}	\N
36	136	36	AWS Architecture - Multi-Region	Expert	Design Resilient Architectures	A global trading platform requires <10ms latency for 99% of users across US, EU, and APAC regions. The platform processes 1 million trades daily with strict consistency requirements. Each trade must be reflected globally within 100ms. Current single-region deployment shows 150ms+ latency for distant users. Which multi-region architecture best meets these requirements?	[{"text": "Deploy full stack in each region with eventual consistency", "isCorrect": false}, {"text": "Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data", "isCorrect": true}, {"text": "Single region with CloudFront caching for static content", "isCorrect": false}, {"text": "Active-active regions with asynchronous replication", "isCorrect": false}]	Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data	Aurora Global Database provides <1 second replication with strong consistency, local read replicas ensure <10ms reads, while DynamoDB Global Tables handle session state with automatic conflict resolution.	{"summary": "Multi-region architecture for low latency trading:", "breakdown": ["Aurora Global: <1 second cross-region replication", "Local read replicas: <10ms read latency per region", "Write forwarding: Maintains consistency for trades", "DynamoDB Global Tables: Multi-master for session data"], "otherOptions": "Eventual consistency violates trade requirements\\nCloudFront doesn't help with dynamic trade data\\nAsync replication can't guarantee 100ms consistency"}	\N
38	138	38	AWS Performance - Hybrid Networking	Application	Design High-Performing Architectures	A media company needs to transfer 50TB of daily video content from on-premises editing stations to S3 for processing. Current internet upload takes 20 hours, causing production delays. The solution must complete transfers within 4 hours and support 100 concurrent editor workstations. Which approach best meets these performance requirements?	[{"text": "Multiple Site-to-Site VPN connections with ECMP", "isCorrect": false}, {"text": "AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration", "isCorrect": true}, {"text": "Storage Gateway File Gateway with local cache", "isCorrect": false}, {"text": "Daily AWS Snowball Edge shipments", "isCorrect": false}]	AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration	Direct Connect provides dedicated bandwidth for consistent transfer speeds, VIFs enable private S3 connectivity, and Transfer Acceleration optimizes the upload path for concurrent workstations.	{"summary": "High-performance hybrid transfer solution:", "breakdown": ["Direct Connect: 10Gbps dedicated bandwidth", "VIF to S3: Private connectivity, no internet congestion", "Transfer Acceleration: Optimizes for 100 concurrent uploads", "50TB in 4 hours: Requires ~3.5Gbps sustained throughput"], "otherOptions": "VPN limited to ~1.25Gbps aggregate bandwidth\\nFile Gateway adds latency and cache limitations\\nSnowball shipping time exceeds 4-hour requirement"}	\N
39	139	39	AWS Architecture - Event-Driven Serverless	Expert	Design High-Performing Architectures	An e-commerce platform needs to process order events with multiple downstream systems: inventory (must process once), shipping (at-least-once), analytics (can tolerate duplicates), and email (exactly-once). Order volume varies from 100/minute to 10,000/minute during flash sales. Which event-driven architecture ensures correct delivery semantics for each consumer?	[{"text": "SNS with SQS queues for all consumers", "isCorrect": false}, {"text": "EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email", "isCorrect": true}, {"text": "Kinesis Data Streams with Lambda consumers", "isCorrect": false}, {"text": "Direct Lambda invocations with error handling", "isCorrect": false}]	EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email	EventBridge provides content-based routing, SQS FIFO ensures exactly-once processing, standard SQS handles at-least-once, Kinesis supports analytics replay, and Step Functions manages email workflow state.	{"summary": "Event delivery semantics per consumer:", "breakdown": ["SQS FIFO: Exactly-once for critical inventory updates", "Standard SQS: At-least-once with retry for shipping", "Kinesis: Replay capability for analytics reprocessing", "Step Functions: Manages email state to prevent duplicates"], "otherOptions": "SNS+SQS doesn't provide exactly-once semantics\\nKinesis requires all consumers to process in order\\nDirect invocations lack delivery guarantees"}	\N
40	140	40	AWS Cost Optimization - Modern Architectures	Expert	Design Cost-Optimized Architectures	A SaaS company runs 200 customer environments, each with identical architecture: ALB, 2-10 EC2 instances, RDS MySQL, and 50GB of S3 storage. Monthly costs are $400,000 with only 30% resource utilization. Customers require isolation for compliance. Which modernization approach provides maximum cost reduction while maintaining isolation?	[{"text": "Containerize applications and run all customers on shared EKS cluster", "isCorrect": false}, {"text": "AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies", "isCorrect": true}, {"text": "Lambda functions with RDS Proxy and separate VPCs", "isCorrect": false}, {"text": "Consolidate all customers into a single multi-tenant application", "isCorrect": false}]	AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies	App Runner provides serverless compute with environment isolation, Aurora Serverless v2 scales database resources based on actual usage, eliminating overprovisioning while maintaining compliance isolation.	{"summary": "Serverless multi-tenant cost optimization:", "breakdown": ["App Runner: Pay-per-request pricing, no idle EC2 costs", "Aurora Serverless v2: Scales down to 0.5 ACU when idle", "Environment isolation: Separate App Runner services per customer", "Estimated savings: 70% reduction through utilization-based pricing"], "otherOptions": "Shared cluster violates compliance isolation requirements\\nLambda cold starts problematic for web applications\\nMulti-tenant architecture breaks compliance requirements"}	\N
45	145	45	AWS S3 Deep Dive	Expert	Design Cost-Optimized Architectures	A content delivery platform stores 200TB of video files with unpredictable access patterns. Analytics show some videos remain popular for months while others are rarely accessed after the first week. They need cost optimization without performance penalties for popular content. Which S3 storage strategy provides optimal cost savings?	[{"text": "Lifecycle policy transitioning all content to S3 IA after 30 days", "isCorrect": false}, {"text": "S3 Intelligent-Tiering with Deep Archive Access tier enabled", "isCorrect": true}, {"text": "Keep popular content in Standard, move others to Glacier based on view counts", "isCorrect": false}, {"text": "Use S3 One Zone-IA for all content to reduce costs", "isCorrect": false}]	S3 Intelligent-Tiering with Deep Archive Access tier enabled	S3 Intelligent-Tiering automatically optimizes costs based on access patterns without retrieval fees for frequent/infrequent tiers, perfect for unpredictable patterns.	{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic optimization: No manual lifecycle policies needed", "No retrieval fees: For frequent and infrequent access tiers", "Deep Archive tier: Additional savings for rarely accessed content", "Performance maintained: Popular content stays in frequent tier"], "otherOptions": "Fixed lifecycle doesn't handle unpredictable patterns\\nManual management based on view counts is complex and error-prone\\nOne Zone-IA lacks resilience for valuable content"}	\N
50	150	50	AWS File Systems Deep Dive	Expert	Design High-Performing Architectures	A Windows-based application requires high-performance file storage with Active Directory integration, SMB protocol support, and sub-millisecond latencies. The workload involves intensive random I/O operations on small files. Current on-premises Windows File Server achieves 100,000 IOPS. Which AWS solution provides equivalent performance?	[{"text": "Amazon FSx for Windows File Server with SSD storage", "isCorrect": true}, {"text": "Amazon EFS with Windows file gateway", "isCorrect": false}, {"text": "EC2 Windows instance with attached io2 EBS volumes", "isCorrect": false}, {"text": "Amazon FSx for Lustre with Windows connectivity", "isCorrect": false}]	Amazon FSx for Windows File Server with SSD storage	FSx for Windows File Server provides native SMB, Active Directory integration, SSD storage for high IOPS, and sub-millisecond latencies specifically optimized for Windows workloads.	{"summary": "FSx for Windows File Server benefits:", "breakdown": ["Native SMB: Full Windows file system compatibility", "Active Directory: Seamless user authentication and authorization", "SSD storage: Up to 100,000+ IOPS for random I/O workloads", "Sub-millisecond latency: Optimized for Windows application performance"], "otherOptions": "EFS doesn't natively support SMB or Active Directory\\nSelf-managed solution lacks native SMB optimization\\nFSx for Lustre designed for Linux HPC workloads, not Windows"}	\N
51	151	51	AWS Hybrid Storage Migration	Application	Design Resilient Architectures	A company needs to migrate 500TB of archival data from on-premises tape storage to AWS. The data is rarely accessed but must be retrievable within 12 hours when needed. The migration must complete within 3 months with minimal impact on existing network bandwidth. Which migration strategy is MOST efficient?	[{"text": "AWS DataSync over Direct Connect for gradual migration", "isCorrect": false}, {"text": "Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive", "isCorrect": true}, {"text": "Storage Gateway Tape Gateway for gradual cloud migration", "isCorrect": false}, {"text": "AWS Transfer Family for secure file transfer", "isCorrect": false}]	Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive	Snowball Edge devices handle large data volumes efficiently without network impact, while Glacier Deep Archive provides cost-effective storage with 12-hour retrieval.	{"summary": "Large-scale archival migration strategy:", "breakdown": ["Snowball Edge capacity: 80TB per device, 7 devices for 500TB", "No network impact: Physical data transfer bypasses internet", "Glacier Deep Archive: Lowest cost storage for rarely accessed data", "12-hour retrieval: Meets business requirement for archive access"], "otherOptions": "DataSync over network would take months and impact bandwidth\\nTape Gateway for ongoing hybrid, not one-time migration\\nTransfer Family not optimized for 500TB bulk migration"}	\N
76	176	76	Cost Optimization - Serverless Architecture	Advanced	Design Cost-Optimized Architectures	An image processing application receives 500 uploads daily with unpredictable timing. Processing takes 3-8 minutes per image and requires 4GB memory. Current EC2 instances run idle 85% of the time. Which solution optimizes costs?	[{"text": "Increase Lambda memory and extend timeout to 15 minutes", "isCorrect": false}, {"text": "Use SQS to queue jobs and process with ECS Fargate auto-scaling", "isCorrect": true}, {"text": "Implement Step Functions to coordinate multiple Lambda functions", "isCorrect": false}, {"text": "Use Spot Instances with Auto Scaling based on SQS queue depth", "isCorrect": false}]	Use SQS to queue jobs and process with ECS Fargate auto-scaling	Fargate provides right-sizing for memory-intensive, long-running tasks without server management, eliminating idle time costs with auto-scaling.	{"summary": "Serverless solution for long-running tasks:", "breakdown": ["SQS: reliable job queuing", "Fargate: no server management, pay per use", "Auto-scaling: eliminates idle time", "4GB memory support: handles resource requirements"], "otherOptions": "Lambda has 15-minute maximum execution time\\nStep Functions still limited by Lambda constraints\\nSpot Instances can be interrupted during processing"}	\N
61	161	61	AWS Data Lifecycle	Application	Design Cost-Optimized Architectures	A research institution stores genomics data with the following access pattern: analyzed intensively for 30 days, occasionally referenced for 6 months, and archived for 10 years for compliance. Current costs are $50,000/month in S3 Standard. Which lifecycle policy provides maximum cost optimization while meeting access requirements?	[{"text": "Standard for 30 days  IA at 30 days  Glacier at 180 days  Deep Archive at 1 year", "isCorrect": true}, {"text": "Intelligent-Tiering for all data with Deep Archive enabled", "isCorrect": false}, {"text": "Standard for 30 days  Glacier immediately at 30 days", "isCorrect": false}, {"text": "One Zone-IA for all data to reduce costs", "isCorrect": false}]	Standard for 30 days  IA at 30 days  Glacier at 180 days  Deep Archive at 1 year	Lifecycle transitions match access patterns: Standard for intensive analysis, IA for occasional access, Glacier for long-term storage, Deep Archive for compliance.	{"summary": "Optimized lifecycle transitions for research data:", "breakdown": ["Days 1-30: S3 Standard for intensive analysis", "Days 31-180: S3 IA for occasional reference (68% cost reduction)", "Days 181-365: S3 Glacier for backup storage (77% cost reduction)", "Years 1-10: Deep Archive for compliance (80% cost reduction)"], "otherOptions": "Intelligent-Tiering adds overhead for predictable access patterns\\nEarly Glacier transition makes occasional access expensive\\nOne Zone-IA lacks resilience for valuable research data"}	\N
62	162	62	AWS Data Lifecycle	Expert	Design Cost-Optimized Architectures	A media company has complex data lifecycle needs: news content (hot for 7 days, cold afterwards), evergreen content (unpredictable access), and archived footage (rare access but immediate retrieval when needed). They want automated optimization without management overhead. Which strategy handles all three patterns optimally?	[{"text": "Separate buckets with different lifecycle policies for each content type", "isCorrect": false}, {"text": "S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies", "isCorrect": true}, {"text": "Manual lifecycle management based on content metadata", "isCorrect": false}, {"text": "Single lifecycle policy with shortest transition times", "isCorrect": false}]	S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies	Intelligent-Tiering with object tags enables different optimization strategies per content type while maintaining automation and handling unpredictable access patterns.	{"summary": "Tag-based intelligent lifecycle management:", "breakdown": ["Object tagging: Classify content types (news, evergreen, archive)", "Intelligent-Tiering: Automatic optimization for unpredictable evergreen content", "Tag-based policies: Specific rules for news (7-day pattern) and archive content", "No management overhead: All transitions automated based on access patterns"], "otherOptions": "Multiple buckets increase management complexity\\nManual management doesn't scale and introduces errors\\nSingle policy can't optimize for different access patterns"}	\N
63	163	63	AWS Storage Cost Management	Expert	Design Cost-Optimized Architectures	A data analytics company spends $100,000/month on storage across S3, EBS, and EFS for various workloads. They need comprehensive cost optimization while maintaining performance. Analysis shows: 40% rarely accessed, 30% predictable patterns, 20% unpredictable access, 10% high-performance needs. Which multi-service optimization strategy provides maximum savings?	[{"text": "Move all data to cheapest storage classes regardless of access patterns", "isCorrect": false}, {"text": "S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access", "isCorrect": true}, {"text": "Consolidate all storage to S3 with single lifecycle policy", "isCorrect": false}, {"text": "Implement Reserved Capacity for all storage services", "isCorrect": false}]	S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access	Tailored optimization per access pattern: lifecycle for predictable patterns, Intelligent-Tiering for unpredictable, gp3 for cost-effective EBS performance, EFS IA for infrequent access.	{"summary": "Multi-service cost optimization strategy:", "breakdown": ["40% rarely accessed: S3 lifecycle to Glacier/Deep Archive (75% savings)", "20% unpredictable: S3 Intelligent-Tiering (40% average savings)", "EBS optimization: gp3 volumes with right-sized IOPS (20% savings)", "EFS optimization: IA storage class for infrequent access (85% savings)"], "otherOptions": "Ignoring access patterns can cause performance issues and retrieval costs\\nNot all workloads suit object storage architecture\\nReserved Capacity not available for all storage types and may not match usage patterns"}	\N
64	164	64	AWS Storage Cost Management	Application	Design Cost-Optimized Architectures	A startup's S3 storage costs have grown to $25,000/month with 80% of objects never accessed after 90 days. They also have high data transfer costs from direct client downloads. The application serves user-uploaded images and documents globally. Which combination provides the most comprehensive cost reduction?	[{"text": "S3 lifecycle policy to Glacier and CloudFront distribution", "isCorrect": true}, {"text": "S3 Intelligent-Tiering and S3 Transfer Acceleration", "isCorrect": false}, {"text": "Move all data to S3 One Zone-IA with direct client access", "isCorrect": false}, {"text": "Compress all objects and implement client-side caching", "isCorrect": false}]	S3 lifecycle policy to Glacier and CloudFront distribution	Lifecycle policy to Glacier reduces storage costs by 68% for unused objects, while CloudFront eliminates data transfer costs and provides global acceleration.	{"summary": "Comprehensive S3 cost optimization:", "breakdown": ["Lifecycle to Glacier: 68% storage cost reduction for 80% of objects", "CloudFront: Eliminates data transfer costs from S3 to internet", "Global caching: Improves performance while reducing origin costs", "Combined savings: Approximately 70% total cost reduction"], "otherOptions": "Intelligent-Tiering adds overhead for predictable 90-day pattern\\nOne Zone-IA lacks resilience and doesn't address transfer costs\\nCompression helps but doesn't address core storage lifecycle and transfer costs"}	\N
92	191	91	Cost Optimization - Reserved Capacity Strategy	Expert	Design Cost-Optimized Architectures	A company runs diverse workloads: 40% steady-state (3-year predictable), 30% seasonal (6-month cycles), 20% development (weekdays only), 10% experimental (unpredictable). Current monthly costs are $200,000. Which purchasing strategy maximizes savings?	[{"text": "3-year Standard RIs for 40%, 1-year Convertible RIs for 30%, Scheduled RIs for 20%, On-Demand for 10%", "isCorrect": false}, {"text": "Compute Savings Plans for 70%, Spot Instances for 20%, On-Demand for 10%", "isCorrect": false}, {"text": "3-year Standard RIs for 40%, Compute Savings Plans for 30%, EC2 Instance Savings Plans for 20%, Spot for 10%", "isCorrect": true}, {"text": "All Reserved Instances with different term lengths", "isCorrect": false}]	3-year Standard RIs for 40%, Compute Savings Plans for 30%, EC2 Instance Savings Plans for 20%, Spot for 10%	This strategy optimizes for each workload pattern: Standard RIs for steady-state (maximum savings), Compute Savings Plans for seasonal flexibility, Instance Savings Plans for development consistency, Spot for experimental cost efficiency.	{"summary": "Optimized purchasing mix strategy:", "breakdown": ["Standard RIs (40%): Maximum 70% savings for predictable workloads", "Compute Savings Plans (30%): 66% savings with seasonal flexibility", "Instance Savings Plans (20%): Weekday development with family flexibility", "Spot Instances (10%): Up to 90% savings for fault-tolerant experiments"], "otherOptions": "Scheduled RIs are discontinued\\nSavings Plans alone don't maximize savings for steady-state workloads\\nRIs lack flexibility for seasonal and development workloads"}	\N
93	192	92	Cost Optimization - Data Transfer	Advanced	Design Cost-Optimized Architectures	A content platform transfers 500TB monthly: 60% to end users globally, 25% between AWS regions for processing, 15% to on-premises for archival. Current data transfer costs are $45,000/month. Which optimization provides maximum cost reduction?	[{"text": "CloudFront for user content, VPC Peering for inter-region, DirectConnect for on-premises", "isCorrect": true}, {"text": "S3 Transfer Acceleration for all transfers", "isCorrect": false}, {"text": "Regional S3 buckets with Cross-Region Replication", "isCorrect": false}, {"text": "Compress all data and use multiple smaller transfers", "isCorrect": false}]	CloudFront for user content, VPC Peering for inter-region, DirectConnect for on-premises	CloudFront eliminates internet transfer costs for 60% of traffic, VPC Peering reduces inter-region costs, DirectConnect provides predictable pricing for on-premises transfers.	{"summary": "Data transfer cost optimization strategy:", "breakdown": ["CloudFront (60%): Eliminates $27,000/month in internet transfer costs", "VPC Peering (25%): Reduces inter-region transfer by 50%", "DirectConnect (15%): Predictable pricing vs internet gateway", "Total savings: Approximately 70% reduction in transfer costs"], "otherOptions": "Transfer Acceleration adds cost and doesn't optimize for user downloads\\nCRR doesn't reduce transfer costs, adds replication costs\\nCompression helps but doesn't address underlying transfer pricing"}	\N
94	193	93	Cost Optimization - Serverless vs Container Economics	Expert	Design Cost-Optimized Architectures	A microservices platform runs 200 services with varying traffic patterns: 50 services get constant low traffic, 100 services have predictable business-hour spikes, 50 services have unpredictable traffic. Current container costs are $300,000/month. Which architecture mix optimizes costs?	[{"text": "All services migrate to Lambda for serverless benefits", "isCorrect": false}, {"text": "Fargate for all services with auto-scaling enabled", "isCorrect": false}, {"text": "Lambda for unpredictable traffic, Fargate Spot for spiky services, ECS EC2 Reserved for constant traffic", "isCorrect": true}, {"text": "EKS with cluster auto-scaling for all services", "isCorrect": false}]	Lambda for unpredictable traffic, Fargate Spot for spiky services, ECS EC2 Reserved for constant traffic	This hybrid approach optimizes for traffic patterns: Lambda eliminates idle costs for unpredictable traffic, Fargate Spot reduces costs for spiky workloads, Reserved instances provide maximum savings for constant traffic.	{"summary": "Hybrid architecture cost optimization:", "breakdown": ["Lambda (50 services): Pay-per-request eliminates idle time costs", "Fargate Spot (100 services): 70% savings for fault-tolerant spiky workloads", "ECS Reserved (50 services): 70% savings for predictable constant traffic", "Estimated total savings: 60% reduction from current $300,000"], "otherOptions": "Lambda cold starts and execution time limits problematic for all services\\nFargate On-Demand expensive for constant traffic\\nEKS has control plane costs and doesn't optimize for traffic patterns"}	\N
74	174	74	Cost Optimization - EC2 Purchasing Strategy	Intermediate	Design Cost-Optimized Architectures	A web application runs 24/7 with predictable baseline traffic and occasional spikes. Current setup uses 12 m5.large On-Demand instances. Which purchasing strategy provides maximum cost savings while maintaining availability?	[{"text": "Convert all instances to 3-year Reserved Instances", "isCorrect": false}, {"text": "Use 8 Reserved Instances + Auto Scaling with On-Demand for spikes", "isCorrect": true}, {"text": "Replace all instances with Spot Instances and diversified types", "isCorrect": false}, {"text": "Use Savings Plans for all compute requirements", "isCorrect": false}]	Use 8 Reserved Instances + Auto Scaling with On-Demand for spikes	Reserved Instances for baseline capacity provide significant savings (~70%), while Auto Scaling with On-Demand handles unpredictable traffic spikes.	{"summary": "Hybrid purchasing strategy benefits:", "breakdown": ["Reserved Instances: ~70% savings for baseline", "Auto Scaling: handles traffic variability", "On-Demand: flexibility for spikes", "Maintains availability: no interruption risk"], "otherOptions": "Over-provisioning with RIs wastes money during low traffic\\nSpot Instances can be interrupted, affecting availability\\nSavings Plans alone don't optimize for traffic patterns"}	\N
75	175	75	Cost Optimization - Storage Lifecycle	Advanced	Design Cost-Optimized Architectures	A backup system stores 200TB monthly. Data accessed frequently for 30 days, occasionally for 6 months, rarely for 2 years, then archived permanently. Which S3 storage strategy minimizes costs?	[{"text": "S3 Standard for all data with intelligent tiering", "isCorrect": false}, {"text": "S3 Standard  S3-IA (30 days)  S3 Glacier (6 months)  S3 Glacier Deep Archive (2 years)", "isCorrect": true}, {"text": "S3 Standard  S3 One Zone-IA (30 days)  S3 Glacier Deep Archive (6 months)", "isCorrect": false}, {"text": "S3 Intelligent-Tiering for all data regardless of access patterns", "isCorrect": false}]	S3 Standard  S3-IA (30 days)  S3 Glacier (6 months)  S3 Glacier Deep Archive (2 years)	This lifecycle progression optimizes costs by moving data through appropriate storage classes based on access patterns and retrieval requirements.	{"summary": "Optimal S3 lifecycle progression:", "breakdown": ["S3 Standard: frequent access (0-30 days)", "S3-IA: occasional access (30 days-6 months)", "S3 Glacier: rare access (6 months-2 years)", "S3 Deep Archive: long-term archival (2+ years)"], "otherOptions": "Intelligent-Tiering has monitoring costs for predictable patterns\\nSkips optimal intermediate storage classes\\nNot cost-effective for known access patterns"}	\N
10	110	10	AWS Security - Network Security	Application	Design Secure Applications	A three-tier web application (web, app, database) needs to be secured according to the principle of least privilege. The web tier must be accessible from the internet, app tier only from web tier, and database tier only from app tier. Which security group configuration is CORRECT?	[{"text": "Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306", "isCorrect": true}, {"text": "Web SG: 0.0.0.0/0:80,443 | App SG: 0.0.0.0/0:8080 | DB SG: 0.0.0.0/0:3306", "isCorrect": false}, {"text": "All tiers: VPC CIDR:All ports for internal communication", "isCorrect": false}, {"text": "Web SG: 0.0.0.0/0:All | App SG: VPC CIDR:All | DB SG: VPC CIDR:All", "isCorrect": false}]	Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306	Security groups should reference other security groups for internal communication, implementing true least privilege access between tiers.	{"summary": "Proper three-tier security group design:", "breakdown": ["Web tier: Internet access on standard web ports (80, 443)", "App tier: Only accepts traffic from web security group", "Database tier: Only accepts traffic from app security group", "Security group references: More secure than IP-based rules"], "otherOptions": "App and DB exposed to internet\\nToo broad access within VPC\\nOverly permissive on all tiers"}	\N
44	144	44	AWS S3 Deep Dive	Application	Design Secure Architectures	A healthcare organization stores patient records in S3 and needs to maintain full control over encryption keys while ensuring HIPAA compliance. They want to minimize key management overhead but need complete audit trails. Which encryption approach provides the BEST balance?	[{"text": "SSE-S3 with AWS-managed keys", "isCorrect": false}, {"text": "SSE-KMS with customer-managed keys", "isCorrect": true}, {"text": "SSE-C with customer-provided keys", "isCorrect": false}, {"text": "Client-side encryption with S3 Encryption Client", "isCorrect": false}]	SSE-KMS with customer-managed keys	SSE-KMS with customer-managed keys provides control over keys, audit trails via CloudTrail, and AWS handles key management infrastructure.	{"summary": "SSE-KMS benefits for HIPAA compliance:", "breakdown": ["Customer control: Create, rotate, and manage encryption keys", "Audit trails: CloudTrail logs all key usage and access", "AWS management: Infrastructure and availability handled by AWS", "HIPAA compliant: Meets regulatory requirements for key control"], "otherOptions": "AWS-managed keys don't provide sufficient customer control\\nSSE-C requires managing key storage and availability\\nClient-side encryption adds complexity without AWS integration benefits"}	\N
53	153	53	AWS Data Security Concepts	Application	Design Secure Architectures	A financial services company needs to automatically discover and protect sensitive data across 200 S3 buckets containing customer documents. They require automated classification, policy enforcement, and immediate alerts for data exposure risks. Which combination provides comprehensive data security automation?	[{"text": "AWS Config rules for bucket policies and CloudWatch for monitoring", "isCorrect": false}, {"text": "Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access", "isCorrect": true}, {"text": "AWS Inspector for vulnerability scanning and GuardDuty for threats", "isCorrect": false}, {"text": "Custom Lambda functions with CloudTrail for access monitoring", "isCorrect": false}]	Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access	Macie uses ML to discover sensitive data, provides automatic classification, EventBridge enables real-time alerts, and Block Public Access prevents exposure.	{"summary": "Automated data security components:", "breakdown": ["Amazon Macie: ML-powered sensitive data discovery and classification", "EventBridge integration: Real-time alerts for policy violations", "S3 Block Public Access: Prevents accidental public exposure", "Automated findings: Continuous monitoring across all 200 buckets"], "otherOptions": "Config and CloudWatch don't provide sensitive data discovery\\nInspector and GuardDuty focus on infrastructure, not data classification\\nCustom solutions require significant development and maintenance"}	\N
54	154	54	AWS Data Security Concepts	Expert	Design Secure Architectures	A healthcare organization must implement data classification and protection for patient records across multiple AWS services (S3, RDS, DynamoDB). They need automated discovery of PHI, policy-based access controls, and audit trails meeting HIPAA requirements. Which architecture provides comprehensive data protection?	[{"text": "Amazon Macie for S3, database activity streams for RDS, and VPC Flow Logs", "isCorrect": false}, {"text": "Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services", "isCorrect": true}, {"text": "AWS Config for compliance monitoring and GuardDuty for threat detection", "isCorrect": false}, {"text": "Custom data loss prevention (DLP) solution with third-party tools", "isCorrect": false}]	Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services	Macie discovers PHI in S3, CloudTrail data events provide comprehensive audit trails, and customer-managed KMS keys ensure encryption control across all services.	{"summary": "HIPAA-compliant data protection architecture:", "breakdown": ["Macie: Automated PHI discovery and classification in S3", "CloudTrail data events: Complete audit trail of data access", "Customer-managed KMS: Encryption control across S3, RDS, DynamoDB", "Policy-based access: IAM policies with resource-based conditions"], "otherOptions": "Missing encryption control and comprehensive audit coverage\\nConfig and GuardDuty don't provide PHI discovery and classification\\nCustom solutions require extensive compliance validation"}	\N
55	155	55	AWS Encryption	Expert	Design Secure Architectures	A multinational corporation needs end-to-end encryption for data stored across AWS services in multiple regions. They require customer control over encryption keys, automatic rotation, cross-region access, and integration with their existing HSM infrastructure. Which encryption strategy provides the MOST comprehensive solution?	[{"text": "AWS KMS customer-managed keys with automatic rotation in each region", "isCorrect": false}, {"text": "AWS CloudHSM cluster with custom key management application", "isCorrect": true}, {"text": "Client-side encryption with application-managed keys", "isCorrect": false}, {"text": "AWS KMS with imported key material from on-premises HSM", "isCorrect": false}]	AWS CloudHSM cluster with custom key management application	CloudHSM provides dedicated hardware security modules with full customer control, integrates with existing HSM infrastructure, and supports cross-region key management.	{"summary": "CloudHSM comprehensive encryption benefits:", "breakdown": ["Dedicated HSM: Full customer control over encryption operations", "FIPS 140-2 Level 3: Highest security certification available", "Cross-region clustering: Keys available across multiple regions", "HSM integration: Compatible with existing on-premises HSM infrastructure"], "otherOptions": "KMS keys are region-specific and don't integrate with existing HSM\\nApplication-managed keys lack HSM security and are difficult to manage at scale\\nImported key material in KMS doesn't provide full HSM integration"}	\N
56	156	56	AWS Encryption	Application	Design Secure Architectures	A development team needs to encrypt application data before storing it in DynamoDB. They want to minimize performance impact while ensuring field-level encryption for sensitive columns (SSN, credit card numbers) but allow searching on encrypted email addresses. Which encryption approach provides the BEST balance of security and functionality?	[{"text": "DynamoDB encryption at rest with AWS managed keys", "isCorrect": false}, {"text": "AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields", "isCorrect": true}, {"text": "Application-level AES encryption for all fields", "isCorrect": false}, {"text": "AWS KMS encryption for entire DynamoDB table", "isCorrect": false}]	AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields	DynamoDB Encryption Client provides field-level encryption with deterministic encryption for searchable fields and probabilistic encryption for maximum security on sensitive data.	{"summary": "Field-level encryption strategy:", "breakdown": ["Deterministic encryption: Same email always produces same ciphertext (searchable)", "Probabilistic encryption: Different ciphertext each time (maximum security)", "Client-side encryption: Data encrypted before sending to DynamoDB", "Performance optimized: Only sensitive fields encrypted, not entire records"], "otherOptions": "Table-level encryption doesn't provide field-level control\\nApplication encryption without library optimization impacts performance\\nKMS table encryption encrypts all data, doesn't enable selective searching"}	\N
57	157	57	AWS Governance Compliance	Expert	Design Secure Architectures	A regulated financial institution must demonstrate continuous compliance with SOC 2 requirements for data storage. They need automated policy enforcement, compliance reporting, and evidence collection across 50 AWS accounts. Non-compliant resources must be automatically remediated. Which governance framework provides comprehensive compliance automation?	[{"text": "AWS Config with custom rules and SNS notifications", "isCorrect": false}, {"text": "AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation", "isCorrect": true}, {"text": "AWS CloudFormation with compliance templates", "isCorrect": false}, {"text": "AWS Security Hub with manual remediation workflows", "isCorrect": false}]	AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation	Organizations provides account governance, Conformance Packs enable SOC 2 compliance checks, and Systems Manager Automation handles remediation across all accounts.	{"summary": "Comprehensive compliance automation:", "breakdown": ["Organizations + SCPs: Account-level policy enforcement", "Config Conformance Packs: Pre-built SOC 2 compliance rules", "Systems Manager Automation: Automatic remediation of non-compliant resources", "Centralized reporting: Compliance status across all 50 accounts"], "otherOptions": "Config alone lacks account-level governance and automated remediation\\nCloudFormation provides deployment compliance, not ongoing monitoring\\nSecurity Hub requires manual remediation, not automated"}	\N
58	158	58	AWS Governance Compliance	Application	Design Secure Architectures	A global company must ensure S3 buckets across all regions comply with data residency requirements. EU data must stay in EU regions, US data in US regions, with automatic policy enforcement and compliance reporting. Which solution provides the MOST effective governance?	[{"text": "IAM policies restricting S3 actions based on user location", "isCorrect": false}, {"text": "S3 bucket policies with aws:RequestedRegion condition keys", "isCorrect": false}, {"text": "AWS Organizations SCPs with region restrictions based on data classification tags", "isCorrect": true}, {"text": "AWS Config rules monitoring bucket creation across regions", "isCorrect": false}]	AWS Organizations SCPs with region restrictions based on data classification tags	Organizations SCPs provide account-level enforcement that cannot be overridden, with tag-based conditions enabling automatic data residency compliance.	{"summary": "Data residency governance with SCPs:", "breakdown": ["SCPs: Cannot be overridden by account-level permissions", "Tag-based conditions: Automatically enforce based on data classification", "Region restrictions: Prevent EU data creation in non-EU regions", "Organization-wide: Applies to all current and future accounts"], "otherOptions": "IAM policies can be overridden by account administrators\\nBucket policies only apply after bucket creation\\nConfig rules detect violations but don't prevent them"}	\N
67	167	67	Security - IAM Cross-Account Access	Intermediate	Design Secure Architectures	A development team needs access to production AWS resources from their development account. The security team requires auditable, temporary access with no permanent credentials. Which solution provides the MOST secure approach?	[{"text": "Create IAM users in production account with temporary passwords", "isCorrect": false}, {"text": "Configure cross-account IAM roles with AssumeRole permissions", "isCorrect": true}, {"text": "Share access keys between accounts using AWS Secrets Manager", "isCorrect": false}, {"text": "Use AWS SSO with permanent group assignments", "isCorrect": false}]	Configure cross-account IAM roles with AssumeRole permissions	Cross-account roles provide temporary credentials via STS, enable auditing through CloudTrail, and follow security best practices. The trust policy defines who can assume the role, while permissions policy controls what they can do.	{"summary": "Cross-account roles for secure access:", "breakdown": ["Temporary credentials via STS", "Complete audit trail in CloudTrail", "No permanent credentials to manage", "Trust policy controls who can assume role"], "otherOptions": "Users still require permanent management\\nAccess keys are permanent credentials\\nPermanent access violates least privilege"}	\N
78	178	78	Security - Secrets Rotation	Advanced	Design Secure Architectures	A microservices application uses 20+ different databases with rotating credentials every 7 days. The application experiences downtime during credential rotation. Which solution provides zero-downtime credential rotation?	[{"text": "Use AWS Secrets Manager with automatic rotation and version staging", "isCorrect": true}, {"text": "Store credentials in Parameter Store with manual rotation scripts", "isCorrect": false}, {"text": "Use IAM roles with temporary credentials for database access", "isCorrect": false}, {"text": "Implement client-side credential caching with TTL expiration", "isCorrect": false}]	Use AWS Secrets Manager with automatic rotation and version staging	Secrets Manager supports multi-version secrets during rotation (AWSPENDING, AWSCURRENT) allowing applications to gracefully transition to new credentials without downtime.	{"summary": "Zero-downtime rotation with Secrets Manager:", "breakdown": ["Version staging: AWSPENDING and AWSCURRENT versions available simultaneously", "Automatic rotation: Lambda-based rotation functions", "Application compatibility: Gradual transition between credential versions", "Multiple databases: Supports various database engines"], "otherOptions": "Parameter Store lacks automatic rotation capabilities\\nIAM roles don't provide database credentials\\nClient caching doesn't solve rotation downtime"}	\N
69	169	69	Security - Zero Trust Architecture	Expert	Design Secure Architectures	A financial services company implements zero trust where every request must be verified regardless of source. Microservices communicate across VPCs and on-premises. Which combination provides comprehensive zero trust? (Choose THREE)	[{"text": "AWS PrivateLink for service-to-service communication", "isCorrect": true}, {"text": "IAM roles for service authentication with short-lived tokens", "isCorrect": true}, {"text": "VPC security groups allowing traffic from any source", "isCorrect": false}, {"text": "AWS Certificate Manager for mutual TLS authentication", "isCorrect": true}, {"text": "Shared service accounts across all microservices", "isCorrect": false}]	AWS PrivateLink for service-to-service communication, IAM roles for service authentication with short-lived tokens, AWS Certificate Manager for mutual TLS authentication	Zero trust requires: private connectivity (PrivateLink), verified identity (IAM roles), and encrypted communication (mTLS) for every interaction.	{"summary": "Zero trust implementation components:", "breakdown": ["PrivateLink: Private service connectivity", "IAM roles: Verified service identity", "Short-lived tokens: Limit credential exposure", "mTLS: Encrypted and authenticated communication"], "otherOptions": "Open security groups violate zero trust principles\\nShared accounts prevent proper identity verification"}	\N
80	180	80	Security - Network Micro-segmentation	Advanced	Design Secure Architectures	A financial application requires micro-segmentation where each microservice can only communicate with specific other services. Traditional security groups become unmanageable with 50+ services. Which approach provides scalable micro-segmentation?	[{"text": "AWS App Mesh with service-to-service mTLS and traffic policies", "isCorrect": true}, {"text": "VPC security groups with service discovery automation", "isCorrect": false}, {"text": "Network ACLs with subnet-level service isolation", "isCorrect": false}, {"text": "WAF rules with API Gateway for each microservice", "isCorrect": false}]	AWS App Mesh with service-to-service mTLS and traffic policies	App Mesh provides service mesh architecture with automatic mTLS, traffic policies for fine-grained access control, and service discovery integration for scalable micro-segmentation.	{"summary": "Service mesh micro-segmentation benefits:", "breakdown": ["Automatic mTLS: Service-to-service encryption and authentication", "Traffic policies: Fine-grained communication rules", "Service discovery: Dynamic service endpoint management", "Scalability: Manages 50+ service communications efficiently"], "otherOptions": "Security groups don't scale well for complex service-to-service communication\\nNetwork ACLs too coarse-grained for microservice communication\\nWAF operates at HTTP level, not service mesh communication"}	\N
81	181	81	Security - Compliance Automation	Expert	Design Secure Architectures	A multinational bank must demonstrate continuous compliance with SOX, PCI-DSS, and GDPR across 200 AWS accounts. They need automated evidence collection, policy enforcement, and violation remediation. Which solution provides comprehensive compliance automation?	[{"text": "AWS Audit Manager + Config Conformance Packs + Security Hub + Systems Manager Automation", "isCorrect": true}, {"text": "CloudTrail + Config + CloudWatch + Manual compliance reports", "isCorrect": false}, {"text": "Well-Architected Tool + Trusted Advisor + Cost Explorer", "isCorrect": false}, {"text": "Inspector + GuardDuty + Macie + Custom Lambda functions", "isCorrect": false}]	AWS Audit Manager + Config Conformance Packs + Security Hub + Systems Manager Automation	Audit Manager automates evidence collection for SOX/PCI/GDPR, Conformance Packs implement compliance rules, Security Hub centralizes findings, Systems Manager automates remediation.	{"summary": "Multi-regulation compliance automation:", "breakdown": ["Audit Manager: Automated evidence collection for SOX, PCI-DSS, GDPR", "Conformance Packs: Pre-built compliance rule sets", "Security Hub: Centralized compliance dashboard", "Systems Manager: Automated remediation of non-compliant resources"], "otherOptions": "Manual processes don't scale for 200 accounts\\nThese tools don't address compliance requirements\\nSecurity tools but lack compliance framework integration"}	\N
82	182	82	Security - Advanced Threat Detection	Advanced	Design Secure Architectures	A SaaS platform experiences sophisticated attacks including credential stuffing, API abuse, and account takeovers. They need behavioral analysis and adaptive security responses. Which solution provides the MOST comprehensive threat detection?	[{"text": "GuardDuty + Detective + Security Hub with custom response automation", "isCorrect": true}, {"text": "WAF + Shield + CloudTrail with manual analysis", "isCorrect": false}, {"text": "Config + Inspector + Trusted Advisor with reporting", "isCorrect": false}, {"text": "VPC Flow Logs + CloudWatch + SNS with basic alerting", "isCorrect": false}]	GuardDuty + Detective + Security Hub with custom response automation	GuardDuty detects threats using ML, Detective provides behavioral analysis and investigation graphs, Security Hub correlates findings, automation enables adaptive responses.	{"summary": "Advanced threat detection and response:", "breakdown": ["GuardDuty: ML-based threat detection for credential stuffing and API abuse", "Detective: Behavioral analysis and investigation workflows", "Security Hub: Threat correlation and centralized management", "Response automation: Adaptive security measures based on threat intelligence"], "otherOptions": "WAF and Shield address different attack vectors, lack behavioral analysis\\nThese services focus on compliance, not threat detection\\nBasic monitoring without advanced threat detection capabilities"}	\N
47	147	47	AWS EBS Deep Dive	Application	Design Resilient Architectures	A production database uses EBS volumes and requires point-in-time recovery capability with minimal data loss. The current snapshot strategy creates daily snapshots, but the RPO requirement is 4 hours. Which enhancement provides the BEST backup strategy improvement?	[{"text": "Increase snapshot frequency to every 4 hours using scheduled snapshots", "isCorrect": true}, {"text": "Enable EBS Fast Snapshot Restore on existing daily snapshots", "isCorrect": false}, {"text": "Configure EBS Multi-Attach to create live replicas", "isCorrect": false}, {"text": "Use AWS Backup service with continuous backup enabled", "isCorrect": false}]	Increase snapshot frequency to every 4 hours using scheduled snapshots	Scheduling snapshots every 4 hours directly meets the RPO requirement by ensuring maximum data loss is limited to 4 hours.	{"summary": "Snapshot strategy for 4-hour RPO:", "breakdown": ["Scheduled snapshots: Every 4 hours meets exact RPO requirement", "Incremental backups: Only changed blocks stored, cost-effective", "Point-in-time recovery: Can restore to any snapshot timestamp", "Automated lifecycle: Can retain snapshots based on retention policy"], "otherOptions": "Fast Snapshot Restore improves RTO, not RPO\\nMulti-Attach doesn't provide backup functionality\\nContinuous backup overkill for 4-hour RPO requirement"}	\N
59	159	59	AWS Storage Resiliency Scalability	Expert	Design Resilient Architectures	A video streaming platform stores 1PB of content with global distribution requirements. They need 99.99% availability, automatic failover, and the ability to handle 10x traffic spikes during major events. Current single-region S3 architecture shows latency issues for international users. Which architecture provides optimal resiliency and global performance?	[{"text": "S3 Cross-Region Replication with CloudFront distribution", "isCorrect": true}, {"text": "Multi-region S3 buckets with Route 53 latency-based routing", "isCorrect": false}, {"text": "S3 Transfer Acceleration with single-region storage", "isCorrect": false}, {"text": "EFS with VPC peering across regions", "isCorrect": false}]	S3 Cross-Region Replication with CloudFront distribution	S3 CRR provides data resilience and regional availability, while CloudFront edge locations ensure global low-latency access and can handle traffic spikes automatically.	{"summary": "Global content delivery architecture:", "breakdown": ["S3 CRR: Automatic replication across regions for resilience", "CloudFront: 400+ edge locations for global low-latency delivery", "Auto-scaling: CloudFront handles 10x traffic spikes automatically", "99.99% availability: S3 + CloudFront combined SLA exceeds requirement"], "otherOptions": "Route 53 doesn't provide edge caching for content delivery\\nTransfer Acceleration only helps uploads, not global content delivery\\nEFS not designed for 1PB content delivery workloads"}	\N
84	183	83	Resilience - Chaos Engineering	Expert	Design Resilient Architectures	A streaming service wants to implement chaos engineering to test system resilience. They need to simulate various failure scenarios including AZ outages, service degradation, and network partitions. Which approach provides systematic chaos engineering capabilities?	[{"text": "AWS Fault Injection Simulator with pre-built experiment templates", "isCorrect": true}, {"text": "Manual instance termination and service disruption scripts", "isCorrect": false}, {"text": "Auto Scaling stress testing with load generation tools", "isCorrect": false}, {"text": "Blue-green deployments with rollback procedures", "isCorrect": false}]	AWS Fault Injection Simulator with pre-built experiment templates	AWS FIS provides controlled chaos engineering experiments with built-in safety mechanisms, experiment templates for common failure scenarios, and integration with AWS services.	{"summary": "Systematic chaos engineering with AWS FIS:", "breakdown": ["Controlled experiments: Safe failure injection with stop conditions", "Pre-built templates: Common failure scenarios like AZ outages", "Service integration: Works with EC2, ECS, RDS, and other AWS services", "Monitoring integration: Tracks system behavior during experiments"], "otherOptions": "Manual approaches lack safety controls and systematic methodology\\nLoad testing doesn't simulate infrastructure failures\\nBlue-green deployments are for safe releases, not failure testing"}	\N
85	184	84	Resilience - Database Failover	Advanced	Design Resilient Architectures	A financial trading application requires automatic database failover with zero data loss and sub-60 second RTO. The database processes 100,000 transactions per second. Which solution provides the BEST performance and reliability?	[{"text": "Aurora Global Database with cross-region automated failover", "isCorrect": true}, {"text": "RDS Multi-AZ with read replicas and manual promotion", "isCorrect": false}, {"text": "DynamoDB with Global Tables and eventual consistency", "isCorrect": false}, {"text": "Self-managed PostgreSQL with streaming replication", "isCorrect": false}]	Aurora Global Database with cross-region automated failover	Aurora Global Database provides synchronous replication within region (zero data loss), automatic failover in <60 seconds, and can handle high transaction volumes with read replica scaling.	{"summary": "High-performance database failover:", "breakdown": ["Synchronous replication: Zero data loss within region", "Automatic failover: Sub-60 second RTO with health monitoring", "High throughput: Designed for 100,000+ TPS workloads", "Global availability: Cross-region disaster recovery"], "otherOptions": "Manual promotion exceeds 60-second RTO requirement\\nEventual consistency inappropriate for financial trading\\nSelf-managed solutions require complex operational overhead"}	\N
86	185	85	Resilience - Circuit Breaker Pattern	Advanced	Design Resilient Architectures	A microservices application experiences cascading failures when one service becomes unavailable. Dependencies continue to call the failing service, causing system-wide outages. Which implementation provides the BEST circuit breaker pattern?	[{"text": "Application Load Balancer with health checks and connection draining", "isCorrect": false}, {"text": "AWS App Mesh with circuit breaker and retry policies", "isCorrect": true}, {"text": "API Gateway with throttling and request timeout settings", "isCorrect": false}, {"text": "Lambda functions with SQS dead letter queues", "isCorrect": false}]	AWS App Mesh with circuit breaker and retry policies	App Mesh provides service mesh capabilities with built-in circuit breaker patterns, configurable failure thresholds, automatic service isolation, and intelligent retry policies.	{"summary": "Service mesh circuit breaker implementation:", "breakdown": ["Circuit breaker: Automatic isolation of failing services", "Failure detection: Configurable thresholds and timeouts", "Graceful degradation: Prevents cascading failures", "Retry policies: Intelligent backoff and circuit restoration"], "otherOptions": "Load balancers provide basic health checks, not circuit breaker patterns\\nAPI Gateway throttling doesn't implement circuit breaker logic\\nDead letter queues handle failed messages, not circuit breaking"}	\N
87	186	86	Resilience - Multi-Region Backup Strategy	Expert	Design Resilient Architectures	A global SaaS platform stores 50TB of customer data daily across multiple regions. They need automated backup with RPO of 15 minutes, cross-region replication, and long-term retention (7 years) for compliance. Which solution provides optimal backup resilience?	[{"text": "AWS Backup with cross-region copy, lifecycle policies, and compliance reports", "isCorrect": true}, {"text": "EBS snapshots with Lambda automation and S3 replication", "isCorrect": false}, {"text": "Database native backup tools with manual cross-region copy", "isCorrect": false}, {"text": "Third-party backup software with tape gateway integration", "isCorrect": false}]	AWS Backup with cross-region copy, lifecycle policies, and compliance reports	AWS Backup provides centralized backup across services, automated cross-region replication, lifecycle management for long-term retention, and compliance reporting for audit requirements.	{"summary": "Comprehensive backup strategy components:", "breakdown": ["Centralized backup: Manages EC2, RDS, DynamoDB, EFS across regions", "15-minute RPO: Continuous and frequent backup scheduling", "Cross-region copy: Automated replication for disaster recovery", "7-year retention: Lifecycle policies with compliance reporting"], "otherOptions": "Manual automation doesn't scale for 50TB daily across multiple services\\nManual processes don't meet 15-minute RPO requirements\\nThird-party solutions add complexity without native AWS integration"}	\N
71	171	71	Resilience - Multi-AZ High Availability	Advanced	Design Resilient Architectures	A critical web application must survive an entire AZ failure with zero data loss and < 2 minute recovery time. Current architecture: ALB  EC2  RDS MySQL. Which modification provides the highest availability?	[{"text": "Auto Scaling Group across 3 AZs + RDS Multi-AZ + ElastiCache", "isCorrect": true}, {"text": "Single large EC2 instance + RDS with automated backups", "isCorrect": false}, {"text": "Auto Scaling Group in 1 AZ + RDS read replicas across AZs", "isCorrect": false}, {"text": "Lambda functions + DynamoDB Global Tables", "isCorrect": false}]	Auto Scaling Group across 3 AZs + RDS Multi-AZ + ElastiCache	Multi-AZ Auto Scaling provides instance-level resilience, RDS Multi-AZ offers automatic failover with zero data loss, and ElastiCache reduces database load during recovery scenarios.	{"summary": "Multi-AZ high availability design:", "breakdown": ["Auto Scaling Group: automatic instance replacement across AZs", "RDS Multi-AZ: synchronous replication with automatic failover", "Zero data loss: synchronous database replication", "Sub-2 minute recovery: automated failover processes"], "otherOptions": "Single instance creates single point of failure\\nSingle AZ Auto Scaling doesn't protect against AZ failure\\nMajor architecture change, may not meet recovery time"}	\N
95	196	96	Database High Availability	Intermediate	Design Resilient Architectures	A financial application uses Amazon RDS MySQL with Multi-AZ deployment. During a planned maintenance window, AWS performs a failover to the standby instance. How long should the application expect to be unavailable, and what happens to the database endpoint?	[{"text": "The application experiences 60-120 seconds of downtime while DNS propagates to the standby instance; the endpoint remains unchanged", "isCorrect": true}, {"text": "The application must update its connection string to point to the new primary instance after failover", "isCorrect": false}, {"text": "The failover is instantaneous with zero downtime because Multi-AZ uses synchronous replication", "isCorrect": false}, {"text": "The application experiences 5-10 minutes of downtime while the standby instance is promoted and initialized", "isCorrect": false}]	The application experiences 60-120 seconds of downtime while DNS propagates to the standby instance; the endpoint remains unchanged	RDS Multi-AZ failover typically completes in 60-120 seconds. During this time, AWS automatically updates the DNS record to point to the standby instance, which is promoted to primary. The database endpoint (connection string) remains unchanged, so applications automatically reconnect to the new primary without code changes. The standby instance is already running and synchronized, so no lengthy initialization is required.	{"summary": "Multi-AZ failover takes 60-120 seconds with automatic DNS update to standby instance", "breakdown": ["Synchronous replication: Standby is always up-to-date (zero data loss)", "DNS-based failover: Endpoint stays same, DNS record updated automatically", "Typical failover time: 60-120 seconds for DNS propagation and connection re-establishment", "Application reconnection: Most DB clients automatically retry failed connections", "No endpoint change: Applications don't need configuration updates", "Standby promotion: New primary immediately accepts connections", "Health checks: RDS monitors primary and triggers failover on failure"], "otherOptions": "Applications don't need endpoint updates (DNS handled automatically); Failover isn't instantaneous (60-120s required); 5-10 minutes is too long for standard failover"}	t
96	197	97	Storage Optimization	Advanced	Design Cost-Optimized Architectures	A media company stores 500TB of video content in S3 Standard. Analysis shows that 80% of videos are not accessed after 30 days, and 95% are never accessed after 90 days. They need to retain all content for 7 years for compliance. Which lifecycle policy provides maximum cost savings while meeting requirements?	[{"text": "Transition to S3 Standard-IA after 30 days, then to S3 Glacier Flexible Retrieval after 90 days, then to S3 Glacier Deep Archive after 1 year", "isCorrect": true}, {"text": "Transition to S3 Intelligent-Tiering immediately and let AWS automatically optimize storage classes", "isCorrect": false}, {"text": "Transition to S3 One Zone-IA after 30 days, then to S3 Glacier Deep Archive after 90 days", "isCorrect": false}, {"text": "Keep all content in S3 Standard and enable S3 Lifecycle policies for automatic deletion after 7 years", "isCorrect": false}]	Transition to S3 Standard-IA after 30 days, then to S3 Glacier Flexible Retrieval after 90 days, then to S3 Glacier Deep Archive after 1 year	This multi-tier approach maximizes savings by matching storage class to access patterns. Standard-IA (30+ days) reduces costs by ~50% for infrequently accessed data while maintaining quick retrieval. Glacier Flexible Retrieval (90+ days) saves ~80% for rarely accessed data. Glacier Deep Archive (1+ years) offers 95% cost reduction for long-term compliance storage. This strategy balances cost savings with retrieval capabilities based on actual usage patterns.	{"summary": "Multi-tier lifecycle transitions optimize costs by matching storage class to access patterns", "breakdown": ["Days 0-30: S3 Standard ($0.023/GB) - frequent access period", "Days 30-90: Standard-IA ($0.0125/GB) - 46% cost reduction", "Days 90-365: Glacier Flexible ($0.0036/GB) - 84% cost reduction", "Years 1-7: Glacier Deep Archive ($0.00099/GB) - 96% cost reduction", "Total estimated savings: ~85% over 7 years vs keeping in Standard", "Compliance maintained: All content retained for required 7 years", "Retrieval times: Standard-IA (milliseconds), Glacier (minutes-hours), Deep Archive (12-48 hours)"], "otherOptions": "Intelligent-Tiering has monitoring fees and may not be optimal for predictable patterns; One Zone-IA lacks durability for compliance data; Keeping in Standard wastes 85% in unnecessary costs"}	t
97	198	98	Network Architecture	Advanced	Design High-Performing Architectures	A company has 15 VPCs across multiple AWS regions that need to communicate with each other. They anticipate adding 10 more VPCs over the next year. Currently using VPC peering, they are experiencing management complexity. What is the PRIMARY advantage of migrating to AWS Transit Gateway?	[{"text": "Transit Gateway eliminates the need for VPC peering connections, reducing from 105 peering connections to a hub-and-spoke model with centralized routing", "isCorrect": true}, {"text": "Transit Gateway provides faster network performance between VPCs compared to VPC peering", "isCorrect": false}, {"text": "Transit Gateway automatically encrypts all inter-VPC traffic without requiring additional configuration", "isCorrect": false}, {"text": "Transit Gateway reduces data transfer costs by 50% compared to VPC peering", "isCorrect": false}]	Transit Gateway eliminates the need for VPC peering connections, reducing from 105 peering connections to a hub-and-spoke model with centralized routing	With 15 VPCs, full mesh VPC peering requires n(n-1)/2 = 105 connections. Adding 10 more VPCs would require 300 connections total. Transit Gateway uses a hub-and-spoke model where each VPC connects once to the central gateway, reducing to 15 connections (eventually 25). This dramatically simplifies routing, management, and scaling while providing centralized route control and inter-region connectivity via Transit Gateway peering.	{"summary": "Transit Gateway replaces complex mesh topology with centralized hub-and-spoke architecture", "breakdown": ["VPC Peering mesh: 15 VPCs = 105 connections; 25 VPCs = 300 connections (scales at n)", "Transit Gateway: 15 VPCs = 15 attachments; 25 VPCs = 25 attachments (scales linearly)", "Management reduction: Single routing table vs hundreds of route entries", "Centralized control: One place to manage inter-VPC routing policies", "Transitive routing: Supported (not possible with VPC peering)", "Multi-region support: Transit Gateway peering for cross-region connectivity", "Scaling: Can attach up to 5,000 VPCs per gateway"], "otherOptions": "Performance is comparable, not faster; Encryption is same as peering (not automatic advantage); Data transfer costs are similar, not reduced by 50%"}	t
98	199	99	Serverless Compute	Intermediate	Design High-Performing Architectures	A company's AWS Lambda function processes incoming webhooks from a third-party service. During peak hours, they receive errors indicating "Rate Exceeded" and "Service Unavailable." What is the MOST likely cause, and what should be implemented to resolve it?	[{"text": "The Lambda function is reaching the account-level concurrent execution limit; implement reserved concurrency for the function and request a limit increase from AWS Support", "isCorrect": true}, {"text": "The Lambda function timeout is too short; increase the timeout value to allow longer execution times", "isCorrect": false}, {"text": "The API Gateway throttling limit is too restrictive; increase the API Gateway rate limits", "isCorrect": false}, {"text": "The Lambda function memory allocation is insufficient; increase memory to improve performance and reduce errors", "isCorrect": false}]	The Lambda function is reaching the account-level concurrent execution limit; implement reserved concurrency for the function and request a limit increase from AWS Support	AWS Lambda has a default account-level concurrent execution limit of 1,000 across all functions in a region. "Rate Exceeded" errors indicate the function is hitting this limit during peak loads. Reserved concurrency guarantees a portion of the account limit for critical functions, preventing other functions from consuming all available concurrency. For sustained high-volume workloads, request a limit increase through AWS Support to raise the regional concurrency limit.	{"summary": "Lambda concurrency limits cause throttling; use reserved concurrency and request limit increases", "breakdown": ["Account limit: Default 1,000 concurrent executions per region", "Reserved concurrency: Guarantees capacity for critical functions", "Unreserved pool: Minimum 100 concurrency always available for other functions", "Rate exceeded error: Indicates concurrency limit reached", "Throttling behavior: New invocations rejected when limit exceeded", "Async invocations: Automatically retried (may cause delays)", "Sync invocations: Return 429 error (caller must retry)", "Solution: Set reserved concurrency + request permanent limit increase"], "otherOptions": "Timeout doesn't cause Rate Exceeded errors; API Gateway throttling shows different error messages; Memory allocation affects performance, not concurrency limits"}	t
99	200	100	Storage Performance	Intermediate	Design High-Performing Architectures	A database application requires 20,000 IOPS with consistent low latency for transaction processing. The database size is 500GB. Which EBS volume type provides the MOST cost-effective solution while meeting performance requirements?	[{"text": "Provisioned IOPS SSD (io2) with 20,000 IOPS provisioned", "isCorrect": false}, {"text": "General Purpose SSD (gp3) with 16,000 IOPS included baseline and 4,000 IOPS provisioned", "isCorrect": true}, {"text": "Throughput Optimized HDD (st1) with enhanced networking enabled", "isCorrect": false}, {"text": "General Purpose SSD (gp2) with 1,500 IOPS baseline (3 IOPS per GB  500GB)", "isCorrect": false}]	General Purpose SSD (gp3) with 16,000 IOPS included baseline and 4,000 IOPS provisioned	GP3 volumes provide 3,000 IOPS baseline (sufficient for most workloads) and allow provisioning up to 16,000 IOPS at no extra cost. Additional IOPS can be provisioned beyond 16,000 at lower cost than io2. For this scenario, provisioning 20,000 IOPS on gp3 costs significantly less than io2 while providing the same performance. GP3 also offers consistent low latency suitable for transactional databases, making it the most cost-effective choice.	{"summary": "GP3 offers best cost-performance ratio for most database workloads under 64,000 IOPS", "breakdown": ["GP3 pricing: $0.08/GB + $0.005/provisioned IOPS over 3,000", "GP3 for 20,000 IOPS: (500GB  $0.08) + (17,000  $0.005) = $40 + $85 = $125/month", "io2 pricing: $0.125/GB + $0.065/IOPS = (500  $0.125) + (20,000  $0.065) = $62.50 + $1,300 = $1,362.50/month", "Cost savings: GP3 is ~91% cheaper than io2 for this workload", "Performance: Both provide consistent sub-millisecond latency", "GP3 max: 16,000 IOPS included, up to 64,000 total provisioned", "Use io2 when: Need >64,000 IOPS or 99.999% durability"], "otherOptions": "io2 is unnecessarily expensive for this requirement; st1 is for throughput, not IOPS; gp2 only provides 1,500 IOPS (insufficient)"}	t
100	201	101	Content Delivery - High Availability	Advanced	Design Resilient Architectures	A media streaming company uses CloudFront to deliver video content from an S3 bucket. They want to implement automatic failover to a secondary S3 bucket in another region if the primary origin becomes unavailable. What configuration is required?	[{"text": "Create a CloudFront origin group with primary and secondary origins, configure health checks for the primary origin", "isCorrect": true}, {"text": "Set up S3 Cross-Region Replication and use Route 53 failover routing to switch between origins", "isCorrect": false}, {"text": "Enable S3 Transfer Acceleration on both buckets and configure CloudFront to use the fastest origin", "isCorrect": false}, {"text": "Configure multiple CloudFront distributions with Route 53 health checks to switch DNS between distributions", "isCorrect": false}]	Create a CloudFront origin group with primary and secondary origins, configure health checks for the primary origin	CloudFront origin groups provide native high availability by automatically failing over to a secondary origin when the primary origin fails health checks or returns specific HTTP status codes (500, 502, 503, 504, 404, 403). This happens transparently at the CloudFront edge without requiring DNS changes or manual intervention. You configure an origin group with primary and secondary origins (S3 buckets), and CloudFront automatically routes requests to the secondary when the primary is unhealthy.	{"summary": "CloudFront origin groups enable automatic failover between primary and secondary origins", "breakdown": ["Origin group: Contains 2 origins (primary and secondary)", "Failover triggers: HTTP 500, 502, 503, 504 status codes from primary", "Health checks: CloudFront monitors origin health automatically", "Automatic failover: Transparent to end users (no DNS change required)", "Failback: Returns to primary when health restored", "Configuration: Set up via CloudFront console or API", "Secondary origin: Can be S3 bucket in different region", "Best practice: Use S3 CRR to keep secondary bucket synchronized"], "otherOptions": "Route 53 failover adds unnecessary complexity and slower DNS TTL; Transfer Acceleration is for upload performance, not failover; Multiple distributions require DNS switching (slower)"}	t
101	202	102	Database Design - Performance	Advanced	Design High-Performing Architectures	An e-commerce application stores order data in DynamoDB with UserID as the partition key and OrderDate as the sort key. During flash sales, a few power users place hundreds of orders, causing throttling issues. What is the BEST solution to prevent hot partitions while maintaining query efficiency?	[{"text": "Add a random suffix (1-10) to UserID to distribute writes across multiple partitions, and query all suffixes to retrieve user orders", "isCorrect": false}, {"text": "Change the partition key to OrderID (UUID) and create a Global Secondary Index (GSI) with UserID as the partition key", "isCorrect": true}, {"text": "Use OrderDate as the partition key and UserID as the sort key to distribute writes over time", "isCorrect": false}, {"text": "Enable DynamoDB auto scaling and increase provisioned write capacity units", "isCorrect": false}]	Change the partition key to OrderID (UUID) and create a Global Secondary Index (GSI) with UserID as the partition key	Using a unique OrderID (UUID) as the partition key ensures even write distribution across all partitions, eliminating hot partitions. A GSI with UserID as the partition key enables efficient queries for all orders by a specific user. This architecture separates write distribution (via random OrderID) from query access patterns (via GSI on UserID), providing both high write throughput and efficient queries without throttling.	{"summary": "UUID partition key distributes writes; GSI on UserID maintains query efficiency", "breakdown": ["Hot partition problem: Few UserIDs receive disproportionate writes", "UUID partition key: Guarantees random distribution across all partitions", "Write distribution: No single partition receives excessive traffic", "GSI purpose: Enables querying by UserID despite different partition key", "Query pattern: Query GSI with UserID to retrieve all user orders", "GSI cost: Additional storage and query capacity, but solves throttling", "Scalability: Supports unlimited users and orders without hot partitions", "Alternative pattern: Event-driven architecture with composite key"], "otherOptions": "Random suffixes require multiple queries (inefficient); OrderDate as partition key creates time-based hot partitions; Auto scaling doesn't solve hot partition issue (throttling persists)"}	t
102	203	103	Governance - Access Control	Advanced	Design Secure Applications and Architectures	A company uses AWS Organizations with multiple accounts. They want to prevent all accounts in the Development OU from launching EC2 instances larger than t3.large, while allowing the Production OU full access. Despite having IAM permissions, developers report they cannot launch t3.xlarge instances. What is the evaluation order that explains this behavior?	[{"text": "SCP deny at OU level blocks the action regardless of IAM Allow policies, as SCPs set the maximum permissions boundary", "isCorrect": true}, {"text": "IAM policies are evaluated first, then SCPs, so the IAM Allow should override the SCP deny", "isCorrect": false}, {"text": "Resource-based policies on EC2 can override SCP restrictions for specific instance types", "isCorrect": false}, {"text": "Identity-based IAM policies attached to users take precedence over OU-level SCPs", "isCorrect": false}]	SCP deny at OU level blocks the action regardless of IAM Allow policies, as SCPs set the maximum permissions boundary	Service Control Policies (SCPs) define the maximum permissions for accounts in an organization, acting as a guardrail. Even if an IAM policy grants permission, an SCP deny will prevent the action. The evaluation order is: SCP (defines maximum allowed permissions)  IAM policies (grant specific permissions within SCP boundaries)  Resource-based policies. SCPs filter what actions are possible; IAM policies then determine what is actually permitted within those boundaries.	{"summary": "SCPs set permission boundaries that override IAM Allow policies", "breakdown": ["Policy evaluation order: SCPs  Identity-based IAM  Resource-based policies", "SCP as boundary: Defines maximum possible permissions for entire account", "IAM cannot exceed: IAM Allow policies only work within SCP boundaries", "Explicit deny: SCP deny blocks action even with IAM Allow", "OU inheritance: SCPs applied to OU affect all accounts within it", "Production OU: Different SCP allowing all instance types", "Use case: Prevent costly resource launches, enforce compliance", "Testing: Developers see access denied despite having IAM permissions"], "otherOptions": "IAM policies evaluated after SCPs (not first); Resource-based policies can't override SCP denies; User IAM policies must work within SCP boundaries"}	t
103	204	104	Container Orchestration	Intermediate	Design High-Performing Architectures	A containerized application runs on ECS with 20 tasks across 5 EC2 instances. During deployment, they notice that some instances run 8 tasks while others run only 2, causing uneven resource utilization. Which task placement strategy should be configured to distribute tasks evenly across instances?	[{"text": "Use the spread placement strategy with the attribute ecs.availability-zone to distribute tasks across AZs", "isCorrect": false}, {"text": "Use the binpack placement strategy with memory to maximize resource utilization on each instance", "isCorrect": false}, {"text": "Use the spread placement strategy with the attribute instance-id to distribute tasks evenly across instances", "isCorrect": true}, {"text": "Use the random placement strategy to distribute tasks without specific constraints", "isCorrect": false}]	Use the spread placement strategy with the attribute instance-id to distribute tasks evenly across instances	The spread placement strategy distributes tasks across specified attributes. Using "instance-id" ensures tasks are placed evenly across all container instances, preventing any single instance from being overloaded. This strategy maximizes fault tolerance and resource distribution. With 20 tasks and 5 instances, spread will place approximately 4 tasks per instance, ensuring even resource utilization across the cluster.	{"summary": "Spread strategy with instance-id attribute ensures even task distribution across instances", "breakdown": ["Spread strategy: Distributes tasks based on specified attribute (AZ, instance-id, etc.)", "Instance-id attribute: Ensures even distribution across all instances", "Result: 20 tasks  5 instances = 4 tasks per instance (approximately)", "Fault tolerance: No single instance overloaded or under-utilized", "Resource utilization: CPU and memory distributed evenly", "AZ spread: Use ecs.availability-zone for cross-AZ distribution", "Combination: Can use multiple strategies (spread for AZ, then instance)", "Best practice: Spread for HA, binpack for cost optimization"], "otherOptions": "AZ spread distributes across zones, not instances; Binpack intentionally concentrates tasks (opposite of even distribution); Random doesn't guarantee even distribution"}	t
104	205	105	Secure Data Access	Intermediate	Design Secure Applications and Architectures	A web application needs to allow users to upload profile photos directly to S3 without exposing AWS credentials in the client-side code. The uploads should be restricted to specific users for a limited time. What is the MOST secure approach?	[{"text": "Generate S3 presigned URLs on the backend with PUT permissions and short expiration times (e.g., 15 minutes), provide to authenticated users", "isCorrect": true}, {"text": "Create an IAM user with S3 write permissions and embed the access keys in the client application", "isCorrect": false}, {"text": "Make the S3 bucket public with a bucket policy that allows uploads from the application's IP address", "isCorrect": false}, {"text": "Use S3 Access Points with a resource policy that grants upload permissions to all users", "isCorrect": false}]	Generate S3 presigned URLs on the backend with PUT permissions and short expiration times (e.g., 15 minutes), provide to authenticated users	Presigned URLs allow temporary access to S3 objects without requiring AWS credentials in the client. The backend generates a URL signed with its AWS credentials that grants specific permissions (e.g., PUT object) for a limited time. Users can upload directly to S3 using this URL, which automatically expires, ensuring time-limited access. This keeps credentials secure on the backend while enabling direct client-to-S3 uploads, reducing backend bandwidth and improving performance.	{"summary": "Presigned URLs provide secure, time-limited direct access to S3 without exposing credentials", "breakdown": ["Backend generation: Server creates presigned URL using its IAM role credentials", "Time-limited: URL expires after specified duration (e.g., 15 minutes)", "Permission control: URL grants specific operation (PUT for upload, GET for download)", "Direct upload: Client uploads directly to S3, not through backend", "No credentials exposed: Client never sees AWS access keys", "Authentication: Backend verifies user identity before generating URL", "Bandwidth savings: Upload traffic doesn't go through application servers", "Security: URL expires automatically, limiting exposure window"], "otherOptions": "Embedded credentials in client code is extremely insecure; Public bucket violates least privilege principle; S3 Access Points don't provide time-limited access like presigned URLs"}	t
\.


--
-- TOC entry 5021 (class 0 OID 24591)
-- Dependencies: 221
-- Data for Name: comptia_cloud_plus_questions; Type: TABLE DATA; Schema: prepper; Owner: ericbo
--

COPY prepper.comptia_cloud_plus_questions (id, question_id, question_number, category, difficulty, domain, question_text, options, correct_answer, explanation, explanation_details, multiple_answers, correct_answers) FROM stdin;
217	217	217	DevOps - CI/CD Pipelines	Intermediate	DevOps Fundamentals	A development team wants to implement continuous integration for their cloud application. Which component is MOST critical for a successful CI pipeline?	[{"text": "Automated testing at each commit", "isCorrect": true}, {"text": "Manual code review process", "isCorrect": false}, {"text": "Weekly deployment schedule", "isCorrect": false}, {"text": "Database backup automation", "isCorrect": false}]	Automated testing at each commit	Continuous Integration requires automated testing to catch issues early. Tests run automatically on every code commit, providing immediate feedback to developers.	{"summary": "CI Pipeline Requirements:", "breakdown": ["Automated builds on every commit", "Automated unit and integration tests", "Fast feedback loops (under 10 minutes)", "Version control integration"], "otherOptions": "Manual reviews are good but slow down CI\\\\nWeekly deployments are too infrequent\\\\nBackups are important but not CI-specific"}	\N	\N
224	221	221	DevOps - Container Orchestration	Beginner	DevOps Fundamentals	What is the primary purpose of Kubernetes in a cloud environment?	[{"text": "To replace virtual machines entirely", "isCorrect": false}, {"text": "To orchestrate and manage containerized applications at scale", "isCorrect": true}, {"text": "To provide serverless computing capabilities", "isCorrect": false}, {"text": "To eliminate the need for load balancers", "isCorrect": false}]	To orchestrate and manage containerized applications at scale	Kubernetes automates deployment, scaling, and management of containerized applications across clusters of hosts. It handles container lifecycle, networking, and resource allocation.	{"summary": "Kubernetes Key Functions:", "breakdown": ["Automatic container scheduling and placement", "Self-healing: restart failed containers", "Horizontal scaling based on metrics", "Service discovery and load balancing"], "otherOptions": "Kubernetes runs on VMs, doesn't replace them\\\\nServerless is a different paradigm (e.g., AWS Lambda)\\\\nLoad balancing is a Kubernetes feature, not eliminated"}	\N	\N
225	220	220	DevOps - Infrastructure as Code	Advanced	DevOps Fundamentals	A DevOps team uses Terraform to manage cloud infrastructure. They need to ensure that infrastructure changes are reviewed and tested before applying to production. What is the BEST workflow?	[{"text": "Apply changes directly to production after local testing", "isCorrect": false}, {"text": "Use Terraform plan in CI, apply after peer review and approval", "isCorrect": true}, {"text": "Manually review Terraform files without using plan", "isCorrect": false}, {"text": "Apply changes during low-traffic hours without review", "isCorrect": false}]	Use Terraform plan in CI, apply after peer review and approval	Terraform plan shows proposed changes without applying them. Running in CI with peer review ensures changes are validated before production deployment.	{"summary": "IaC Best Practices:", "breakdown": ["Run 'terraform plan' in CI pipeline", "Require peer review for infrastructure changes", "Test in non-production environments first", "Use state locking to prevent concurrent modifications"], "otherOptions": "Direct production changes are risky\\\\nManual review can miss configuration errors\\\\nTiming doesn't replace proper review process"}	\N	\N
226	219	219	DevOps - Infrastructure as Code	Intermediate	DevOps Fundamentals	What is the primary benefit of using Infrastructure as Code (IaC) tools like Terraform or CloudFormation?	[{"text": "Eliminates the need for cloud expertise", "isCorrect": false}, {"text": "Provides version-controlled, repeatable infrastructure deployments", "isCorrect": true}, {"text": "Automatically optimizes cloud costs", "isCorrect": false}, {"text": "Removes the need for security configurations", "isCorrect": false}]	Provides version-controlled, repeatable infrastructure deployments	IaC treats infrastructure configuration as code, enabling version control, peer review, automated testing, and consistent deployments across environments.	{"summary": "Infrastructure as Code Benefits:", "breakdown": ["Version control for infrastructure changes", "Consistent deployments across environments", "Self-documenting infrastructure", "Enables disaster recovery and environment replication"], "otherOptions": "IaC requires cloud knowledge to write templates\\\\nCost optimization requires separate tools\\\\nSecurity must still be explicitly configured"}	\N	\N
227	222	222	DevOps - Container Orchestration	Advanced	DevOps Fundamentals	A microservices application deployed on Kubernetes experiences intermittent connection failures between services. Which Kubernetes feature should be investigated FIRST?	[{"text": "Ingress controller configuration", "isCorrect": false}, {"text": "Service mesh implementation", "isCorrect": false}, {"text": "Service discovery and DNS resolution", "isCorrect": true}, {"text": "Pod CPU limits", "isCorrect": false}]	Service discovery and DNS resolution	Intermittent connection issues between services typically indicate DNS resolution problems or service discovery failures. Kubernetes DNS (CoreDNS) enables service-to-service communication.	{"summary": "Kubernetes Networking Troubleshooting:", "breakdown": ["Check CoreDNS pod health and logs", "Verify Service definitions and selectors", "Test DNS resolution from within pods", "Ensure network policies don't block traffic"], "otherOptions": "Ingress handles external traffic, not inter-service\\\\nService mesh is optional and adds complexity\\\\nCPU limits affect performance, not connectivity"}	\N	\N
228	223	223	DevOps - Configuration Management	Intermediate	DevOps Fundamentals	An organization needs to ensure consistent server configurations across 500 cloud instances. Which approach BEST aligns with DevOps principles?	[{"text": "Manually configure each server following a checklist", "isCorrect": false}, {"text": "Create a golden image and deploy copies", "isCorrect": false}, {"text": "Use configuration management tools like Ansible or Chef", "isCorrect": true}, {"text": "Write custom shell scripts for each server", "isCorrect": false}]	Use configuration management tools like Ansible or Chef	Configuration management tools provide idempotent, automated configuration that can be version-controlled and tested. They ensure consistency and enable configuration drift detection.	{"summary": "Configuration Management Benefits:", "breakdown": ["Idempotent operations: safe to run multiple times", "Version-controlled configurations", "Automated compliance and drift detection", "Scalable to thousands of nodes"], "otherOptions": "Manual configuration is error-prone and slow\\\\nGolden images become stale and lack flexibility\\\\nCustom scripts are hard to maintain and test"}	\N	\N
229	224	224	DevOps - GitOps Workflows	Advanced	DevOps Fundamentals	A team wants to implement GitOps for their Kubernetes deployments. What is the core principle of GitOps?	[{"text": "All deployment automation scripts stored in Git", "isCorrect": false}, {"text": "Git repository serves as the single source of truth for declarative infrastructure", "isCorrect": true}, {"text": "Using GitHub for code reviews", "isCorrect": false}, {"text": "Automatic merging of all Git branches", "isCorrect": false}]	Git repository serves as the single source of truth for declarative infrastructure	GitOps uses Git as the single source of truth. Desired system state is declared in Git, and automated agents ensure the actual state matches. All changes go through Git workflows (PR, review, merge).	{"summary": "GitOps Principles:", "breakdown": ["Declarative infrastructure and application definitions", "Git as single source of truth", "Automated synchronization from Git to clusters", "Pull-based deployment model for security"], "otherOptions": "It's about the workflow, not just storage location\\\\nCode review is part of it but not the core principle\\\\nAutomatic merging contradicts proper review processes"}	\N	\N
173	173	188	Automation	Knowledge	DevOps and Automation	Which of the following is a declarative Infrastructure as Code (IaC) tool?	[{"text": "A bash script with a series of CLI commands.", "isCorrect": false}, {"text": "An Ansible playbook.", "isCorrect": false}, {"text": "A Terraform configuration file.", "isCorrect": true}, {"text": "A Python script using a cloud SDK.", "isCorrect": false}]	A Terraform configuration file.	Terraform is a prime example of a declarative IaC tool. You define the desired end state of your infrastructure in HCL (HashiCorp Configuration Language), and Terraform's engine figures out the necessary API calls to create, update, or delete resources to achieve that state.	{"summary": "Terraform is a declarative IaC tool.", "breakdown": ["You declare *what* you want, not *how* to create it.", "Terraform builds a dependency graph and executes actions in the correct order.", "It is idempotent, meaning you can apply the same configuration multiple times with no changes after the first successful run."], "otherOptions": "A, Bash and Python scripts are imperative; they define the specific, step-by-step commands to execute.\\nAnsible is largely declarative but is primarily a configuration management tool, not an infrastructure provisioning tool, although it can do both."}	1	{"A Terraform configuration file."}
148	148	164	Operations	Knowledge	Operations and Support	What does it mean for an Infrastructure as Code (IaC) template to be idempotent?	[{"text": "It can only be used one time before it needs to be rewritten.", "isCorrect": false}, {"text": "It will result in the same defined end state regardless of how many times it is applied.", "isCorrect": true}, {"text": "It can be used to deploy infrastructure to multiple cloud providers simultaneously.", "isCorrect": false}, {"text": "It automatically encrypts all resources that it creates.", "isCorrect": false}]	It will result in the same defined end state regardless of how many times it is applied.	Idempotence is a key principle of modern IaC and configuration management. It means that running the operation multiple times will have the same result as running it once. For IaC, this means if you apply a template to create a server, and then apply the same template again, it will recognize the server already exists and make no changes.	{"summary": "Idempotency means repeated applications result in the same state.", "breakdown": ["It makes deployments predictable and safe to re-run.", "If a deployment fails midway, you can simply run it again to complete the setup.", "It is the core principle that allows IaC tools to manage the state of infrastructure over time."], "otherOptions": "This is incorrect; the goal is reusability.\\nThis describes a cloud-agnostic tool, which is a different concept.\\nEncryption is a configuration choice within the template, not an inherent property of idempotency."}	1	{"It will result in the same defined end state regardless of how many times it is applied."}
22	22	24	Deployments - Infrastructure as Code	Application	Cloud Deployment	Your team needs to deploy identical environments across development, staging, and production. Which approach ensures consistency and reduces manual errors?	[{"text": "Copying virtual machine images", "isCorrect": false}, {"text": "Using different configurations for each environment", "isCorrect": false}, {"text": "Infrastructure as Code (IaC) templates", "isCorrect": true}, {"text": "Manual deployment through cloud console", "isCorrect": false}]	Infrastructure as Code (IaC) templates	IaC templates ensure consistent, repeatable, and version-controlled infrastructure deployments.	{"summary": "IaC benefits for environment consistency:", "breakdown": ["Version-controlled infrastructure definitions", "Automated and repeatable deployments", "Eliminates configuration drift", "Enables testing of infrastructure changes"], "otherOptions": "Manual processes are error-prone\\nVM images don't cover full infrastructure\\nDifferent configs create inconsistency"}	\N	\N
80	80	80	Infrastructure as Code	Application	Cloud Deployment	A DevOps team manages infrastructure across development, staging, and production environments. They experience configuration drift and inconsistencies between environments, leading to deployment failures. Which approach would BEST solve these consistency issues?	[{"text": "Use configuration management scripts", "isCorrect": false}, {"text": "Create golden images for all server configurations", "isCorrect": false}, {"text": "Implement Infrastructure as Code (IaC) with version control", "isCorrect": true}, {"text": "Document all configurations in detailed runbooks", "isCorrect": false}]	Implement Infrastructure as Code (IaC) with version control	Infrastructure as Code ensures consistent, repeatable deployments across environments by defining infrastructure in version-controlled code templates.	{"summary": "IaC benefits for environment consistency:", "breakdown": ["Declarative definitions: Infrastructure defined as code templates", "Version control: Track and rollback infrastructure changes", "Consistency: Identical deployments across all environments", "Automation: Eliminates manual configuration errors"], "otherOptions": "Documentation doesn't prevent manual configuration errors\\nGolden images don't address infrastructure configuration drift\\nScripts can vary in execution and may not be declarative"}	\N	\N
72	72	71	DevOps - Automation	Application	DevOps and Automation	A DevOps team wants to automate the provisioning of new virtual machines, network configurations, and security groups whenever a new project starts. Which practice is best suited for this task?	[{"text": "Implementing Infrastructure as Code (IaC) with templating tools.", "isCorrect": true}, {"text": "Using shell scripts for server setup and manual network configuration.", "isCorrect": false}, {"text": "Manual configuration via cloud console for each project.", "isCorrect": false}, {"text": "Creating a comprehensive manual checklist for infrastructure setup.", "isCorrect": false}]	Implementing Infrastructure as Code (IaC) with templating tools.	Infrastructure as Code (Iaallows defining and provisioning infrastructure using code, ensuring repeatability, consistency, and reduced manual errors for new project environments.	{"summary": "IaC for automated provisioning:", "breakdown": ["**Repeatability:** Ensures identical environments are deployed every time.", "**Consistency:** Eliminates configuration drift between environments.", "**Reduced Errors:** Automates complex setup processes, minimizing human error.", "**Version Control:** Infrastructure definitions can be versioned and managed like application code."], "otherOptions": "Manual configuration is prone to errors and inconsistencies, especially for complex setups. \\nShell scripts can automate some tasks but typically lack the comprehensive state management and idempotency of IaC tools for full infrastructure provisioning. Network configuration would still likely be manual or poorly managed. \\nA manual checklist helps but does not automate or guarantee consistency, nor does it reduce the time spent on manual setup."}	\N	\N
231	218	217	Operations - Cost Management	Comprehension	Operations and Support	Which of the following BEST summarizes key cost considerations for cloud usage?	[{"text": "A) The main focus of cloud cost management is on dedicated host billing models", "isCorrect": false}, {"text": "B) Cloud cost management is primarily focused on minimizing upfront costs rather than ongoing usage optimization", "isCorrect": false}, {"text": "C) Cloud costs involve implementing tagging for continuously rightsizing resources", "isCorrect": true}, {"text": "D) Most cloud costs are predictable, making real-time monitoring and usage optimization less critical for cost management", "isCorrect": false}]	C) Cloud costs involve implementing tagging for continuously rightsizing resources	Effective cloud cost management requires comprehensive tagging for cost allocation and visibility, combined with continuous rightsizing to ensure resources match actual workload requirements. These practices enable organizations to track spending by team/project and eliminate waste from over-provisioned resources.	{"summary": "Key cloud cost management practices:", "breakdown": ["Tagging strategy: Label all resources with project, team, environment, cost-center", "Cost allocation: Track spending by department, application, or business unit", "Continuous rightsizing: Regularly analyze and adjust resource sizes to match demand", "Eliminate waste: Identify and remove idle, unused, or over-provisioned resources", "Reserved capacity: Use commitments for predictable workloads (30-70% savings)", "Real-time monitoring: Set budgets, alerts, and anomaly detection for cost spikes"], "otherOptions": "A) Cloud cost management covers all pricing models, not just dedicated hosts\\\\nB) Cloud advantage is low upfront costs; challenge is managing ongoing usage\\\\nD) Cloud costs are often unpredictable due to elasticity - monitoring IS critical"}	\N	\N
232	232	335	Cloud Architecture - Multi-tenant Design	Intermediate	Cloud Architecture and Design	A healthcare SaaS provider needs to ensure their multi-tenant application isolates customer data while maximizing resource efficiency. They want to prevent one tenant's workload from impacting another's performance. What architecture pattern should they implement?	[{"text": "Single database with shared tables and tenant ID filtering", "isCorrect": false}, {"text": "Separate database instances for each tenant", "isCorrect": false}, {"text": "Resource quotas and container-based isolation per tenant", "isCorrect": true}, {"text": "Single application instance serving all tenants without isolation", "isCorrect": false}]	Resource quotas and container-based isolation per tenant	Resource quotas combined with container-based isolation provide the optimal balance between security, performance isolation, and resource efficiency in multi-tenant architectures. This approach ensures that each tenant's workload runs in isolated containers with defined CPU, memory, and I/O limits, preventing 'noisy neighbor' issues where one tenant's resource consumption affects others.	{"summary": "Multi-tenant Isolation Strategies:", "breakdown": ["Container-based isolation provides process and resource separation", "Resource quotas prevent resource exhaustion by single tenants", "More cost-effective than full database instance separation", "Kubernetes namespaces and resource limits enforce boundaries"], "otherOptions": "Single database filtering offers weak isolation\\nSeparate instances are costly and inefficient\\nNo isolation creates security and performance risks"}	\N	\N
233	233	336	Cloud Architecture - Scaling Strategies	Intermediate	Cloud Architecture and Design	A global e-commerce company experiences seasonal traffic spikes during holiday shopping events. Their application currently uses fixed-capacity infrastructure. What combination of cloud features would BEST handle these predictable demand patterns while minimizing costs?	[{"text": "Over-provision resources year-round to handle peak capacity", "isCorrect": false}, {"text": "Scheduled auto-scaling with predictive scaling policies", "isCorrect": true}, {"text": "Manual scaling based on real-time monitoring alerts", "isCorrect": false}, {"text": "Reactive auto-scaling based only on CPU utilization thresholds", "isCorrect": false}]	Scheduled auto-scaling with predictive scaling policies	Scheduled auto-scaling with predictive scaling policies is ideal for handling known, recurring traffic patterns like holiday shopping events. This approach allows the system to proactively scale resources before demand increases, ensuring capacity is available when needed without performance degradation.	{"summary": "Predictive Scaling Benefits:", "breakdown": ["Scheduled scaling prepares for known peak periods (Black Friday, etc.)", "Predictive policies use ML to forecast demand from historical data", "Proactive scaling prevents lag from reactive-only approaches", "Significantly reduces costs compared to constant over-provisioning"], "otherOptions": "Over-provisioning wastes resources during off-peak times\\nManual scaling is error-prone and too slow\\nReactive-only scaling has lag time during rapid spikes"}	\N	\N
234	234	337	Cloud Architecture - Availability	Advanced	Cloud Architecture and Design	An enterprise is designing a cloud architecture that must maintain operations even if an entire availability zone fails. Their application consists of web servers, application servers, and a relational database. What is the MINIMUM requirement to achieve this level of fault tolerance?	[{"text": "Deploy all components in a single availability zone with backup instances", "isCorrect": false}, {"text": "Distribute application tiers across multiple availability zones with synchronous replication for the database", "isCorrect": true}, {"text": "Use a single region with automatic failover to another region", "isCorrect": false}, {"text": "Deploy read replicas in different regions for disaster recovery", "isCorrect": false}]	Distribute application tiers across multiple availability zones with synchronous replication for the database	To withstand an entire availability zone failure, all critical components must be distributed across multiple availability zones within the same region. This means deploying web servers, application servers, and database instances (or replicas) in at least two different availability zones.	{"summary": "Multi-AZ High Availability Architecture:", "breakdown": ["Deploy each tier (web, app, database) across 2+ availability zones", "Use synchronous replication for databases to prevent data loss", "Load balancers automatically route traffic to healthy zones", "Provides fault tolerance without cross-region complexity"], "otherOptions": "Single AZ deployment loses everything during zone failure\\nCross-region adds latency and complexity for HA\\nRead replicas alone don't provide write availability"}	\N	\N
235	235	338	Cloud Architecture - Storage	Intermediate	Cloud Architecture and Design	A media company needs to design storage architecture for their content delivery platform. They have three types of data: frequently accessed video thumbnails (hot data), completed videos accessed occasionally (warm data), and archived content rarely retrieved (cold data). What storage strategy would optimize costs while meeting performance requirements?	[{"text": "Store all data in high-performance SSD storage for consistent access speeds", "isCorrect": false}, {"text": "Implement lifecycle policies to automatically transition data between storage tiers based on access patterns", "isCorrect": true}, {"text": "Manually move data to cheaper storage after 30 days regardless of access frequency", "isCorrect": false}, {"text": "Use object storage exclusively with no tiering strategy", "isCorrect": false}]	Implement lifecycle policies to automatically transition data between storage tiers based on access patterns	Implementing automated lifecycle policies that transition data between storage tiers based on actual access patterns provides the optimal balance of cost and performance. Cloud providers offer multiple storage tiers with different cost and performance characteristics.	{"summary": "Storage Tiering Strategy:", "breakdown": ["Hot tier: High cost, low latency - for thumbnails and recent content", "Cool/Warm tier: Lower cost, moderate latency - for older videos", "Archive tier: Lowest cost, retrieval time in hours - for archived content", "Automated policies monitor access patterns and transition objects intelligently"], "otherOptions": "All SSD storage is unnecessarily expensive for infrequently accessed data\\nManual transitions can't respond to changing access patterns\\nNo tiering strategy misses significant cost optimization opportunities"}	\N	\N
236	236	339	Cloud Architecture - Compliance	Advanced	Cloud Architecture and Design	A financial services company must comply with regulations requiring data sovereignty, ensuring customer data never leaves specific geographic boundaries. They need a multi-region presence for disaster recovery. What architecture should they implement?	[{"text": "Single region deployment with backup tapes shipped to an off-site location", "isCorrect": false}, {"text": "Multi-region active-active deployment with data replication across all regions", "isCorrect": false}, {"text": "Regional data residency with disaster recovery infrastructure in the same geographic compliance zone", "isCorrect": true}, {"text": "Global CDN caching all customer data for performance", "isCorrect": false}]	Regional data residency with disaster recovery infrastructure in the same geographic compliance zone	Data sovereignty requirements mandate that data remains within specific geographic or legal boundaries. To meet these requirements while maintaining disaster recovery capabilities, the company should deploy infrastructure and disaster recovery resources within regions that comply with the same data sovereignty regulations.	{"summary": "Data Sovereignty Compliance Architecture:", "breakdown": ["Deploy primary and DR infrastructure within the same compliance zone", "Example: EU customer data stays within EU regions (GDPR compliance)", "Ensures data never crosses into non-compliant geographic zones", "Maintains disaster recovery without violating sovereignty requirements"], "otherOptions": "Global replication violates data sovereignty by crossing boundaries\\nBackup tapes provide inadequate RTO for business continuity\\nGlobal CDN caching would distribute sensitive data outside compliance zones"}	\N	\N
237	227	223	Container Technologies - Networking	Intermediate	Cloud Architecture and Design	An application requires network communication between containers across different hosts. Which networking solution is MOST appropriate?	[{"text": "Bridge network mode on each host", "isCorrect": false}, {"text": "Host network mode to eliminate network overhead", "isCorrect": false}, {"text": "Overlay network that spans multiple hosts", "isCorrect": true}, {"text": "None network mode with manual IP configuration", "isCorrect": false}]	Overlay network that spans multiple hosts	An overlay network is specifically designed for container communication across multiple hosts. It creates a virtual network that spans across the physical host infrastructure, allowing containers on different hosts to communicate as if they were on the same network. Container orchestration platforms like Docker Swarm and Kubernetes use overlay networks to enable seamless multi-host container networking with built-in service discovery and load balancing.	{"summary": "Container Networking Modes:", "breakdown": ["Overlay networks create a distributed network across multiple hosts", "Enables containers to communicate across hosts using virtual network", "Kubernetes uses CNI plugins (Calico, Flannel, Weave) for overlay networking", "Docker Swarm uses built-in overlay driver for multi-host communication", "Provides service discovery and load balancing across the cluster"], "otherOptions": "Bridge network is isolated to single host only\\nHost mode shares host network stack, no isolation between containers\\nNone mode disables networking entirely, requiring complex manual configuration"}	\N	\N
238	228	224	Cloud Architecture - Network Components	Advanced	Cloud Architecture and Design	An international organization with data centers spread across different continents aims to optimize data routing between these locations for maximum availability and the shortest path. What solution should they implement to manage this effectively?	[{"text": "Static routing", "isCorrect": false}, {"text": "OSPF (Open Shortest Path First)", "isCorrect": false}, {"text": "BGP (Border Gateway Protocol)", "isCorrect": true}, {"text": "RIP (Routing Information Protocol)", "isCorrect": false}]	BGP (Border Gateway Protocol)	BGP (Border Gateway Protocol) is the routing protocol used to exchange routing information between different autonomous systems (AS) on the internet. It selects the optimal path based on route metrics, policies, and network conditions to forward data. This protocol is crucial for managing and optimizing data traffic across global networks, ensuring that data is forwarded through the most efficient and reliable routes between geographically distributed data centers.	{"summary": "BGP for Global Routing:", "breakdown": ["Designed for inter-domain routing between autonomous systems", "Supports path selection based on multiple attributes (AS path, MED, local preference)", "Provides redundancy and failover for maximum availability", "Scales to handle internet-sized routing tables", "Enables traffic engineering and policy-based routing"], "otherOptions": "Static routing doesn't adapt to network changes or failures\\nOSPF is an interior gateway protocol, not designed for inter-AS routing\\nRIP has limited scalability and slow convergence for global networks"}	\N	\N
249	239	235	Cloud Architecture - Network Components	Intermediate	Cloud Architecture and Design	What solution would best provide secure and reliable access for a financial institution's employees to internal networks and cloud services from external networks, while ensuring data is encrypted during transmission?	[{"text": "Implementing an MPLS network between all users and cloud services", "isCorrect": false}, {"text": "Using unsecured Wi-Fi networks with additional authentication protocols", "isCorrect": false}, {"text": "Employing a full-tunnel VPN for all remote access", "isCorrect": true}, {"text": "Relying on third-party cloud services without additional security measures", "isCorrect": false}]	Employing a full-tunnel VPN for all remote access	Full-tunnel VPN is a network security solution that ensures all internet traffic from remote user devices, including access to internal networks and cloud services, is securely encrypted. This creates a secure communication channel that protects data during transmission, making it ideal for organizations with high-security requirements such as financial institutions and healthcare providers.	{"summary": "Full-Tunnel VPN Benefits:", "breakdown": ["Encrypts all traffic between remote users and corporate networks", "Provides secure access to both internal networks and cloud services", "Routes all internet traffic through the VPN tunnel for complete protection", "Prevents man-in-the-middle attacks and data interception", "Meets compliance requirements for financial and healthcare sectors"], "otherOptions": "MPLS is expensive and not suitable for distributed remote workers\\nUnsecured Wi-Fi exposes sensitive data regardless of authentication\\nThird-party services without VPN lack encryption for data in transit"}	\N	\N
240	230	226	Cloud Architecture - Network Components	Intermediate	Cloud Architecture and Design	Frank, a cloud architect, is planning a network for a new cloud deployment that will host both production and development environments. He needs to ensure proper isolation between these environments while optimizing the use of IP address space. What solution should Frank implement?	[{"text": "A single subnet for both environments to conserve IP addresses", "isCorrect": false}, {"text": "Separate subnets for each environment within the same VPC", "isCorrect": true}, {"text": "Multiple VPCs, one for each environment", "isCorrect": false}, {"text": "One subnet with a firewall between the production and development environments", "isCorrect": false}]	Separate subnets for each environment within the same VPC	To achieve network isolation while optimizing IP address usage, Frank should create separate subnets for each environment within the same Virtual Private Cloud (VPC). This setup allows the environments to be logically isolated, meaning production and development traffic will not interfere with each other. Additionally, it enables more efficient use of IP address space since the subnets can be sized according to the specific needs of each environment. By keeping both environments within the same VPC, he can also take advantage of centralized management and reduce complexity while still maintaining isolation through network routing and security policies.	{"summary": "VPC Subnet Isolation Strategy:", "breakdown": ["Separate subnets provide logical network isolation within same VPC", "Subnets can be sized appropriately for each environment's needs", "Network ACLs and security groups enforce traffic separation", "Centralized VPC management reduces operational complexity", "Route tables control inter-subnet communication"], "otherOptions": "Single subnet offers no isolation between environments\\nMultiple VPCs add unnecessary complexity and cost\\nFirewall within single subnet is less effective than subnet-level isolation"}	\N	\N
241	231	227	Cloud Architecture - Service Models	Intermediate	Cloud Architecture and Design	A manufacturing company needs to configure and deploy IoT sensor data collection. The deployment needs automatic scaling and minimal infrastructure management. Which service model is most appropriate?	[{"text": "IaaS with manual scaling", "isCorrect": false}, {"text": "PaaS with auto-scaling capabilities", "isCorrect": true}, {"text": "SaaS with limited customization", "isCorrect": false}, {"text": "Container as a Service (CaaS)", "isCorrect": false}]	PaaS with auto-scaling capabilities	Platform as a Service (PaaS) with auto-scaling is ideal for IoT sensor data collection because it abstracts infrastructure management while providing the flexibility to configure custom applications. PaaS platforms handle server provisioning, patching, scaling, and maintenance automatically, allowing the manufacturing company to focus on their IoT application logic rather than infrastructure. The built-in auto-scaling ensures the system can handle variable sensor data loads without manual intervention.	{"summary": "PaaS Benefits for IoT Applications:", "breakdown": ["Automatic infrastructure management and scaling", "Built-in services for data ingestion and processing", "Focus on application development rather than operations", "Elastic scaling handles variable IoT workloads", "Reduced operational overhead compared to IaaS"], "otherOptions": "IaaS requires manual infrastructure management and scaling\\nSaaS lacks customization needed for specific IoT data collection\\nCaaS requires more container orchestration knowledge and management"}	\N	\N
242	232	228	Cloud Architecture - Availability	Intermediate	Cloud Architecture and Design	An e-commerce application requires 99.9% uptime with automatic failover capabilities. Which availability zone configuration should you implement?	[{"text": "Single availability zone with redundant instances", "isCorrect": false}, {"text": "Multi-availability zone deployment with load balancing", "isCorrect": true}, {"text": "Single availability zone with manual failover procedures", "isCorrect": false}, {"text": "Cross-region deployment for all components", "isCorrect": false}]	Multi-availability zone deployment with load balancing	Multi-availability zone (Multi-AZ) deployment with load balancing provides the high availability needed to achieve 99.9% uptime. By distributing application instances across multiple physically separated availability zones within the same region, the system can automatically failover if one zone experiences an outage. Load balancers continuously monitor instance health and automatically route traffic away from failed zones to healthy ones, ensuring minimal disruption and meeting the uptime requirement.	{"summary": "Multi-AZ High Availability:", "breakdown": ["Distributes instances across physically separated data centers", "Load balancer provides automatic health checks and failover", "Achieves 99.9% uptime SLA (8.76 hours downtime per year)", "Protects against zone-level failures", "Lower latency than cross-region deployment"], "otherOptions": "Single AZ deployment fails entirely if the zone goes down\\nManual failover is too slow and violates uptime requirements\\nCross-region adds unnecessary complexity and latency for 99.9% target"}	\N	\N
243	233	229	Cloud Architecture - Disaster Recovery	Advanced	Cloud Architecture and Design	A development services company has an RTO of 4 hours and RPO of 30 minutes. Which disaster recovery model should they implement?	[{"text": "Cold standby with daily backups", "isCorrect": false}, {"text": "Warm standby with continuous replication", "isCorrect": true}, {"text": "Hot standby with active-active configuration", "isCorrect": false}, {"text": "Backup and restore with weekly snapshots", "isCorrect": false}]	Warm standby with continuous replication	Warm standby with continuous replication meets both the 4-hour RTO and 30-minute RPO requirements. In a warm standby configuration, core infrastructure components are already provisioned and running at reduced capacity in the DR site, allowing for relatively quick failover within the 4-hour window. Continuous replication ensures data is synchronized frequently enough to meet the 30-minute RPO, meaning minimal data loss occurs during a disaster event.	{"summary": "DR Models and RTO/RPO Alignment:", "breakdown": ["Warm standby: Core systems running, can scale up within hours", "Continuous replication meets 30-minute RPO requirement", "More cost-effective than hot standby for 4-hour RTO", "Balances recovery time with infrastructure costs", "Automated failover processes enable timely recovery"], "otherOptions": "Cold standby with daily backups cannot meet 30-minute RPO\\nHot standby is unnecessarily expensive for 4-hour RTO\\nWeekly snapshots far exceed 30-minute RPO requirement"}	\N	\N
8	8	10	Cloud Architecture - Storage	Knowledge	Cloud Architecture and Design	Which storage type is best suited for frequently accessed data requiring low latency and high throughput?	[{"text": "Archive storage tier", "isCorrect": false}, {"text": "Warm storage tier", "isCorrect": false}, {"text": "Cold storage tier", "isCorrect": false}, {"text": "Hot storage tier", "isCorrect": true}]	Hot storage tier	Hot storage tier is optimized for frequently accessed data with low latency requirements.	{"summary": "Hot storage characteristics:", "breakdown": ["Optimized for frequent access patterns", "Provides low latency data retrieval", "Higher cost but better performance", "Ideal for production application data"], "otherOptions": "Cold storage for infrequent access\\nArchive for long-term retention\\nWarm storage for moderate access"}	\N	\N
244	234	230	Cloud Architecture - Disaster Recovery	Advanced	Cloud Architecture and Design	A backup strategy requires cross-region replication with a 15-minute recovery point objective. Which configuration is most suitable?	[{"text": "Asynchronous replication with hourly snapshots", "isCorrect": false}, {"text": "Synchronous cross-region replication", "isCorrect": false}, {"text": "Continuous asynchronous replication with automated failover", "isCorrect": true}, {"text": "Daily full backups with transaction logs", "isCorrect": false}]	Continuous asynchronous replication with automated failover	Continuous asynchronous replication is the optimal solution for cross-region backup with a 15-minute RPO. It continuously replicates data changes to the secondary region with minimal delay, ensuring data loss is limited to approximately 15 minutes or less. Asynchronous replication is necessary for cross-region scenarios because synchronous replication would introduce unacceptable latency over long distances. Automated failover capabilities ensure quick recovery when needed.	{"summary": "Cross-Region Replication Strategy:", "breakdown": ["Continuous replication maintains near-real-time data synchronization", "Asynchronous mode avoids latency issues over geographic distances", "Meets 15-minute RPO requirement with frequent data transfers", "Automated failover reduces RTO during disaster events", "More cost-effective than maintaining synchronous cross-region links"], "otherOptions": "Hourly snapshots exceed the 15-minute RPO requirement\\nSynchronous cross-region replication introduces high latency\\nDaily backups with transaction logs cannot meet 15-minute RPO"}	\N	\N
245	235	231	Virtualization	Intermediate	Cloud Architecture and Design	An administrator needs to move a running virtual machine from one physical host to another without any service interruption or downtime. What is this process known as?	[{"text": "Cold migration", "isCorrect": false}, {"text": "Live migration (vMotion)", "isCorrect": true}, {"text": "Snapshot and restore", "isCorrect": false}, {"text": "Failover clustering", "isCorrect": false}]	Live migration (vMotion)	Live migration, also known as vMotion in VMware environments, allows a running virtual machine to be moved from one physical host to another without any downtime or service interruption. The process transfers the VM's active memory, execution state, and storage connections while the VM continues running, making the migration transparent to users and applications.	{"summary": "Live Migration Process:", "breakdown": ["Transfers VM memory and execution state while running", "Maintains active network connections during migration", "Enables zero-downtime maintenance and load balancing", "Requires shared storage or storage migration capability", "VMware vMotion, Hyper-V Live Migration, KVM live migration"], "otherOptions": "Cold migration requires VM shutdown and causes downtime\\nSnapshot and restore is not real-time and causes interruption\\nFailover clustering is for high availability, not seamless migration"}	\N	\N
246	236	232	Virtualization	Intermediate	Cloud Architecture and Design	Your organization needs to run legacy applications that require specific hardware configurations. Which virtualization approach should you use?	[{"text": "Paravirtualization with modified guest OS", "isCorrect": false}, {"text": "Operating system-level virtualization", "isCorrect": false}, {"text": "Container-based virtualization", "isCorrect": false}, {"text": "Full virtualization with hardware emulation", "isCorrect": true}]	Full virtualization with hardware emulation	Full virtualization with hardware emulation is ideal for legacy applications requiring specific hardware configurations because it can emulate the exact hardware environment the application expects without requiring modifications to the guest OS or application. The hypervisor presents virtualized hardware that mimics the original hardware specifications, allowing legacy applications to run unmodified while believing they have direct access to the required hardware.	{"summary": "Full Virtualization for Legacy Apps:", "breakdown": ["Emulates specific hardware without modifying guest OS or applications", "Provides complete hardware abstraction layer", "Supports unmodified legacy operating systems and applications", "Can simulate deprecated or obsolete hardware", "Isolates legacy workloads from modern infrastructure"], "otherOptions": "Paravirtualization requires guest OS modifications, incompatible with legacy apps\\nOS-level virtualization shares kernel, cannot emulate specific hardware\\nContainer-based virtualization lacks hardware emulation capabilities"}	\N	\N
247	237	233	Cloud Migration	Advanced	Cloud Deployment	A manufacturing company wants to migrate their IoT sensor data processing system. The current system processes data in real-time with sub-millisecond latency requirements. Which migration strategy is MOST appropriate?	[{"text": "Rehost (lift-and-shift)", "isCorrect": false}, {"text": "Re-platform with minor optimizations", "isCorrect": false}, {"text": "Refactor for cloud-native architecture", "isCorrect": true}, {"text": "Replace with SaaS solution", "isCorrect": false}]	Refactor for cloud-native architecture	Refactoring for cloud-native architecture is necessary to meet sub-millisecond latency requirements. This approach redesigns the application to leverage edge computing capabilities integrated with cloud services. The refactored architecture processes time-critical data locally at the edge (near sensors) while using cloud services for analytics, storage, and management. This hybrid edge-cloud design is the only migration strategy that can maintain the sub-millisecond latency requirement.	{"summary": "Cloud-Native Refactoring for IoT:", "breakdown": ["Implements edge computing nodes for local data processing", "Maintains sub-millisecond response times at the edge", "Integrates with cloud for analytics and orchestration", "Uses containerized microservices for flexibility", "Leverages cloud-native tools like Kubernetes for edge management"], "otherOptions": "Rehost moves to centralized cloud, adding 50-200ms+ latency\\nRe-platform still centralizes processing in cloud\\nSaaS solutions introduce even greater network latency"}	\N	\N
248	238	234	Cloud Migration	Intermediate	Cloud Deployment	A company has 700 virtual machines running legacy applications that need to be migrated to the cloud. The applications cannot be modified and must maintain exact configurations. Which migration approach is MOST suitable?	[{"text": "Physical-to-Virtual (P2V) migration", "isCorrect": false}, {"text": "Virtual-to-Virtual (V2V) migration", "isCorrect": true}, {"text": "Application refactoring", "isCorrect": false}, {"text": "Complete application rebuild", "isCorrect": false}]	Virtual-to-Virtual (V2V) migration	Virtual-to-Virtual (V2V) migration is the optimal approach for migrating existing virtual machines to the cloud without modification. V2V tools convert VMs from on-premises hypervisors to cloud-compatible formats while preserving exact configurations, operating systems, applications, and data. This allows legacy applications to run unchanged in the cloud environment, meeting the requirement that applications cannot be modified.	{"summary": "V2V Migration Benefits:", "breakdown": ["Preserves exact VM configurations and application state", "Minimal downtime using incremental replication", "Supports bulk migration of hundreds of VMs", "No application code changes required", "Tools like AWS SMS, Azure Migrate automate the process"], "otherOptions": "P2V is for physical servers, not existing VMs\\nRefactoring requires modifying applications\\nComplete rebuild violates the no-modification requirement"}	\N	\N
6	6	8	Cloud Architecture - Availability	Knowledge	Cloud Architecture and Design	What is the primary purpose of availability zones in cloud computing?	[{"text": "To separate different cloud services", "isCorrect": false}, {"text": "To provide different pricing tiers", "isCorrect": false}, {"text": "To comply with data sovereignty requirements", "isCorrect": false}, {"text": "To provide redundancy and fault tolerance", "isCorrect": true}]	To provide redundancy and fault tolerance	Availability zones provide redundancy and fault tolerance by isolating failures to specific geographic locations.	{"summary": "Availability zone characteristics:", "breakdown": ["Isolated data center locations within a region", "Independent power, cooling, and networking", "Designed to prevent cascading failures", "Enable high availability architecture design"], "otherOptions": "Pricing is not determined by AZ\\nServices can span multiple AZs\\nData sovereignty is handled at region level"}	\N	\N
37	37	39	Operations - Backup Strategies	Comprehension	Cloud Operations and Support	What is the primary advantage of incremental backups over full backups?	[{"text": "Better data compression", "isCorrect": false}, {"text": "Less storage space and faster backup time", "isCorrect": true}, {"text": "Higher reliability", "isCorrect": false}, {"text": "Faster restore times", "isCorrect": false}]	Less storage space and faster backup time	Incremental backups only capture changes since the last backup, requiring less storage and time.	{"summary": "Incremental backup advantages:", "breakdown": ["Only backs up changed data since last backup", "Significantly less storage space required", "Faster backup execution time", "Reduced network bandwidth usage"], "otherOptions": "Restore times are actually slower\\nCompression depends on backup software\\nReliability depends on backup chain integrity"}	\N	\N
7	7	9	Cloud Architecture - Availability	Comprehension	Cloud Architecture and Design	Which cloud strategy allows an organization to handle sudden traffic spikes by temporarily using public cloud resources while maintaining their private cloud for normal operations?	[{"text": "Multi-cloud deployment", "isCorrect": false}, {"text": "Edge computing", "isCorrect": false}, {"text": "Cloud bursting", "isCorrect": true}, {"text": "Hybrid cloud architecture", "isCorrect": false}]	Cloud bursting	Cloud bursting allows organizations to scale from private to public cloud during peak demand periods.	{"summary": "Cloud bursting characteristics:", "breakdown": ["Temporary use of public cloud resources", "Handles unexpected traffic spikes", "Cost-effective scaling approach", "Maintains private cloud for normal operations"], "otherOptions": "Multi-cloud uses multiple providers simultaneously\\nHybrid cloud is permanent architecture\\nEdge computing brings processing closer to users"}	\N	\N
2	2	4	Cloud Architecture - Service Models	Knowledge	Cloud Architecture and Design	Which service model is represented by applications like Salesforce, Office 35, and Gmail?	[{"text": "Software as a Service (SaaS)", "isCorrect": true}, {"text": "Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "Function as a Service (FaaS)", "isCorrect": false}, {"text": "Platform as a Service (PaaS)", "isCorrect": false}]	Software as a Service (SaaS)	SaaS delivers complete software applications over the internet that users access through web browsers.	{"summary": "SaaS characteristics:", "breakdown": ["Complete software applications delivered over internet", "No software installation or maintenance required", "Subscription-based pricing model", "Multi-tenant architecture"], "otherOptions": "IaaS provides infrastructure components\\nPaaS provides development platforms\\nFaaS provides serverless function execution"}	\N	\N
3	3	5	Cloud Architecture - Shared Responsibility	Comprehension	Cloud Architecture and Design	In the shared responsibility model, a customer using Amazon RDS (managed database service) is responsible for which of the following?	[{"text": "Physical security of the data center", "isCorrect": false}, {"text": "Hardware maintenance and replacement", "isCorrect": false}, {"text": "Data encryption and access control configuration", "isCorrect": true}, {"text": "Database engine patching and updates", "isCorrect": false}]	Data encryption and access control configuration	In managed services like RDS, customers are responsible for data security, access controls, and encryption configuration.	{"summary": "Customer responsibilities in managed database services:", "breakdown": ["Data encryption at rest and in transit", "User access management and IAM policies", "Network security groups and firewall rules", "Backup retention and recovery testing"], "otherOptions": "AWS manages engine patching\\nAWS handles physical security\\nAWS manages hardware infrastructure"}	\N	\N
4	4	6	Cloud Architecture - Shared Responsibility	Application	Cloud Architecture and Design	Your organization is using IaaS virtual machines. According to the shared responsibility model, which security aspect is the customer responsible for?	[{"text": "Hypervisor security and maintenance", "isCorrect": false}, {"text": "Physical security of the data center", "isCorrect": false}, {"text": "Network infrastructure hardware security", "isCorrect": false}, {"text": "Operating system patches and configuration", "isCorrect": true}]	Operating system patches and configuration	In IaaS, customers are responsible for securing the guest operating system, including patching and configuration.	{"summary": "Customer responsibilities in IaaS:", "breakdown": ["Guest operating system security and patching", "Application security and configuration", "Network traffic protection (encryption)", "Identity and access management"], "otherOptions": "Physical security is provider responsibility\\nHypervisor is managed by cloud provider\\nNetwork hardware is provider responsibility"}	\N	\N
5	5	7	Cloud Architecture - Availability	Application	Cloud Architecture and Design	Your application requires 99.99% uptime and must survive the failure of an entire data center. Which architecture approach best meets these requirements?	[{"text": "Use larger instance types for better reliability", "isCorrect": false}, {"text": "Deploy across multiple availability zones in the same region", "isCorrect": true}, {"text": "Deploy in a single availability zone with multiple instances", "isCorrect": false}, {"text": "Implement only vertical scaling", "isCorrect": false}]	Deploy across multiple availability zones in the same region	Multi-AZ deployment provides data center-level fault tolerance while maintaining low latency.	{"summary": "Multi-AZ deployment benefits:", "breakdown": ["Survives entire data center failures", "Maintains low latency within region", "Automatic failover capabilities", "Meets high availability requirements (99.99%)"], "otherOptions": "Single AZ cannot survive data center failure\\nInstance size doesn't address availability zones\\nVertical scaling doesn't provide fault tolerance"}	\N	\N
9	9	11	Cloud Architecture - Storage	Comprehension	Cloud Architecture and Design	An application requires high IOPS for database operations and low-level access to storage blocks. Which storage combination is most appropriate?	[{"text": "Object storage with HDD", "isCorrect": false}, {"text": "Block storage with SSD", "isCorrect": true}, {"text": "Object storage with SSD", "isCorrect": false}, {"text": "File storage with SSD", "isCorrect": false}]	Block storage with SSD	Block storage provides low-level access ideal for databases, while SSDs deliver the high IOPS required.	{"summary": "Block storage with SSD advantages:", "breakdown": ["Block-level access optimal for database workloads", "SSD provides high IOPS and low latency", "Direct attachment to compute instances", "Suitable for transactional applications"], "otherOptions": "Object storage lacks low-level access; HDD has lower IOPS\\nFile storage not optimal for databases\\nObject storage doesn't provide block-level access"}	\N	\N
10	10	12	Cloud Architecture - Storage	Application	Cloud Architecture and Design	Your organization needs to store 100TB of archived data that is accessed once per year for compliance purposes. Which storage solution offers the best cost optimization?	[{"text": "Hot storage tier", "isCorrect": false}, {"text": "Archive storage tier", "isCorrect": true}, {"text": "Cold storage tier", "isCorrect": false}, {"text": "Warm storage tier", "isCorrect": false}]	Archive storage tier	Archive storage tier is designed for long-term retention of rarely accessed data with lowest cost.	{"summary": "Archive storage characteristics:", "breakdown": ["Lowest cost storage option", "Designed for long-term retention", "Higher retrieval times and costs", "Ideal for compliance and backup data"], "otherOptions": "Hot storage too expensive for rarely accessed data\\nWarm storage still higher cost than needed\\nCold storage more expensive than archive"}	\N	\N
11	11	13	Cloud Architecture - Network Components	Knowledge	Cloud Architecture and Design	Which network component helps reduce latency for global users by caching content at edge locations closest to them?	[{"text": "Application gateway", "isCorrect": false}, {"text": "Network load balancer", "isCorrect": false}, {"text": "Application load balancer", "isCorrect": false}, {"text": "Content Delivery Network (CDN)", "isCorrect": true}]	Content Delivery Network (CDN)	CDNs distribute content across geographically dispersed servers to minimize latency for end users.	{"summary": "CDN functionality:", "breakdown": ["Caches static content at edge locations globally", "Routes requests to nearest geographic server", "Reduces bandwidth usage and server load", "Improves website performance and user experience"], "otherOptions": "ALB distributes traffic to backend servers\\nNLB handles network layer traffic\\nApplication gateway provides secure access"}	\N	\N
12	12	14	Cloud Architecture - Network Components	Comprehension	Cloud Architecture and Design	What is the primary difference between an application load balancer and a network load balancer?	[{"text": "Application load balancer works at Layer 7, network load balancer at Layer 4", "isCorrect": true}, {"text": "Application load balancer only works with HTTP, network load balancer with HTTPS", "isCorrect": false}, {"text": "Network load balancer is more expensive than application load balancer", "isCorrect": false}, {"text": "Network load balancer provides SSL termination, application load balancer does not", "isCorrect": false}]	Application load balancer works at Layer 7, network load balancer at Layer 4	Application load balancers operate at Layer 7 (application layer) while network load balancers operate at Layer 4 (transport layer).	{"summary": "Load balancer layer differences:", "breakdown": ["ALB: Layer 7 - can route based on HTTP headers, URLs, cookies", "NLB: Layer 4 - routes based on IP protocol data", "ALB: Content-based routing capabilities", "NLB: Higher performance, lower latency"], "otherOptions": "Pricing varies by provider and usage\\nBoth can handle HTTP and HTTPS\\nBoth can provide SSL termination"}	\N	\N
13	13	15	Cloud Architecture - Network Components	Application	Cloud Architecture and Design	Your web application needs to route traffic based on URL paths (/api/* to API servers, /images/* to image servers). Which load balancer type should you use?	[{"text": "Classic load balancer", "isCorrect": false}, {"text": "Network load balancer", "isCorrect": false}, {"text": "Application load balancer", "isCorrect": true}, {"text": "Gateway load balancer", "isCorrect": false}]	Application load balancer	Application load balancers can route traffic based on URL paths, headers, and other application-layer information.	{"summary": "Application load balancer routing capabilities:", "breakdown": ["Path-based routing (/api, /images, etc.)", "Header-based routing", "Query parameter-based routing", "Cookie-based routing"], "otherOptions": "Network load balancer routes at Layer 4, cannot inspect URLs\\nClassic load balancer has limited routing capabilities\\nGateway load balancer is for traffic inspection"}	\N	\N
14	14	16	Cloud Architecture - Disaster Recovery	Knowledge	Cloud Architecture and Design	What does RTO (Recovery Time Objective) represent in disaster recovery planning?	[{"text": "The frequency of disaster recovery testing", "isCorrect": false}, {"text": "The maximum acceptable downtime after a disaster", "isCorrect": true}, {"text": "The cost of implementing disaster recovery", "isCorrect": false}, {"text": "The amount of data that can be lost during a disaster", "isCorrect": false}]	The maximum acceptable downtime after a disaster	RTO defines the maximum acceptable duration within which a system must be restored after a disruption.	{"summary": "RTO characteristics:", "breakdown": ["Maximum acceptable downtime duration", "Measured in hours, minutes, or seconds", "Drives disaster recovery strategy selection", "Affects cost and complexity of DR solutions"], "otherOptions": "That describes RPO (Recovery Point Objective)\\nCost is a factor but not what RTO measures\\nTesting frequency is separate from RTO"}	\N	\N
15	15	17	Cloud Architecture - Disaster Recovery	Comprehension	Cloud Architecture and Design	What is the difference between RPO and RTO in disaster recovery?	[{"text": "RPO is about data loss, RTO is about downtime", "isCorrect": true}, {"text": "RPO is for applications, RTO is for infrastructure", "isCorrect": false}, {"text": "RPO is about downtime, RTO is about data loss", "isCorrect": false}, {"text": "RPO and RTO both measure downtime but in different units", "isCorrect": false}]	RPO is about data loss, RTO is about downtime	RPO (Recovery Point Objective) measures acceptable data loss, while RTO (Recovery Time Objective) measures acceptable downtime.	{"summary": "RPO vs RTO:", "breakdown": ["RPO: Maximum tolerable data loss (measured in time)", "RTO: Maximum tolerable downtime", "RPO drives backup frequency requirements", "RTO drives disaster recovery strategy selection"], "otherOptions": "This reverses the definitions\\nThey measure different aspects of recovery\\nBoth apply to applications and infrastructure"}	\N	\N
16	16	18	Cloud Architecture - Disaster Recovery	Application	Cloud Architecture and Design	Your organization has an RTO of 2 hours and RPO of 30 minutes for a critical application. Which disaster recovery strategy best meets these requirements?	[{"text": "Backup and restore only", "isCorrect": false}, {"text": "Cold site with daily backups", "isCorrect": false}, {"text": "Hot site with real-time replication", "isCorrect": false}, {"text": "Warm site with automated failover", "isCorrect": true}]	Warm site with automated failover	Warm site provides the right balance of cost and recovery time to meet 2-hour RTO requirements.	{"summary": "Warm site characteristics for this scenario:", "breakdown": ["Can achieve 2-hour RTO with quick startup", "30-minute RPO achievable with frequent backups", "More cost-effective than hot site", "Automated failover reduces manual intervention"], "otherOptions": "Cold site takes too long for 2-hour RTO\\nHot site exceeds requirements and increases cost\\nBackup/restore cannot meet 2-hour RTO"}	\N	\N
17	17	19	Cloud Architecture - Multicloud	Application	Cloud Architecture and Design	Your organization wants to avoid vendor lock-in while leveraging best-of-breed services from multiple cloud providers. What strategy should you implement?	[{"text": "Single cloud deployment with multiple regions", "isCorrect": false}, {"text": "Private cloud deployment only", "isCorrect": false}, {"text": "Multicloud tenancy strategy", "isCorrect": true}, {"text": "Hybrid cloud with on-premises integration", "isCorrect": false}]	Multicloud tenancy strategy	Multicloud tenancy allows using services from multiple providers, avoiding vendor lock-in while accessing optimal services.	{"summary": "Multicloud tenancy benefits:", "breakdown": ["Avoids dependency on a single cloud provider", "Enables selection of best services from each provider", "Provides redundancy and improved reliability", "Allows cost optimization through competitive pricing"], "otherOptions": "Single cloud still creates vendor dependency\\nHybrid focuses on on-premises integration\\nPrivate cloud limits service options"}	\N	\N
18	18	20	Cloud Architecture - Multicloud	Comprehension	Cloud Architecture and Design	Which scenario best represents a valid use case for multicloud strategy?	[{"text": "Using private cloud for all applications", "isCorrect": false}, {"text": "Using the same cloud provider in multiple regions", "isCorrect": false}, {"text": "Using on-premises servers with cloud storage", "isCorrect": false}, {"text": "Using AWS for compute and Azure for AI/ML services", "isCorrect": true}]	Using AWS for compute and Azure for AI/ML services	Multicloud involves using different cloud providers for different services based on their strengths.	{"summary": "Multicloud strategy benefits:", "breakdown": ["Best-of-breed service selection", "Avoid vendor lock-in", "Improved negotiating position", "Reduced single point of failure risk"], "otherOptions": "Multiple regions with same provider is not multicloud\\nPrivate cloud only is not multicloud\\nHybrid cloud, not multicloud"}	\N	\N
19	19	21	Deployments - Migration Types	Comprehension	Cloud Deployment	Which migration strategy involves moving applications to the cloud with minimal changes, often called ""lift and shift""?	[{"text": "Rebuilding", "isCorrect": false}, {"text": "Refactoring", "isCorrect": false}, {"text": "Rehosting", "isCorrect": true}, {"text": "Rearchitecting", "isCorrect": false}]	Rehosting	Rehosting (lift and shift) moves applications to cloud with minimal modification.	{"summary": "Rehosting characteristics:", "breakdown": ["Minimal application changes required", "Fastest migration approach", "Lower initial cost and complexity", "May not fully utilize cloud benefits"], "otherOptions": "Refactoring modifies applications for cloud\\nRearchitecting redesigns for cloud-native\\nRebuilding creates new applications"}	\N	\N
20	20	22	Deployments - Migration Types	Application	Cloud Deployment	Your legacy application needs significant changes to take advantage of cloud-native features like auto-scaling and managed services. Which migration approach is most appropriate?	[{"text": "Retaining", "isCorrect": false}, {"text": "Refactoring", "isCorrect": true}, {"text": "Retiring", "isCorrect": false}, {"text": "Rehosting", "isCorrect": false}]	Refactoring	Refactoring involves modifying applications to take advantage of cloud-native features and services.	{"summary": "Refactoring characteristics:", "breakdown": ["Modifies applications for cloud optimization", "Enables use of managed services", "Improves scalability and performance", "Higher effort than rehosting but better ROI"], "otherOptions": "Rehosting doesn't modify applications\\nRetiring eliminates the application\\nRetaining keeps app on-premises"}	\N	\N
21	21	23	Deployments - Migration Types	Knowledge	Cloud Deployment	Which migration strategy involves completely redesigning an application to be cloud-native from the ground up?	[{"text": "Refactoring", "isCorrect": false}, {"text": "Rehosting", "isCorrect": false}, {"text": "Rearchitecting", "isCorrect": true}, {"text": "Replacing", "isCorrect": false}]	Rearchitecting	Rearchitecting involves completely redesigning applications to be cloud-native and take full advantage of cloud capabilities.	{"summary": "Rearchitecting characteristics:", "breakdown": ["Complete application redesign", "Full utilization of cloud-native features", "Highest development effort and cost", "Maximum long-term benefits and flexibility"], "otherOptions": "Rehosting moves with minimal changes\\nRefactoring makes modifications but not complete redesign\\nReplacing uses different software"}	\N	\N
23	23	25	Deployments - Infrastructure as Code	Comprehension	Cloud Deployment	What is the primary benefit of using declarative Infrastructure as Code templates?	[{"text": "Requires less storage space than imperative code", "isCorrect": false}, {"text": "Describes desired end state rather than step-by-step instructions", "isCorrect": true}, {"text": "Works only with specific cloud providers", "isCorrect": false}, {"text": "Faster execution than imperative scripts", "isCorrect": false}]	Describes desired end state rather than step-by-step instructions	Declarative IaC describes the desired end state, allowing the system to determine how to achieve it.	{"summary": "Declarative IaC benefits:", "breakdown": ["Describes desired infrastructure state", "System determines implementation steps", "Idempotent operations (safe to run multiple times)", "Easier to understand and maintain"], "otherOptions": "Execution speed depends on implementation\\nStorage size not a primary consideration\\nMany tools work across multiple providers"}	\N	\N
24	24	26	Deployments - Infrastructure as Code	Knowledge	Cloud Deployment	Which of the following is a key characteristic of Infrastructure as Code (IaC)?	[{"text": "Infrastructure changes require physical hardware installation", "isCorrect": false}, {"text": "Infrastructure is managed by third-party vendors only", "isCorrect": false}, {"text": "Infrastructure is managed manually through web consoles", "isCorrect": false}, {"text": "Infrastructure is defined in code and version controlled", "isCorrect": true}]	Infrastructure is defined in code and version controlled	IaC treats infrastructure as code that can be version controlled, tested, and deployed programmatically.	{"summary": "IaC key characteristics:", "breakdown": ["Infrastructure defined in code files", "Version control for infrastructure changes", "Automated deployment and provisioning", "Repeatable and consistent deployments"], "otherOptions": "IaC eliminates manual console management\\nIaC works with virtual/cloud infrastructure\\nIaC can be managed by internal teams"}	\N	\N
25	25	27	Deployments - Deployment Strategies	Application	Cloud Deployment	Your application needs zero-downtime deployment with the ability to quickly rollback if issues occur. Which deployment strategy is most appropriate?	[{"text": "Blue-green deployment", "isCorrect": true}, {"text": "Rolling deployment", "isCorrect": false}, {"text": "In-place deployment", "isCorrect": false}, {"text": "Recreate deployment", "isCorrect": false}]	Blue-green deployment	Blue-green deployment provides zero downtime and instant rollback capabilities by maintaining two identical environments.	{"summary": "Blue-green deployment characteristics:", "breakdown": ["Two identical production environments", "Instant traffic switching between environments", "Zero downtime during deployment", "Quick rollback by switching traffic back"], "otherOptions": "In-place deployment causes downtime\\nRolling deployment has slower rollback\\nRecreate deployment causes downtime"}	\N	\N
26	26	28	Deployments - Deployment Strategies	Comprehension	Cloud Deployment	What is the primary advantage of canary deployment strategy?	[{"text": "Lowest resource requirements", "isCorrect": false}, {"text": "Fastest deployment method", "isCorrect": false}, {"text": "Simplest to implement", "isCorrect": false}, {"text": "Limits impact of issues to small user subset", "isCorrect": true}]	Limits impact of issues to small user subset	Canary deployment gradually releases changes to small user groups, limiting the impact of potential issues.	{"summary": "Canary deployment benefits:", "breakdown": ["Gradual rollout to subset of users", "Early detection of issues with limited impact", "Ability to monitor and validate changes", "Reduced risk of widespread problems"], "otherOptions": "Not the fastest method\\nRequires additional monitoring infrastructure\\nMore complex than simple deployments"}	\N	\N
27	27	29	Deployments - Deployment Strategies	Knowledge	Cloud Deployment	In a rolling deployment strategy, how are application instances updated?	[{"text": "All instances are shut down before updates", "isCorrect": false}, {"text": "All instances are updated simultaneously", "isCorrect": false}, {"text": "Instances are updated one at a time or in small batches", "isCorrect": true}, {"text": "New instances are created while old ones remain", "isCorrect": false}]	Instances are updated one at a time or in small batches	Rolling deployment updates instances gradually, one at a time or in small batches, maintaining service availability.	{"summary": "Rolling deployment characteristics:", "breakdown": ["Gradual instance updates", "Maintains service availability", "Lower resource requirements than blue-green", "Slower rollback process"], "otherOptions": "That describes in-place deployment\\nThat describes blue-green deployment\\nThat would cause service interruption"}	\N	\N
28	28	30	Deployments - CI/CD	Application	Cloud Deployment	Your development team wants to automatically deploy code changes to production after passing all tests in the staging environment. Which CI/CD practice should be implemented?	[{"text": "Continuous Delivery only", "isCorrect": false}, {"text": "Continuous Deployment", "isCorrect": true}, {"text": "Manual deployment with CI", "isCorrect": false}, {"text": "Continuous Integration only", "isCorrect": false}]	Continuous Deployment	Continuous Deployment automatically releases code changes to production after passing all pipeline stages.	{"summary": "Continuous Deployment characteristics:", "breakdown": ["Fully automated pipeline from code to production", "No manual intervention required for deployment", "Requires robust testing and monitoring", "Enables rapid feature delivery to users"], "otherOptions": "CI only handles code integration\\nCD deploys to staging but requires manual production release\\nManual deployment contradicts automation goals"}	\N	\N
29	29	31	Operations - Scaling	Application	Cloud Operations and Support	Your web application experiences predictable traffic spikes every weekday from 9 AM to 5 PM. Which scaling approach would be most cost-effective?	[{"text": "Manual scaling during business hours", "isCorrect": false}, {"text": "Keeping maximum capacity running at all times", "isCorrect": false}, {"text": "Horizontal auto-scaling based on schedule and metrics", "isCorrect": true}, {"text": "Vertical scaling with larger instances", "isCorrect": false}]	Horizontal auto-scaling based on schedule and metrics	Scheduled auto-scaling with metric triggers optimizes both cost and performance for predictable patterns.	{"summary": "Auto-scaling advantages for predictable traffic:", "breakdown": ["Proactive scaling before traffic spikes", "Automatic scale-down during off-hours", "Combines scheduled and reactive scaling", "Optimizes cost while maintaining performance"], "otherOptions": "Vertical scaling requires downtime\\nManual scaling is reactive and error-prone\\nMaximum capacity wastes money during off-hours"}	\N	\N
30	30	32	Operations - Scaling	Comprehension	Cloud Operations and Support	What is the primary difference between horizontal and vertical scaling?	[{"text": "Horizontal works with databases, vertical works with web servers", "isCorrect": false}, {"text": "Horizontal adds more instances, vertical increases instance size", "isCorrect": true}, {"text": "Horizontal is automatic, vertical is manual", "isCorrect": false}, {"text": "Horizontal is cheaper, vertical is more expensive", "isCorrect": false}]	Horizontal adds more instances, vertical increases instance size	Horizontal scaling adds more instances (scale out), while vertical scaling increases the capacity of existing instances (scale up).	{"summary": "Scaling approach differences:", "breakdown": ["Horizontal: Scale out - add more instances", "Vertical: Scale up - increase instance capacity", "Horizontal: Better for distributed applications", "Vertical: Simpler but has hardware limits"], "otherOptions": "Cost depends on specific implementation\\nBoth can be automated\\nBoth work with various application types"}	\N	\N
31	31	33	Operations - Scaling	Knowledge	Cloud Operations and Support	Which scaling trigger would be most appropriate for a CPU-intensive application?	[{"text": "Network bandwidth", "isCorrect": false}, {"text": "Memory utilization", "isCorrect": false}, {"text": "Storage capacity", "isCorrect": false}, {"text": "CPU utilization", "isCorrect": true}]	CPU utilization	CPU-intensive applications should scale based on CPU utilization metrics to ensure adequate processing power.	{"summary": "Scaling trigger selection:", "breakdown": ["CPU utilization for compute-intensive workloads", "Memory utilization for memory-intensive applications", "Network bandwidth for high-throughput applications", "Custom metrics for application-specific needs"], "otherOptions": "Memory not primary bottleneck for CPU-intensive apps\\nNetwork may not be bottleneck\\nStorage not relevant for CPU-intensive scaling"}	\N	\N
32	32	34	Operations - Monitoring	Knowledge	Cloud Operations and Support	Which type of monitoring provides insights into application performance and user experience?	[{"text": "Security monitoring", "isCorrect": false}, {"text": "Network monitoring", "isCorrect": false}, {"text": "Application Performance Monitoring (APM)", "isCorrect": true}, {"text": "Infrastructure monitoring", "isCorrect": false}]	Application Performance Monitoring (APM)	APM focuses on application behavior, response times, and user experience metrics.	{"summary": "APM monitoring includes:", "breakdown": ["Application response times and throughput", "Error rates and exception tracking", "User experience and transaction traces", "Code-level performance insights"], "otherOptions": "Infrastructure monitors servers and resources\\nNetwork monitoring focuses on connectivity\\nSecurity monitoring tracks threats and vulnerabilities"}	\N	\N
33	33	35	Operations - Monitoring	Comprehension	Cloud Operations and Support	What is the primary purpose of distributed tracing in microservices architecture?	[{"text": "Measure network latency", "isCorrect": false}, {"text": "Monitor individual service performance", "isCorrect": false}, {"text": "Collect application logs", "isCorrect": false}, {"text": "Track requests across multiple services", "isCorrect": true}]	Track requests across multiple services	Distributed tracing tracks request paths through multiple services, identifying bottlenecks and failures.	{"summary": "Distributed tracing benefits:", "breakdown": ["Visualizes request journey across microservices", "Identifies latency bottlenecks in service chain", "Correlates spans across distributed components", "Enables root cause analysis for performance issues"], "otherOptions": "That's service-level monitoring\\nThat's log aggregation\\nThat's network monitoring"}	\N	\N
34	34	36	Operations - Monitoring	Application	Cloud Operations and Support	Your application is experiencing intermittent performance issues. Which observability practice would best help trace the request flow through your microservices architecture?	[{"text": "Log aggregation", "isCorrect": false}, {"text": "Alert configuration", "isCorrect": false}, {"text": "Distributed tracing", "isCorrect": true}, {"text": "Metrics monitoring", "isCorrect": false}]	Distributed tracing	Distributed tracing tracks request paths through multiple services, identifying bottlenecks and failures.	{"summary": "Distributed tracing advantages:", "breakdown": ["Visualizes request journey across microservices", "Identifies latency bottlenecks in service chain", "Correlates spans across distributed components", "Enables root cause analysis for performance issues"], "otherOptions": "Logs provide events but not request flow\\nMetrics show performance but not trace paths\\nAlerts notify of issues but don't trace flow"}	\N	\N
35	35	37	Operations - Backup Strategies	Knowledge	Cloud Operations and Support	Which backup type only captures changes made since the last full backup?	[{"text": "Snapshot backup", "isCorrect": false}, {"text": "Differential backup", "isCorrect": true}, {"text": "Incremental backup", "isCorrect": false}, {"text": "Full backup", "isCorrect": false}]	Differential backup	Differential backups capture all changes since the last full backup, not since the last backup of any type.	{"summary": "Differential backup characteristics:", "breakdown": ["Captures changes since last full backup", "Faster than full backup, slower than incremental", "Requires only full backup + latest differential for restore", "Size grows until next full backup"], "otherOptions": "Full backup captures everything\\nIncremental captures changes since last backup\\nSnapshot is point-in-time image"}	\N	\N
36	36	38	Operations - Backup Strategies	Application	Cloud Operations and Support	Your organization has an RPO of 4 hours for a critical database. The database receives constant updates throughout business hours. Which backup strategy best meets this requirement?	[{"text": "Full backup weekly with 4-hour incremental backups", "isCorrect": true}, {"text": "Weekly full backups with daily differentials", "isCorrect": false}, {"text": "Monthly full backups with weekly incrementals", "isCorrect": false}, {"text": "Daily full backups at midnight", "isCorrect": false}]	Full backup weekly with 4-hour incremental backups	Incremental backups every 4 hours ensure data loss is limited to the RPO requirement of 4 hours.	{"summary": "Backup strategy for 4-hour RPO:", "breakdown": ["Incremental backups every 4 hours meet RPO exactly", "Weekly full backups provide baseline restore point", "Captures all changes within acceptable data loss window", "Balances storage efficiency with recovery requirements"], "otherOptions": "Daily backups allow up to 24 hours data loss\\nDaily differentials still allow 24 hours data loss\\nWeekly incrementals allow up to 7 days data loss"}	\N	\N
38	38	40	Security - IAM	Application	Cloud Security	A developer needs temporary access to debug a production issue in a specific S3 bucket. What is the most secure approach following the principle of least privilege?	[{"text": "Share root account credentials", "isCorrect": false}, {"text": "Create time-limited IAM role with bucket-specific permissions", "isCorrect": true}, {"text": "Add developer to administrators group", "isCorrect": false}, {"text": "Create IAM user with permanent S3 full access", "isCorrect": false}]	Create time-limited IAM role with bucket-specific permissions	Time-limited IAM roles with specific permissions minimize security exposure while providing necessary access.	{"summary": "Secure temporary access principles:", "breakdown": ["Time-bound access that automatically expires", "Scope limited to specific resources needed", "No permanent credentials to manage", "Audit trail of role assumption"], "otherOptions": "Administrative access violates least privilege\\nPermanent access creates long-term security risk\\nRoot credentials should never be shared"}	\N	\N
39	39	41	Security - IAM	Comprehension	Cloud Security	What is the primary difference between Role-Based Access Control (RBAand Attribute-Based Access Control (ABAC)?	[{"text": "RBAC works with cloud, ABAC works with on-premises", "isCorrect": false}, {"text": "RBAC is newer technology than ABAC", "isCorrect": false}, {"text": "RBAC is more secure than ABAC", "isCorrect": false}, {"text": "RBAC uses job functions, ABAC uses multiple attributes", "isCorrect": true}]	RBAC uses job functions, ABAC uses multiple attributes	RBAC assigns permissions based on job roles, while ABAC uses multiple attributes like location, time, and resource sensitivity.	{"summary": "RBAC vs ABAC comparison:", "breakdown": ["RBAC: Access based on predefined roles", "ABAC: Access based on multiple dynamic attributes", "RBAC: Simpler to implement and manage", "ABAC: More flexible and granular control"], "otherOptions": "Security depends on implementation\\nBoth work in cloud and on-premises\\nABAC is actually newer than RBAC"}	\N	\N
40	40	42	Security - IAM	Knowledge	Cloud Security	Which authentication method provides the highest level of security for cloud access?	[{"text": "Single sign-on (SSO)", "isCorrect": false}, {"text": "Multi-factor authentication (MFA)", "isCorrect": true}, {"text": "API keys", "isCorrect": false}, {"text": "Username and password only", "isCorrect": false}]	Multi-factor authentication (MFA)	MFA requires multiple authentication factors, significantly increasing security by requiring something you know, have, or are.	{"summary": "MFA security benefits:", "breakdown": ["Requires multiple authentication factors", "Protects against credential theft", "Combines knowledge, possession, and inherence factors", "Significantly reduces unauthorized access risk"], "otherOptions": "Single factor is easily compromised\\nSSO is about convenience, not necessarily security\\nAPI keys are single factor"}	\N	\N
41	41	43	Security - Encryption	Comprehension	Cloud Security	Which encryption approach protects data while it is being transmitted between your application and cloud storage?	[{"text": "Encryption at rest", "isCorrect": false}, {"text": "File system encryption", "isCorrect": false}, {"text": "Encryption in transit", "isCorrect": true}, {"text": "Database encryption", "isCorrect": false}]	Encryption in transit	Encryption in transit protects data during transmission using protocols like TLS/SSL.	{"summary": "Encryption in transit protects:", "breakdown": ["Data moving between client and server", "API calls and responses", "File uploads and downloads", "Database connections and queries"], "otherOptions": "At rest protects stored data\\nDatabase encryption protects stored database data\\nFile system encryption protects local storage"}	\N	\N
42	42	44	Security - Encryption	Application	Cloud Security	Your organization requires that sensitive data be encrypted both when stored and when transmitted. Which encryption strategy should be implemented?	[{"text": "Both encryption at rest and in transit", "isCorrect": true}, {"text": "Encryption in transit only", "isCorrect": false}, {"text": "Application-level encryption only", "isCorrect": false}, {"text": "Encryption at rest only", "isCorrect": false}]	Both encryption at rest and in transit	Comprehensive data protection requires encrypting data both when stored (at rest) and when transmitted (in transit).	{"summary": "Complete encryption strategy includes:", "breakdown": ["Encryption at rest protects stored data", "Encryption in transit secures data movement", "Protects against both storage and network attacks", "Meets compliance requirements for data protection"], "otherOptions": "Transit-only leaves stored data vulnerable\\nRest-only leaves network traffic vulnerable\\nApplication-level alone insufficient for comprehensive protection"}	\N	\N
43	43	45	Security - Encryption	Knowledge	Cloud Security	What is the primary purpose of key management in cloud encryption?	[{"text": "To eliminate the need for encryption", "isCorrect": false}, {"text": "To reduce encryption costs", "isCorrect": false}, {"text": "To securely store, rotate, and control access to encryption keys", "isCorrect": true}, {"text": "To improve encryption performance", "isCorrect": false}]	To securely store, rotate, and control access to encryption keys	Key management ensures encryption keys are securely stored, regularly rotated, and access is properly controlled.	{"summary": "Key management responsibilities:", "breakdown": ["Secure key storage and protection", "Regular key rotation and lifecycle management", "Access control and audit logging", "Key recovery and backup procedures"], "otherOptions": "Cost reduction is not primary purpose\\nPerformance optimization is secondary\\nKey management supports encryption, not eliminates it"}	\N	\N
44	44	46	DevOps - CI/CD	Knowledge	DevOps Fundamentals	What is the primary purpose of Continuous Integration (CI) in a DevOps pipeline?	[{"text": "Integrate code changes frequently and run automated tests", "isCorrect": true}, {"text": "Manage infrastructure as code", "isCorrect": false}, {"text": "Monitor application performance", "isCorrect": false}, {"text": "Automatically deploy to production", "isCorrect": false}]	Integrate code changes frequently and run automated tests	CI focuses on frequently integrating code changes and running automated builds and tests.	{"summary": "Continuous Integration benefits:", "breakdown": ["Early detection of integration issues", "Automated build and test execution", "Frequent code integration reduces conflicts", "Faster feedback to development teams"], "otherOptions": "Production deployment is Continuous Deployment\\nPerformance monitoring is separate from CI\\nIaC management is infrastructure automation"}	\N	\N
45	45	47	DevOps - CI/CD	Comprehension	DevOps Fundamentals	What is the difference between Continuous Delivery and Continuous Deployment?	[{"text": "Continuous Delivery is for testing, Continuous Deployment is for production", "isCorrect": false}, {"text": "Continuous Delivery deploys automatically, Continuous Deployment requires manual approval", "isCorrect": false}, {"text": "They are the same thing with different names", "isCorrect": false}, {"text": "Continuous Delivery requires manual approval for production, Continuous Deployment is fully automated", "isCorrect": true}]	Continuous Delivery requires manual approval for production, Continuous Deployment is fully automated	Continuous Delivery prepares code for production deployment but requires manual approval, while Continuous Deployment automatically deploys to production.	{"summary": "CD vs CD comparison:", "breakdown": ["Continuous Delivery: Automated pipeline with manual production approval", "Continuous Deployment: Fully automated pipeline to production", "Both require robust testing and quality gates", "Continuous Deployment requires higher confidence in automation"], "otherOptions": "This reverses the definitions\\nThey have different automation levels\\nBoth involve production deployment"}	\N	\N
46	46	48	DevOps - CI/CD	Application	DevOps Fundamentals	Your development team wants to catch integration issues early and run automated tests on every code commit. Which DevOps practice should be implemented first?	[{"text": "Configuration Management", "isCorrect": false}, {"text": "Infrastructure as Code", "isCorrect": false}, {"text": "Continuous Integration", "isCorrect": true}, {"text": "Continuous Deployment", "isCorrect": false}]	Continuous Integration	Continuous Integration should be implemented first to establish automated builds and testing on every code commit.	{"summary": "CI as foundation practice:", "breakdown": ["Establishes automated build processes", "Runs tests on every code commit", "Provides immediate feedback to developers", "Foundation for more advanced DevOps practices"], "otherOptions": "CD builds on CI foundation\\nIaC is infrastructure focused\\nConfiguration management is separate concern"}	\N	\N
47	47	49	DevOps - Containers	Comprehension	DevOps Fundamentals	Which container orchestration approach is best for production environments requiring automated scaling and service discovery?	[{"text": "Containers running on a single host", "isCorrect": false}, {"text": "Standalone containers managed manually", "isCorrect": false}, {"text": "Virtual machines with containers installed", "isCorrect": false}, {"text": "Container orchestration platform like Kubernetes", "isCorrect": true}]	Container orchestration platform like Kubernetes	Container orchestration platforms provide automated management, scaling, and service discovery for production workloads.	{"summary": "Orchestration platform benefits:", "breakdown": ["Automated container lifecycle management", "Built-in scaling and load balancing", "Service discovery and networking", "Rolling updates and rollback capabilities"], "otherOptions": "Manual management doesn't scale for production\\nVMs add unnecessary overhead\\nSingle host creates single point of failure"}	\N	\N
48	48	50	DevOps - Containers	Application	DevOps Fundamentals	Your team needs to deploy a microservices application that can automatically scale, handle failures, and manage service discovery. Which approach is most suitable?	[{"text": "Serverless functions only", "isCorrect": false}, {"text": "Virtual machines with manual deployment", "isCorrect": false}, {"text": "Standalone containers on virtual machines", "isCorrect": false}, {"text": "Container orchestration with Kubernetes", "isCorrect": true}]	Container orchestration with Kubernetes	Kubernetes provides comprehensive container orchestration with auto-scaling, self-healing, and service discovery capabilities.	{"summary": "Kubernetes orchestration features:", "breakdown": ["Automatic scaling based on resource utilization", "Self-healing with pod restart and rescheduling", "Built-in service discovery and load balancing", "Rolling updates and rollback capabilities"], "otherOptions": "Standalone containers lack orchestration features\\nVMs with manual deployment don't provide automation\\nServerless alone insufficient for complex microservices"}	\N	\N
49	49	51	Troubleshooting - Performance	Application	Troubleshooting	Users report slow application response times. Your monitoring shows high CPU utilization but normal memory and disk usage. What should be your first troubleshooting step?	[{"text": "Increase memory allocation", "isCorrect": false}, {"text": "Increase disk storage capacity", "isCorrect": false}, {"text": "Restart all application servers", "isCorrect": false}, {"text": "Analyze CPU-intensive processes and optimize or scale CPU resources", "isCorrect": true}]	Analyze CPU-intensive processes and optimize or scale CPU resources	High CPU utilization directly correlates with the performance issue, making CPU analysis the logical first step.	{"summary": "CPU troubleshooting approach:", "breakdown": ["Identify CPU-intensive processes or queries", "Analyze application code for optimization opportunities", "Consider vertical scaling for more CPU power", "Implement horizontal scaling to distribute load"], "otherOptions": "Memory is not the bottleneck here\\nRestart is temporary and doesn't address root cause\\nDisk capacity is not related to CPU issues"}	\N	\N
50	50	52	Troubleshooting - Performance	Comprehension	Troubleshooting	What is the most likely cause of high IOPS (Input/Output Operations Per Second) in a cloud environment?	[{"text": "Memory leaks", "isCorrect": false}, {"text": "Network latency issues", "isCorrect": false}, {"text": "Database queries or file system operations", "isCorrect": true}, {"text": "CPU overutilization", "isCorrect": false}]	Database queries or file system operations	High IOPS typically indicates intensive database operations or file system read/write activities.	{"summary": "Common causes of high IOPS:", "breakdown": ["Database queries and transactions", "File system read/write operations", "Application logging activities", "Backup and data replication processes"], "otherOptions": "Network latency affects throughput, not IOPS\\nCPU issues don't directly cause IOPS\\nMemory leaks affect memory usage, not IOPS"}	\N	\N
51	51	53	Troubleshooting - Network	Comprehension	Troubleshooting	An application cannot connect to a database in another subnet. The database is running and accessible from other sources. What is the most likely cause?	[{"text": "Network security group or firewall blocking the connection", "isCorrect": true}, {"text": "Application code error", "isCorrect": false}, {"text": "Database server is down", "isCorrect": false}, {"text": "DNS resolution failure", "isCorrect": false}]	Network security group or firewall blocking the connection	Network connectivity issues between subnets typically involve security group or firewall configuration problems.	{"summary": "Network troubleshooting for subnet connectivity:", "breakdown": ["Check security group rules for required ports", "Verify network ACL configurations", "Ensure route table entries for subnet communication", "Confirm firewall rules on both source and destination"], "otherOptions": "Database is confirmed running and accessible\\nDNS would affect name resolution, not subnet connectivity\\nCode error wouldn't be subnet-specific"}	\N	\N
52	52	54	Troubleshooting - Network	Application	Troubleshooting	A multi-tier application can communicate between web and application tiers, but the application tier cannot reach the database tier. What should you check first?	[{"text": "Database server hardware status", "isCorrect": false}, {"text": "Application server logs", "isCorrect": false}, {"text": "Web server configuration", "isCorrect": false}, {"text": "Network security groups and routing between application and database tiers", "isCorrect": true}]	Network security groups and routing between application and database tiers	Network connectivity issues between specific tiers typically involve security group rules or routing configuration.	{"summary": "Network troubleshooting for tier connectivity:", "breakdown": ["Security groups may block database port access", "Subnet routing tables might be misconfigured", "Network ACLs could prevent tier communication", "VPC peering or transit gateway issues possible"], "otherOptions": "Web tier communication works, not a web server issue\\nApp logs won't show network configuration problems\\nHardware status wouldn't be tier-specific"}	\N	\N
53	53	55	Troubleshooting - Security	Comprehension	Troubleshooting	An application suddenly cannot access a cloud storage bucket that worked fine yesterday. No code changes were made. What is the most likely cause?	[{"text": "Storage bucket has been deleted", "isCorrect": false}, {"text": "Network connectivity issues", "isCorrect": false}, {"text": "Application server hardware failure", "isCorrect": false}, {"text": "IAM permissions or security policies changed", "isCorrect": true}]	IAM permissions or security policies changed	Sudden access failures without code changes typically indicate permission or security policy modifications.	{"summary": "Common causes of sudden access loss:", "breakdown": ["IAM role permissions modified or revoked", "Security group rules changed", "Access keys expired or rotated", "Bucket policies or ACLs updated"], "otherOptions": "Deletion would affect all access, not just this app\\nNetwork issues would show connectivity errors\\nHardware failure would cause broader application issues"}	\N	\N
54	54	56	Cloud Architecture - Microservices	Comprehension	Cloud Architecture and Design	What is the primary benefit of using microservices architecture in cloud environments?	[{"text": "Lower infrastructure costs", "isCorrect": false}, {"text": "Reduced development complexity", "isCorrect": false}, {"text": "Simplified monitoring and logging", "isCorrect": false}, {"text": "Independent scaling and deployment of services", "isCorrect": true}]	Independent scaling and deployment of services	Microservices allow each service to be developed, deployed, and scaled independently, providing flexibility and resilience.	{"summary": "Microservices benefits:", "breakdown": ["Independent service deployment and scaling", "Technology diversity across services", "Fault isolation and resilience", "Team autonomy and faster development cycles"], "otherOptions": "Actually increases complexity\\nMay increase infrastructure costs\\nMonitoring becomes more complex"}	\N	\N
55	55	57	Cloud Architecture - Managed Services	Application	Cloud Architecture and Design	Your organization wants to reduce operational overhead for database management while maintaining high availability. Which approach is most suitable?	[{"text": "Self-managed database on virtual machines", "isCorrect": false}, {"text": "Managed database service with multi-AZ deployment", "isCorrect": true}, {"text": "On-premises database with cloud backup", "isCorrect": false}, {"text": "Containerized database with manual orchestration", "isCorrect": false}]	Managed database service with multi-AZ deployment	Managed database services with multi-AZ deployment provide high availability while reducing operational overhead.	{"summary": "Managed database benefits:", "breakdown": ["Automated patching and maintenance", "Built-in high availability with multi-AZ", "Automated backups and point-in-time recovery", "Reduced operational overhead"], "otherOptions": "Self-managed increases operational overhead\\nManual orchestration increases complexity\\nOn-premises doesn't reduce overhead"}	\N	\N
56	56	58	Deployments - Version Control	Knowledge	Cloud Deployment	Which version control operation allows developers to propose changes for review before merging into the main branch?	[{"text": "Git merge", "isCorrect": false}, {"text": "Git commit", "isCorrect": false}, {"text": "Git push", "isCorrect": false}, {"text": "Pull request", "isCorrect": true}]	Pull request	Pull requests enable code review and discussion before changes are merged into the main codebase.	{"summary": "Pull request benefits:", "breakdown": ["Facilitates peer code review process", "Enables discussion and feedback on changes", "Maintains code quality through review gates", "Provides audit trail of changes and approvals"], "otherOptions": "Push uploads code to repository\\nCommit saves changes locally\\nMerge combines branches without review"}	\N	\N
57	57	59	Operations - Lifecycle Management	Comprehension	Cloud Operations and Support	What is the primary purpose of patch management in cloud environments?	[{"text": "Reduce infrastructure costs", "isCorrect": false}, {"text": "Increase storage capacity", "isCorrect": false}, {"text": "Address security vulnerabilities and bugs", "isCorrect": true}, {"text": "Improve application performance", "isCorrect": false}]	Address security vulnerabilities and bugs	Patch management primarily addresses security vulnerabilities and software bugs to maintain system security and stability.	{"summary": "Patch management objectives:", "breakdown": ["Address security vulnerabilities", "Fix software bugs and issues", "Maintain system stability", "Ensure compliance with security standards"], "otherOptions": "Performance improvements are secondary\\nCost reduction is not primary purpose\\nStorage capacity is unrelated to patching"}	\N	\N
58	58	60	Security - Compliance	Knowledge	Cloud Security	Which compliance framework is specifically designed for organizations handling credit card data?	[{"text": "GDPR", "isCorrect": false}, {"text": "HIPAA", "isCorrect": false}, {"text": "SOC 2", "isCorrect": false}, {"text": "PCI DSS", "isCorrect": true}]	PCI DSS	PCI DSS (Payment Card Industry Data Security Standard) is specifically designed for organizations that handle credit card data.	{"summary": "PCI DSS requirements:", "breakdown": ["Secure network and system configuration", "Protect cardholder data", "Maintain vulnerability management program", "Implement access control measures"], "otherOptions": "SOC 2 is for service organizations\\nGDPR is for data privacy\\nHIPAA is for healthcare data"}	\N	\N
59	59	61	Cloud Migration and Capacity Planning	Application	Cloud Architecture and Design	A retail company with 200 stores operates a legacy inventory system requiring 48 CPU cores, 256GB RAM, and 10TB storage with 15,000 IOPS. The system experiences 300% load increase during Black Friday sales. They want to migrate to the cloud with the ability to handle peak loads cost-effectively. Which migration strategy best meets these requirements?	[{"text": "Containerize the application as-is and deploy to managed Kubernetes", "isCorrect": false}, {"text": "Lift-and-shift to cloud with 3x capacity provisioned year-round", "isCorrect": false}, {"text": "Re-architect as microservices with auto-scaling and cloud-native database", "isCorrect": true}, {"text": "Hybrid approach keeping database on-premises with cloud compute", "isCorrect": false}]	Re-architect as microservices with auto-scaling and cloud-native database	Re-architecting as microservices enables auto-scaling for the 300% peak load without over-provisioning year-round, while cloud-native databases provide elastic IOPS scaling.	{"summary": "Microservices migration benefits for variable loads:", "breakdown": ["Auto-scaling handles 300% peak without year-round costs", "Cloud-native database scales IOPS on demand", "Service isolation allows scaling only needed components", "Pay-per-use model optimizes costs during normal operations"], "otherOptions": "3x capacity year-round wastes resources and budget\\nHybrid approach doesn't solve IOPS scaling challenge\\nContainerizing monolith doesn't enable granular scaling"}	\N	\N
60	60	96	Security - Identity and Access Management	Advanced	Cloud Security	A multinational corporation uses multiple cloud providers and requires centralized identity management with single sign-on capability. Users need access to resources across AWS, Azure, and on-premises systems. Which solution provides the MOST comprehensive approach?	[{"text": "Multi-factor authentication on each system independently", "isCorrect": false}, {"text": "Shared service accounts across all platforms", "isCorrect": false}, {"text": "Separate identity systems for each cloud provider", "isCorrect": false}, {"text": "Federated identity management with SAML 2.0", "isCorrect": true}]	Federated identity management with SAML 2.0	Federated identity allows single sign-on across multiple systems and cloud providers using standard protocols like SAML.	{"summary": "Federated identity benefits:", "breakdown": ["Single sign-on across multiple systems", "Centralized user management", "Works with multiple cloud providers", "Reduces password fatigue and improves security"], "otherOptions": "Creates management overhead and security gaps\\nViolates security best practices\\nDoesn't provide centralized management or SSO"}	\N	\N
61	61	62	Cloud Security and Compliance	Analysis	Cloud Security	A healthcare provider must implement cloud storage for patient records with these requirements: data encrypted at rest and in transit, 7-year retention for compliance, access logs retained for 1 year, and automatic deletion after retention period. They need to prove compliance during audits. Which security controls combination ensures all requirements are met?	[{"text": "Third-party encryption tools, backup to tape, and annual compliance audits", "isCorrect": false}, {"text": "Client-side encryption, manual lifecycle policies, and quarterly access reviews", "isCorrect": false}, {"text": "Cloud provider encryption, automated lifecycle rules, immutable audit logs, and compliance certificates", "isCorrect": true}, {"text": "Database encryption, daily backups, and manual log reviews", "isCorrect": false}]	Cloud provider encryption, automated lifecycle rules, immutable audit logs, and compliance certificates	Automated lifecycle rules ensure compliant retention and deletion, immutable audit logs provide tamper-proof evidence, and cloud provider compliance certificates demonstrate adherence to healthcare standards.	{"summary": "Healthcare compliance in cloud storage requires:", "breakdown": ["Automated lifecycle policies prevent human error in retention", "Immutable audit logs ensure tamper-proof compliance evidence", "Provider encryption meets regulatory requirements efficiently", "Compliance certificates (HIPAA, SOC2) simplify audit process"], "otherOptions": "Manual processes risk non-compliance through human error\\nTape backups don't provide automated deletion\\nManual reviews insufficient for audit requirements"}	\N	\N
62	62	63	Cloud Automation and Orchestration	Application	Cloud Deployment	A development team deploys applications across dev, test, and prod environments in multiple cloud regions. They currently spend 15 hours weekly on manual deployments with a 5% error rate causing rollbacks. Which automation approach would best reduce deployment time and errors while maintaining environment-specific configurations?	[{"text": "Infrastructure as Code with parameterized templates and CI/CD pipelines", "isCorrect": true}, {"text": "Container images with hardcoded environment settings", "isCorrect": false}, {"text": "Shell scripts with environment variables for each deployment", "isCorrect": false}, {"text": "Configuration management tools with manual approval gates", "isCorrect": false}]	Infrastructure as Code with parameterized templates and CI/CD pipelines	Infrastructure as Code with parameterized templates enables consistent deployments across environments while CI/CD pipelines automate the process, reducing both time and error rates.	{"summary": "IaC and CI/CD benefits for multi-environment deployments:", "breakdown": ["Parameterized templates handle environment-specific configs", "Version control tracks all infrastructure changes", "Automated testing catches errors before production", "Consistent deployments reduce error rate from 5% to <1%"], "otherOptions": "Shell scripts lack version control and testing capabilities\\nManual approvals don't reduce deployment time\\nHardcoded settings prevent environment flexibility"}	\N	\N
63	63	64	Cloud Performance Optimization	Analysis	Cloud Operations and Support	A SaaS application experiences intermittent performance issues reported by 15% of users. Monitoring shows normal CPU (40%), memory (60%), and network (30%) utilization. However, user session recordings reveal 3-second delays during specific database queries. What combination of tools and techniques would best identify and resolve the root cause?	[{"text": "Enable database query profiling, analyze execution plans, and implement query optimization with caching", "isCorrect": true}, {"text": "Migrate to faster storage and increase network bandwidth", "isCorrect": false}, {"text": "Increase server resources and implement load balancing", "isCorrect": false}, {"text": "Add more monitoring agents and create utilization alerts", "isCorrect": false}]	Enable database query profiling, analyze execution plans, and implement query optimization with caching	Database query profiling identifies specific slow queries, execution plan analysis reveals inefficiencies, and strategic caching prevents repeated expensive operations.	{"summary": "Database performance troubleshooting approach:", "breakdown": ["Query profiling pinpoints exact problematic queries", "Execution plans reveal missing indexes or inefficient joins", "Query optimization reduces 3-second delays to milliseconds", "Caching frequently accessed data prevents repeated slow queries"], "otherOptions": "Resources aren't the issue (40% CPU, 60% memory)\\nMore monitoring won't fix identified query delays\\nHardware upgrades don't address query inefficiency"}	\N	\N
64	64	65	Cloud Business Continuity	Application	Troubleshooting	During a cloud provider outage, a company's primary region becomes unavailable. Their disaster recovery plan activates, but the failover process takes 6 hours instead of the planned 2 hours. Post-incident analysis reveals DNS propagation delays and cold database replicas. Which improvements would most effectively achieve the 2-hour RTO target?	[{"text": "Create detailed runbooks and conduct monthly drills", "isCorrect": false}, {"text": "Increase backup frequency and add more regions", "isCorrect": false}, {"text": "Implement DNS pre-staging with low TTL and maintain warm database replicas", "isCorrect": true}, {"text": "Purchase dedicated network connections between regions", "isCorrect": false}]	Implement DNS pre-staging with low TTL and maintain warm database replicas	DNS pre-staging with low TTL ensures rapid traffic redirection while warm database replicas eliminate lengthy data loading and cache warming during failover.	{"summary": "Achieving 2-hour RTO requires addressing specific bottlenecks:", "breakdown": ["Low TTL DNS (5 minutes) enables quick traffic switching", "DNS pre-staging eliminates record creation time during disaster", "Warm replicas maintain recent data and cache, ready for traffic", "Combined approach reduces failover from 6 hours to under 2 hours"], "otherOptions": "More backups don't address DNS or cold replica issues\\nRunbooks help execution but don't fix technical delays\\nNetwork connections don't solve DNS propagation delays"}	\N	\N
65	65	97	Security - Data Protection	Intermediate	Cloud Security	A healthcare organization stores patient data in the cloud and must comply with HIPAA requirements. Which combination of security controls is MOST important for protecting PHI (Protected Health Information)?	[{"text": "Physical security controls and backup systems", "isCorrect": false}, {"text": "Network firewalls and antivirus software only", "isCorrect": false}, {"text": "Encryption at rest and role-based access controls", "isCorrect": true}, {"text": "Strong passwords and security awareness training", "isCorrect": false}]	Encryption at rest and role-based access controls	HIPAA requires encryption of PHI and strict access controls to ensure only authorized personnel can access patient data.	{"summary": "HIPAA compliance requirements:", "breakdown": ["Encryption protects data if storage is compromised", "Role-based access ensures only authorized access", "Audit trails for compliance reporting", "These are fundamental HIPAA safeguards"], "otherOptions": "Important but insufficient for HIPAA compliance\\nPhysical controls are important but not primary for cloud\\nGood practices but don't directly protect PHI"}	\N	\N
66	66	66	Cloud Cost Management	Analysis	Cloud Architecture and Design	A company's cloud bill increased 150% over 6 months despite stable user numbers. Investigation reveals: 500 unused elastic IPs, 200TB of orphaned snapshots, 50 stopped but not terminated instances, and development databases running 24/7 on production-grade hardware. Which cost optimization strategy would yield the greatest immediate savings?	[{"text": "Reduce application features to decrease resource usage", "isCorrect": false}, {"text": "Migrate to a different cloud provider with lower rates", "isCorrect": false}, {"text": "Implement resource tagging and automated cleanup policies for unused resources", "isCorrect": true}, {"text": "Negotiate enterprise discounts and purchase reserved capacity", "isCorrect": false}]	Implement resource tagging and automated cleanup policies for unused resources	Automated cleanup policies immediately eliminate costs from unused resources (IPs, snapshots, stopped instances) while resource tagging enables ongoing cost visibility and management.	{"summary": "Immediate cost reduction through resource hygiene:", "breakdown": ["Unused elastic IPs: $0.005/hour each = $1,800/month savings", "Orphaned snapshots: $0.05/GB/month = $10,000/month savings", "Stopped instances: Still incur storage costs, termination saves 100%", "Automated policies prevent future resource accumulation"], "otherOptions": "Reserved capacity doesn't address unused resources\\nMigration costs outweigh potential savings\\nFeature reduction impacts business unnecessarily"}	\N	\N
67	67	67	Cloud Network Architecture	Application	Cloud Architecture and Design	A global company needs to connect 15 branch offices to their cloud infrastructure. Each office has different bandwidth requirements (10Mbps to 1Gbps) and varying security policies. Current MPLS costs are $50,000/month. Which cloud networking solution provides the most flexible and cost-effective approach?	[{"text": "Hub-and-spoke topology with central data center", "isCorrect": false}, {"text": "Individual site-to-site VPNs for each branch office", "isCorrect": false}, {"text": "Direct dedicated connections from each office to cloud", "isCorrect": false}, {"text": "SD-WAN overlay with cloud backbone and local internet breakout", "isCorrect": true}]	SD-WAN overlay with cloud backbone and local internet breakout	SD-WAN provides flexible bandwidth allocation, policy-based routing, and leverages cost-effective internet connections while maintaining security and performance.	{"summary": "SD-WAN advantages for multi-site cloud connectivity:", "breakdown": ["Dynamic bandwidth allocation based on real-time needs", "Local internet breakout reduces backhaul costs", "Policy-based routing enforces security requirements per site", "Typical 40-60% cost reduction versus MPLS"], "otherOptions": "15 individual VPNs create management complexity\\nDedicated connections too expensive for small sites\\nHub-and-spoke creates bottlenecks and latency"}	\N	\N
68	68	68	Cloud Monitoring and Logging	Analysis	Cloud Operations and Support	An e-commerce platform processes 1 million transactions daily across 50 microservices. The ops team struggles to troubleshoot issues due to distributed logs, missing correlation IDs, and 10TB daily log volume. Which observability strategy best addresses these challenges?	[{"text": "Centralize all logs to a single database with full-text search", "isCorrect": false}, {"text": "Increase log retention and add more verbose logging", "isCorrect": false}, {"text": "Create service-specific dashboards with custom metrics", "isCorrect": false}, {"text": "Implement distributed tracing, structured logging with correlation IDs, and intelligent log sampling", "isCorrect": true}]	Implement distributed tracing, structured logging with correlation IDs, and intelligent log sampling	Distributed tracing provides end-to-end visibility, correlation IDs link related events across services, and intelligent sampling reduces volume while preserving important events.	{"summary": "Modern observability for microservices requires:", "breakdown": ["Distributed tracing shows complete request flow across services", "Correlation IDs enable tracking single transactions through 50 services", "Structured logging improves queryability and reduces storage", "Intelligent sampling keeps important events while reducing volume 80%"], "otherOptions": "Single database can't handle 10TB daily efficiently\\nMore logs worsen the volume problem\\nDashboards don't solve log correlation issues"}	\N	\N
70	70	70	Cloud Service Models	Comprehension	Cloud Architecture and Design	A software startup needs to choose between IaaS, PaaS, and SaaS solutions for their new mobile app backend. They have 3 developers, limited DevOps experience, need to reach market in 3 months, and have $10,000 monthly budget. Which approach best balances their constraints?	[{"text": "Hybrid approach with IaaS compute and SaaS databases", "isCorrect": false}, {"text": "IaaS with full control over infrastructure and custom configuration", "isCorrect": false}, {"text": "PaaS for backend services with managed databases and authentication", "isCorrect": true}, {"text": "SaaS solutions only with no custom development", "isCorrect": false}]	PaaS for backend services with managed databases and authentication	PaaS provides the right abstraction level for a small team, offering managed services that accelerate development while staying within budget and timeline constraints.	{"summary": "PaaS advantages for startups:", "breakdown": ["Managed infrastructure reduces DevOps burden on 3-person team", "Built-in services (auth, databases) accelerate 3-month timeline", "Pay-per-use model fits $10,000 budget with room to scale", "Focus remains on app development, not infrastructure"], "otherOptions": "IaaS requires DevOps expertise they lack\\nPure SaaS too limiting for custom mobile backend\\nHybrid approach adds unnecessary complexity"}	\N	\N
71	71	76	Cloud Storage Concepts	Analysis	Cloud Architecture and Design	A media company needs storage for their video editing workflow with the following requirements: high IOPS for database operations, large capacity for raw video files, and long-term archival with cost optimization. Which storage architecture provides the BEST solution?	[{"text": "Network-attached storage (NAS) for all data types", "isCorrect": false}, {"text": "Object storage for all data with automated lifecycle policies", "isCorrect": false}, {"text": "All data on high-performance SSD storage", "isCorrect": false}, {"text": "Tiered storage with SSD for databases, HDD for active files, and cold storage for archives", "isCorrect": true}]	Tiered storage with SSD for databases, HDD for active files, and cold storage for archives	Tiered storage matches storage types to specific use cases: SSD for high IOPS databases, HDD for large file capacity, cold storage for cost-effective archival.	{"summary": "Optimal tiered storage strategy:", "breakdown": ["SSD tier: High IOPS and low latency for database operations", "HDD tier: Large capacity and moderate performance for active video files", "Cold storage: Cost-effective for long-term archival with slower retrieval", "Lifecycle automation: Automatic data movement based on access patterns"], "otherOptions": "All-SSD expensive for large video files and archives\\nNAS doesn't optimize for different performance requirements\\nObject storage alone may not provide required IOPS for databases"}	\N	\N
73	73	72	DevOps - CI/CD	Analysis	DevOps Fundamentals	A software company is experiencing frequent integration issues and broken builds after developers merge their code. They also have a slow release cycle. Which two (2) DevOps practices should they prioritize to address these problems?	[{"text": "Implement Continuous Deployment (Cand roll back frequently.", "isCorrect": false}, {"text": "Implement Continuous Integration (CI) and establish automated testing.", "isCorrect": true}, {"text": "Focus on manual code reviews and increase documentation efforts.", "isCorrect": false}, {"text": "Adopt a Microservices architecture and use serverless functions.", "isCorrect": false}]	Implement Continuous Integration (CI) and establish automated testing.	Continuous Integration (CI) focuses on frequent code integration and automated testing to catch issues early, while establishing automated testing verifies code quality and functionality, addressing broken builds and integration problems. These are foundational to speeding up the release cycle.	{"summary": "Addressing integration issues and slow releases with CI and automated testing:", "breakdown": ["**Continuous Integration (CI):** Integrates code changes frequently (multiple times a day), reducing integration hell and catching conflicts early.", "**Automated Testing:** Runs tests on every code commit or integration, immediately identifying broken builds and ensuring code quality and functionality before merging.", "These two practices are fundamental for ensuring a stable codebase and enabling faster, more reliable releases."], "otherOptions": "Microservices and serverless functions are architectural choices that may *support* better CI/CD, but do not directly solve existing integration issues or broken builds. \\nManual code reviews are important but often too slow and cannot reliably catch all integration issues in a fast-paced environment. Increasing documentation does not solve technical problems. \\nImplementing Continuous Deployment *before* fixing CI issues (broken builds, frequent integration problems) would lead to deploying broken software to production more rapidly, exacerbating the problem. Frequent rollbacks indicate a problematic CI/CD pipeline, not a solution."}	\N	\N
74	74	73	Cloud Service Models	Knowledge	Cloud Architecture and Design	A company wants to migrate their email system to the cloud but maintain full control over the operating system and middleware while letting the cloud provider manage the underlying infrastructure. Which service model BEST meets their requirements?	[{"text": "Function as a Service (FaaS)", "isCorrect": false}, {"text": "Software as a Service (SaaS)", "isCorrect": false}, {"text": "Infrastructure as a Service (IaaS)", "isCorrect": true}, {"text": "Platform as a Service (PaaS)", "isCorrect": false}]	Infrastructure as a Service (IaaS)	IaaS provides virtual infrastructure while allowing customers to maintain control over the operating system, middleware, and applications.	{"summary": "IaaS characteristics for email migration:", "breakdown": ["Customer controls: OS, middleware, applications, and data", "Provider manages: Physical hardware, hypervisor, networking", "Email flexibility: Can install any email server software", "Full administrative access to customize configurations"], "otherOptions": "SaaS provides ready-to-use applications with no OS control\\nPaaS abstracts OS layer, limiting administrative control\\nFaaS is for serverless functions, not email systems"}	\N	\N
75	75	74	Cloud Service Models	Application	Cloud Architecture and Design	A development team needs a cloud environment where they can deploy applications without managing servers, operating systems, or runtime environments. They want to focus solely on code development and automatic scaling based on demand. Which service model is MOST appropriate?	[{"text": "Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "Desktop as a Service (DaaS)", "isCorrect": false}, {"text": "Software as a Service (SaaS)", "isCorrect": false}, {"text": "Platform as a Service (PaaS)", "isCorrect": true}]	Platform as a Service (PaaS)	PaaS provides a development platform with automated scaling, allowing developers to focus on code while the platform handles infrastructure management.	{"summary": "PaaS benefits for development teams:", "breakdown": ["Abstracted infrastructure: No server or OS management needed", "Built-in scaling: Automatic resource allocation based on demand", "Development tools: Integrated IDEs, databases, and services", "Focus on code: Developers concentrate on application logic"], "otherOptions": "IaaS requires managing servers and OS\\nSaaS provides ready-made applications, not development platforms\\nDaaS provides virtual desktops, not development platforms"}	\N	\N
76	76	75	Shared Responsibility Model	Application	Cloud Architecture and Design	A healthcare organization is concerned about data security compliance in their PaaS deployment. According to the shared responsibility model, which security aspects remain the customer's responsibility in a PaaS environment?	[{"text": "Physical security of data centers and network controls", "isCorrect": false}, {"text": "Hardware maintenance and power/cooling systems", "isCorrect": false}, {"text": "Application code security, data encryption, and user access management", "isCorrect": true}, {"text": "Hypervisor patching and host operating system security", "isCorrect": false}]	Application code security, data encryption, and user access management	In PaaS, customers are responsible for application-layer security including code security, data encryption, and identity/access management.	{"summary": "PaaS customer responsibilities:", "breakdown": ["Application security: Secure coding practices and vulnerability management", "Data protection: Encryption at rest and in transit", "Identity management: User authentication and authorization", "Compliance: Meeting regulatory requirements for data handling"], "otherOptions": "Physical security is provider responsibility\\nPlatform infrastructure managed by provider\\nHardware and facilities managed by provider"}	\N	\N
77	77	77	Container Technologies	Application	Cloud Architecture and Design	A company runs microservices using standalone containers but experiences challenges with scaling, service discovery, and load balancing during peak traffic. Which approach would BEST address these operational challenges?	[{"text": "Implement container orchestration with Kubernetes or Docker Swarm", "isCorrect": true}, {"text": "Use container registries for better image management", "isCorrect": false}, {"text": "Migrate all containers to virtual machines", "isCorrect": false}, {"text": "Increase container resource limits and add more standalone containers", "isCorrect": false}]	Implement container orchestration with Kubernetes or Docker Swarm	Container orchestration platforms provide automated scaling, service discovery, load balancing, and health management for containerized applications.	{"summary": "Container orchestration benefits:", "breakdown": ["Auto-scaling: Automatic container scaling based on demand", "Service discovery: Automatic service registration and routing", "Load balancing: Built-in traffic distribution across containers", "Health management: Automatic restart of failed containers"], "otherOptions": "Manual scaling doesn't solve automation challenges\\nVMs don't provide the orchestration features needed\\nRegistries help with image management but not runtime orchestration"}	\N	\N
78	78	78	Cloud Deployment Models	Application	Cloud Deployment	A financial institution requires strict data sovereignty, complete infrastructure control, and the ability to meet regulatory compliance requirements while gaining cloud benefits like scalability and self-service provisioning. Which deployment model is MOST suitable?	[{"text": "Public cloud with dedicated instances", "isCorrect": false}, {"text": "Hybrid cloud with data replication", "isCorrect": false}, {"text": "Private cloud hosted on-premises", "isCorrect": true}, {"text": "Community cloud shared with other financial institutions", "isCorrect": false}]	Private cloud hosted on-premises	Private cloud provides complete control, data sovereignty, and regulatory compliance while offering cloud capabilities like automation and scalability.	{"summary": "Private cloud benefits for financial institutions:", "breakdown": ["Complete control: Full authority over infrastructure and security", "Data sovereignty: Data remains within institutional boundaries", "Regulatory compliance: Easier to meet strict financial regulations", "Cloud benefits: Self-service, automation, and scalability features"], "otherOptions": "Public cloud may not meet data sovereignty requirements\\nHybrid cloud introduces complexity for strict compliance needs\\nCommunity cloud shares resources with other organizations"}	\N	\N
79	79	79	Cloud Migration	Analysis	Cloud Deployment	A company has a legacy monolithic application that works well but has outdated dependencies and architecture. They want to move to the cloud quickly while minimizing risk, then modernize later. Which migration strategy is MOST appropriate for the initial move?	[{"text": "Replace with a SaaS solution", "isCorrect": false}, {"text": "Rebuild the entire application using cloud-native services", "isCorrect": false}, {"text": "Refactor to microservices architecture immediately", "isCorrect": false}, {"text": "Rehost (lift and shift) to move quickly with minimal changes", "isCorrect": true}]	Rehost (lift and shift) to move quickly with minimal changes	Rehosting allows quick migration with minimal risk and changes, providing immediate cloud benefits while enabling future modernization phases.	{"summary": "Rehost strategy advantages:", "breakdown": ["Speed: Fastest migration approach with minimal changes", "Low risk: Preserves existing functionality and stability", "Immediate benefits: Cost savings and basic cloud features", "Future flexibility: Provides foundation for later modernization"], "otherOptions": "Refactoring increases complexity and migration risk\\nRebuilding takes significant time and resources\\nSaaS replacement may not maintain existing functionality"}	\N	\N
81	81	81	Cloud Observability	Analysis	Cloud Operations and Support	A microservices application experiences intermittent performance issues that are difficult to trace across multiple services. Standard monitoring shows healthy individual services, but users report slow response times. Which observability approach would BEST identify the root cause?	[{"text": "Set up alerting based on response time thresholds", "isCorrect": false}, {"text": "Implement distributed tracing across microservices", "isCorrect": true}, {"text": "Add more performance counters and metrics", "isCorrect": false}, {"text": "Increase log verbosity on all services", "isCorrect": false}]	Implement distributed tracing across microservices	Distributed tracing follows requests across multiple microservices, providing visibility into the complete request path and identifying bottlenecks.	{"summary": "Distributed tracing benefits:", "breakdown": ["End-to-end visibility: Tracks requests across all microservices", "Bottleneck identification: Shows where delays occur in the request path", "Service dependencies: Maps interactions between services", "Performance analysis: Measures latency at each service hop"], "otherOptions": "More logs don't provide cross-service correlation\\nAdditional metrics don't show service interactions\\nAlerting identifies problems but doesn't show root cause"}	\N	\N
82	82	82	Cloud Scaling	Application	Cloud Operations and Support	An e-commerce application experiences predictable traffic patterns with gradual increases during business hours and sudden spikes during flash sales. Which auto-scaling strategy would provide the BEST performance and cost optimization?	[{"text": "Predictive scaling with scheduled scaling for business hours and reactive scaling for spikes", "isCorrect": true}, {"text": "Manual scaling based on sales calendar events", "isCorrect": false}, {"text": "Fixed scaling with maximum capacity provisioned at all times", "isCorrect": false}, {"text": "Reactive scaling based only on CPU utilization", "isCorrect": false}]	Predictive scaling with scheduled scaling for business hours and reactive scaling for spikes	Combined predictive and reactive scaling handles both predictable patterns efficiently and responds to unexpected spikes automatically.	{"summary": "Hybrid scaling strategy benefits:", "breakdown": ["Predictive scaling: Anticipates business hour traffic increases", "Reactive scaling: Responds automatically to unexpected spikes", "Cost optimization: Scales down during low-traffic periods", "Performance assurance: Maintains responsiveness during all scenarios"], "otherOptions": "CPU-only reactive scaling too slow for sudden spikes\\nFixed capacity wastes resources during low traffic\\nManual scaling can't respond quickly to unexpected events"}	\N	\N
83	83	83	Cloud Backup Strategies	Analysis	Cloud Operations and Support	A company needs to design a backup strategy for critical business data with a Recovery Point Objective (RPO) of 1 hour and Recovery Time Objective (RTO) of 2 hours. The solution must be cost-effective while meeting compliance requirements for 7-year retention. Which backup approach is MOST suitable?	[{"text": "Weekly full backups with manual restore processes", "isCorrect": false}, {"text": "Daily full backups with 7-year retention in hot storage", "isCorrect": false}, {"text": "Real-time replication to a secondary site with immediate failover", "isCorrect": false}, {"text": "Hourly incremental backups with tiered storage and lifecycle policies", "isCorrect": true}]	Hourly incremental backups with tiered storage and lifecycle policies	Hourly incremental backups meet the RPO requirement, while tiered storage and lifecycle policies optimize costs for long-term retention.	{"summary": "Optimal backup strategy components:", "breakdown": ["Hourly incrementals: Meet 1-hour RPO requirement efficiently", "Tiered storage: Hot storage for recent backups, cold for long-term", "Lifecycle policies: Automatic movement to cheaper storage over time", "Fast recovery: 2-hour RTO achievable from recent incremental backups"], "otherOptions": "Daily backups exceed 1-hour RPO requirement\\nReal-time replication expensive for 7-year retention\\nWeekly backups far exceed RPO requirement"}	\N	\N
84	84	84	Cloud Security - Access Management	Application	Cloud Security	A global organization needs to manage user access to cloud resources across multiple locations with different security requirements. They want to implement Zero Trust principles while maintaining user productivity. Which approach BEST achieves these goals?	[{"text": "Role-based access control with periodic access reviews", "isCorrect": false}, {"text": "Single sign-on with basic username/password authentication", "isCorrect": false}, {"text": "VPN access with network-based security controls", "isCorrect": false}, {"text": "Multi-factor authentication with conditional access policies based on context", "isCorrect": true}]	Multi-factor authentication with conditional access policies based on context	Conditional access with MFA implements Zero Trust by continuously verifying users based on context like location, device, and behavior patterns.	{"summary": "Zero Trust conditional access benefits:", "breakdown": ["Context awareness: Considers location, device, time, and behavior", "Continuous verification: Doesn't trust based solely on network location", "Risk-based decisions: Adjusts requirements based on calculated risk", "User productivity: Seamless access for low-risk scenarios"], "otherOptions": "VPN assumes trust based on network location\\nBasic authentication insufficient for Zero Trust\\nRBAC alone doesn't provide continuous verification"}	\N	\N
85	85	85	Cloud Compliance	Analysis	Cloud Security	A healthcare organization moving to the cloud must demonstrate HIPAA compliance for patient data. They need automated compliance monitoring, evidence collection, and remediation capabilities. Which combination of cloud security controls provides the MOST comprehensive compliance framework?	[{"text": "Network segmentation with firewall rules and access logs", "isCorrect": false}, {"text": "Manual security audits with document-based evidence collection", "isCorrect": false}, {"text": "Encryption of all data with annual compliance reviews", "isCorrect": false}, {"text": "Cloud security posture management (CSPM) with automated policy enforcement and audit trails", "isCorrect": true}]	Cloud security posture management (CSPM) with automated policy enforcement and audit trails	CSPM provides continuous compliance monitoring, automated policy enforcement, and detailed audit trails required for HIPAA compliance demonstration.	{"summary": "CSPM benefits for HIPAA compliance:", "breakdown": ["Continuous monitoring: Real-time compliance posture assessment", "Automated enforcement: Immediate remediation of policy violations", "Audit trails: Comprehensive logging for compliance evidence", "Risk assessment: Identifies and prioritizes compliance gaps"], "otherOptions": "Manual audits don't provide continuous compliance monitoring\\nEncryption alone doesn't address all HIPAA requirements\\nNetwork controls are part of compliance but not comprehensive"}	\N	\N
86	86	86	Cloud Security - Vulnerability Management	Application	Cloud Security	A development team deploys applications using container images from various sources. Security scans reveal vulnerabilities in base images and third-party components. Which approach provides the BEST security posture for the container supply chain?	[{"text": "Implement security scanning throughout the CI/CD pipeline with policy enforcement", "isCorrect": true}, {"text": "Perform monthly vulnerability assessments on deployed containers", "isCorrect": false}, {"text": "Scan containers only in production environments", "isCorrect": false}, {"text": "Use only official base images from operating system vendors", "isCorrect": false}]	Implement security scanning throughout the CI/CD pipeline with policy enforcement	Pipeline security scanning catches vulnerabilities early, enforces security policies, and prevents vulnerable images from reaching production.	{"summary": "CI/CD security scanning benefits:", "breakdown": ["Shift-left security: Identifies vulnerabilities early in development", "Policy enforcement: Blocks deployment of vulnerable images", "Continuous scanning: Monitors throughout the software lifecycle", "Supply chain security: Validates all components and dependencies"], "otherOptions": "Production-only scanning allows vulnerabilities to reach live systems\\nOfficial images can still contain vulnerabilities\\nMonthly scans too infrequent for active development"}	\N	\N
87	87	87	DevOps CI/CD	Application	DevOps Fundamentals	A development team wants to implement automated deployments while ensuring code quality and minimizing deployment risks. They currently perform manual testing and deployment processes. Which CI/CD pipeline design BEST balances automation with quality assurance?	[{"text": "Automated deployment only to development environments", "isCorrect": false}, {"text": "Manual build with automated deployment to all environments", "isCorrect": false}, {"text": "Automated build and deployment without testing stages", "isCorrect": false}, {"text": "Automated build, test, and staged deployment with approval gates", "isCorrect": true}]	Automated build, test, and staged deployment with approval gates	Staged deployment with automated testing and approval gates provides comprehensive automation while maintaining quality controls and risk mitigation.	{"summary": "Comprehensive CI/CD pipeline benefits:", "breakdown": ["Automated testing: Catches issues early in the pipeline", "Staged deployment: Progressive rollout reduces risk", "Approval gates: Human oversight for critical stages", "Quality assurance: Multiple validation points ensure code quality"], "otherOptions": "No testing increases deployment risk\\nManual build defeats automation benefits\\nLimited to dev environments doesn't provide full deployment automation"}	\N	\N
88	88	88	DevOps Version Control	Knowledge	DevOps Fundamentals	A DevOps team manages both application code and infrastructure configurations. They need to implement version control strategies that support collaboration, change tracking, and rollback capabilities. Which approach provides the MOST comprehensive version management?	[{"text": "Version control for application code only, with manual infrastructure management", "isCorrect": false}, {"text": "Single repository with unified branching strategy for both code and infrastructure", "isCorrect": true}, {"text": "Multiple repositories per microservice with independent versioning", "isCorrect": false}, {"text": "Separate repositories for code and infrastructure with different branching strategies", "isCorrect": false}]	Single repository with unified branching strategy for both code and infrastructure	Unified repository and branching strategy ensures synchronized changes between application code and infrastructure, simplifying deployment and rollback procedures.	{"summary": "Unified version control benefits:", "breakdown": ["Synchronized changes: Code and infrastructure changes tracked together", "Simplified rollbacks: Single point to revert both code and infrastructure", "Consistent branching: Same workflow for all team members", "Atomic deployments: Code and infrastructure deployed as single unit"], "otherOptions": "Separate repositories can lead to version mismatches\\nManual infrastructure management introduces inconsistency\\nMultiple repositories increase complexity and coordination overhead"}	\N	\N
89	89	89	Cloud Troubleshooting - Network	Analysis	Troubleshooting	Users report intermittent connectivity issues to a cloud-hosted web application. The application works fine from the office but fails sporadically from remote locations. Network monitoring shows no infrastructure issues. Which troubleshooting approach would MOST effectively identify the root cause?	[{"text": "Analyze network paths and implement distributed monitoring from multiple locations", "isCorrect": true}, {"text": "Review application performance metrics and database queries", "isCorrect": false}, {"text": "Check firewall logs and security group configurations", "isCorrect": false}, {"text": "Increase server resources and add more instances", "isCorrect": false}]	Analyze network paths and implement distributed monitoring from multiple locations	Distributed monitoring from multiple geographic locations helps identify network path issues, ISP problems, or regional connectivity challenges.	{"summary": "Distributed network troubleshooting approach:", "breakdown": ["Geographic perspective: Monitoring from affected user locations", "Network path analysis: Traces routes to identify bottlenecks", "ISP correlation: Identifies provider-specific issues", "Performance baselines: Compares connectivity quality across locations"], "otherOptions": "Resource scaling doesn't address location-specific connectivity\\nSecurity configurations affect access, not intermittent connectivity\\nApplication metrics don't reveal network path issues"}	\N	\N
90	90	90	Cloud Troubleshooting - Performance	Analysis	Troubleshooting	A cloud application experiences performance degradation only during specific hours despite consistent user load. CPU and memory utilization remain within normal ranges. Database queries show normal execution times. Which factor is MOST likely causing the performance issues?	[{"text": "Database connection pool exhaustion", "isCorrect": false}, {"text": "Application memory leaks accumulating over time", "isCorrect": false}, {"text": "Network bandwidth limitations during peak traffic", "isCorrect": false}, {"text": "Resource contention with other workloads sharing the same physical infrastructure", "isCorrect": true}]	Resource contention with other workloads sharing the same physical infrastructure	Time-specific performance issues with normal resource utilization often indicate "noisy neighbor" problems where other workloads compete for underlying physical resources.	{"summary": "Noisy neighbor characteristics:", "breakdown": ["Time correlation: Performance degrades at specific, recurring times", "Normal metrics: Application-level resources appear adequate", "Shared infrastructure: Multiple workloads compete for physical resources", "External dependency: Performance affected by factors outside direct control"], "otherOptions": "Memory leaks would show gradually increasing memory usage\\nConnection pool issues would show in database connection metrics\\nNetwork bandwidth problems would show in network utilization metrics"}	\N	\N
91	91	91	Cloud Troubleshooting - Security	Application	Troubleshooting	A cloud application suddenly starts receiving "Access Denied" errors for API calls that were previously working. No code changes were deployed recently. Security logs show successful authentication but failed authorization. Which troubleshooting approach would MOST quickly identify the issue?	[{"text": "Examine application logs for authentication token issues", "isCorrect": false}, {"text": "Verify SSL certificates and encryption configurations", "isCorrect": false}, {"text": "Review recent changes to IAM roles, policies, and resource permissions", "isCorrect": true}, {"text": "Check network security group rules and firewall configurations", "isCorrect": false}]	Review recent changes to IAM roles, policies, and resource permissions	Access Denied with successful authentication indicates an authorization problem, typically caused by recent changes to IAM policies or role permissions.	{"summary": "Authorization troubleshooting focus areas:", "breakdown": ["IAM policy changes: Recent modifications to access permissions", "Role updates: Changes to role assignments or capabilities", "Resource permissions: Updates to resource-specific access controls", "Time-based policies: Scheduled changes or policy expirations"], "otherOptions": "Network rules affect connectivity, not authorization after authentication\\nSSL issues would prevent successful authentication\\nAuthentication is working; the issue is with authorization"}	\N	\N
92	92	92	Cloud Troubleshooting - Integration	Expert	Troubleshooting	After migrating a legacy application to the cloud, users report that batch processing jobs that completed in 2 hours on-premises now take 6 hours in the cloud. The application code was not modified during migration. Which factors should be investigated FIRST to identify the performance degradation?	[{"text": "Network latency between cloud services and data storage locations", "isCorrect": false}, {"text": "Application timeout settings and connection pool configurations", "isCorrect": false}, {"text": "Cloud instance sizing and compute resources compared to on-premises hardware", "isCorrect": false}, {"text": "Storage I/O performance and disk configuration differences", "isCorrect": true}]	Storage I/O performance and disk configuration differences	Batch processing performance is often heavily dependent on storage I/O patterns, which can be significantly different between on-premises and cloud storage configurations.	{"summary": "Storage I/O impact on batch processing:", "breakdown": ["I/O patterns: Batch jobs typically involve intensive read/write operations", "Storage types: Cloud storage may have different performance characteristics", "Configuration differences: RAID, caching, and optimization settings", "Sequential vs random: Batch workloads often require high sequential throughput"], "otherOptions": "Compute resources would show in CPU/memory utilization\\nNetwork latency affects real-time applications more than batch processing\\nConfiguration issues would likely cause failures, not just slower performance"}	\N	\N
93	93	93	Cloud Architecture - Deployment Models	Intermediate	Cloud Architecture and Design	A financial services company requires complete control over their cloud infrastructure while meeting strict compliance requirements. They want to leverage cloud benefits but cannot share physical hardware with other organizations. Which deployment model BEST meets these requirements?	[{"text": "Hybrid cloud with encrypted connections", "isCorrect": false}, {"text": "Public cloud with dedicated tenancy", "isCorrect": false}, {"text": "Community cloud with financial sector partners", "isCorrect": false}, {"text": "Private cloud with on-premises infrastructure", "isCorrect": true}]	Private cloud with on-premises infrastructure	Private cloud provides dedicated infrastructure, complete control, and meets compliance requirements without sharing hardware.	{"summary": "Private cloud characteristics:", "breakdown": ["Complete infrastructure control", "No shared hardware with other organizations", "Meets strict compliance requirements", "Maintains cloud benefits like scalability"], "otherOptions": "Public cloud still shares hardware\\nHybrid involves public cloud components\\nCommunity cloud shares with other organizations"}	\N	\N
94	94	94	Cloud Architecture - Service Models	Beginner	Cloud Architecture and Design	Your development team wants to deploy applications without managing operating systems, runtime environments, or middleware. Which cloud service model provides this capability?	[{"text": "Software as a Service (SaaS)", "isCorrect": false}, {"text": "Function as a Service (FaaS)", "isCorrect": false}, {"text": "Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "Platform as a Service (PaaS)", "isCorrect": true}]	Platform as a Service (PaaS)	PaaS provides a platform for developing and deploying applications without managing underlying infrastructure components.	{"summary": "PaaS eliminates infrastructure management:", "breakdown": ["Provides development platforms and runtime environments", "Manages OS, middleware, and runtime automatically", "Developers focus on application code only", "Examples: Azure App Service, Google App Engine"], "otherOptions": "IaaS requires OS and middleware management\\nSaaS provides complete applications, not development platforms\\nFaaS is for serverless functions, not full applications"}	\N	\N
95	95	95	Cloud Architecture - Scaling Strategies	Intermediate	Cloud Architecture and Design	An e-commerce application experiences predictable traffic spikes during holiday seasons. The current infrastructure manually scales servers, causing delays and potential revenue loss. Which scaling approach provides the MOST efficient solution?	[{"text": "Load balancing across existing servers only", "isCorrect": false}, {"text": "Vertical scaling with larger instances during peak periods", "isCorrect": false}, {"text": "Horizontal scaling with auto-scaling groups", "isCorrect": true}, {"text": "Manual scaling with pre-provisioned servers", "isCorrect": false}]	Horizontal scaling with auto-scaling groups	Auto-scaling automatically adds/removes instances based on demand, providing cost efficiency and handling unpredictable traffic patterns.	{"summary": "Auto-scaling benefits:", "breakdown": ["Automatically responds to demand changes", "Cost-effective: pay only for needed resources", "Handles unpredictable traffic patterns", "Reduces manual intervention and delays"], "otherOptions": "Vertical scaling has limits and potential downtime\\nManual scaling causes delays and inefficiency\\nLoad balancing without scaling doesn't add capacity"}	\N	\N
96	96	98	Operations - Monitoring and Logging	Advanced	Cloud Operations and Support	A cloud application experiences intermittent performance issues that are difficult to reproduce. The operations team needs comprehensive visibility into application performance, user experience, and infrastructure metrics. Which monitoring approach provides the BEST observability?	[{"text": "Network monitoring with bandwidth analysis", "isCorrect": false}, {"text": "Application Performance Monitoring (APM) with distributed tracing", "isCorrect": true}, {"text": "Log aggregation with keyword searching", "isCorrect": false}, {"text": "Infrastructure monitoring with basic alerting", "isCorrect": false}]	Application Performance Monitoring (APM) with distributed tracing	APM with distributed tracing provides comprehensive visibility into application performance across all components and services.	{"summary": "APM with distributed tracing provides:", "breakdown": ["End-to-end transaction visibility", "Performance bottleneck identification", "Service dependency mapping", "Real user experience monitoring"], "otherOptions": "Infrastructure monitoring lacks application-level insights\\nLog aggregation is reactive, not proactive\\nNetwork monitoring only covers network layer issues"}	\N	\N
97	97	99	Operations - Backup and Recovery	Intermediate	Cloud Operations and Support	A company's critical database requires a Recovery Point Objective (RPO) of 15 minutes and Recovery Time Objective (RTO) of 1 hour. Which backup and recovery strategy BEST meets these requirements?	[{"text": "Hourly incremental backups with manual restoration", "isCorrect": false}, {"text": "Real-time snapshots with 4-hour restoration window", "isCorrect": false}, {"text": "Daily full backups with weekly testing", "isCorrect": false}, {"text": "Continuous data replication with automated failover", "isCorrect": true}]	Continuous data replication with automated failover	Continuous replication ensures minimal data loss (15-minute RPO) and automated failover meets the 1-hour RTO requirement.	{"summary": "Meeting RPO/RTO requirements:", "breakdown": ["Continuous replication: minimal data loss", "Automated failover: fast recovery time", "15-minute RPO: very recent data recovery", "1-hour RTO: quick service restoration"], "otherOptions": "Daily backups exceed RPO requirements\\nManual restoration may exceed RTO\\n4-hour restoration exceeds RTO requirement"}	\N	\N
98	98	100	Operations - Automation and Orchestration	Advanced	Cloud Operations and Support	A company needs to deploy identical applications across multiple cloud environments (AWS, Azure, GCP) with consistent configuration and automated updates. Which approach provides the BEST multi-cloud orchestration?	[{"text": "Container orchestration with Kubernetes only", "isCorrect": false}, {"text": "Native cloud provider tools for each environment", "isCorrect": false}, {"text": "Manual deployment procedures with documentation", "isCorrect": false}, {"text": "Infrastructure as Code with cloud-agnostic tools like Terraform", "isCorrect": true}]	Infrastructure as Code with cloud-agnostic tools like Terraform	Terraform provides cloud-agnostic infrastructure provisioning with consistent syntax and state management across multiple cloud providers.	{"summary": "Multi-cloud IaC benefits:", "breakdown": ["Cloud-agnostic: Single tool for multiple providers", "Consistent configuration: Same syntax across environments", "Version control: Infrastructure changes tracked", "Automated deployment: Reduces manual errors"], "otherOptions": "Native tools create vendor lock-in and inconsistency\\nManual procedures are error-prone and don't scale\\nKubernetes handles container orchestration, not infrastructure provisioning"}	\N	\N
99	99	101	Troubleshooting - Performance Issues	Advanced	Troubleshooting	Users report slow application response times during peak hours. Monitoring shows high CPU utilization on application servers but normal database performance. Network latency is within acceptable ranges. Which troubleshooting approach should be the FIRST priority?	[{"text": "Implement application server horizontal scaling", "isCorrect": true}, {"text": "Optimize database query performance", "isCorrect": false}, {"text": "Upgrade network bandwidth capacity", "isCorrect": false}, {"text": "Increase database connection pool size", "isCorrect": false}]	Implement application server horizontal scaling	High CPU utilization on application servers indicates the bottleneck is at the compute layer, requiring additional server capacity.	{"summary": "Performance troubleshooting methodology:", "breakdown": ["Identify the bottleneck component (application servers)", "Address the root cause (CPU utilization)", "Scale horizontally for load distribution", "Monitor results and adjust as needed"], "otherOptions": "Database performance is normal\\nNetwork latency is acceptable\\nDatabase isn't the performance bottleneck"}	\N	\N
100	100	102	Troubleshooting - Network Connectivity	Intermediate	Troubleshooting	A cloud application deployed across multiple availability zones experiences intermittent connectivity issues between services. Some requests succeed while others timeout. Which troubleshooting steps should be performed FIRST?	[{"text": "Increase instance sizes across all zones", "isCorrect": false}, {"text": "Contact cloud provider support immediately", "isCorrect": false}, {"text": "Restart all application services", "isCorrect": false}, {"text": "Check security group rules and network ACLs", "isCorrect": true}]	Check security group rules and network ACLs	Intermittent connectivity issues often indicate network-level blocking. Security groups and NACLs are the most common cause of partial connectivity problems.	{"summary": "Network troubleshooting approach:", "breakdown": ["Security groups: Instance-level firewall rules", "Network ACLs: Subnet-level traffic control", "Intermittent issues: Often indicate partial blocking", "Rule verification: Check allowed ports and protocols"], "otherOptions": "Service restart doesn't address network-level issues\\nInstance size doesn't affect connectivity\\nShould troubleshoot systematically before escalating"}	\N	\N
101	101	238	Cloud Operations - Log Management	Application	Cloud Operations and Support	A healthcare organization operates 200 cloud-based application servers across multiple regions. They need centralized log collection for compliance auditing and must ensure accurate timestamps for all log entries. Which TWO solutions should be implemented? (Choose TWO)	[{"text": "Deploy log aggregation agents on each server", "isCorrect": false}, {"text": "Configure centralized syslog forwarding", "isCorrect": true}, {"text": "Enable NTP synchronization across all servers", "isCorrect": true}, {"text": "Implement local log rotation policies", "isCorrect": false}, {"text": "Use cloud provider managed logging service", "isCorrect": false}]	Configure centralized syslog forwarding, Enable NTP synchronization across all servers	Centralized syslog forwarding collects logs from all servers, while NTP synchronization ensures accurate timestamps essential for compliance auditing and log correlation.	{"summary": "Centralized logging requirements:", "breakdown": ["Syslog forwarding: Collects logs from distributed servers", "NTP synchronization: Ensures accurate, correlated timestamps", "Compliance needs: Audit trails require precise timing", "Cross-region consistency: All servers must use synchronized time"], "otherOptions": "Agents add complexity without solving time sync\\nLocal rotation doesn't provide centralization \\nManaged services still need time synchronization"}	1	{"Configure centralized syslog forwarding","Enable NTP synchronization across all servers"}
102	102	239	Cloud Architecture - API Design	Comprehension	Cloud Architecture and Design	A mobile application needs to minimize bandwidth usage when retrieving user profile data from a cloud API. The app only needs specific fields like name, email, and avatar, but the current REST API returns all profile fields. Which API design pattern would BEST optimize data transfer?	[{"text": "Implement API caching with Redis", "isCorrect": false}, {"text": "Use GraphQL with field selection", "isCorrect": true}, {"text": "Compress API responses with gzip", "isCorrect": false}, {"text": "Implement API pagination", "isCorrect": false}]	Use GraphQL with field selection	GraphQL allows clients to request exactly the fields they need, reducing bandwidth by eliminating unnecessary data transfer compared to REST APIs that return fixed response structures.	{"summary": "GraphQL bandwidth optimization:", "breakdown": ["Field selection: Request only needed data fields", "Single request: Eliminate multiple API calls", "Bandwidth reduction: Transfer only necessary information", "Mobile optimization: Critical for limited data plans"], "otherOptions": "Caching improves response time but does not reduce initial data transfer\\nCompression helps but does not eliminate unnecessary fields\\nPagination limits data volume but does not select specific fields"}	\N	\N
103	103	240	Cloud Troubleshooting - VDI	Application	Troubleshooting	Users can successfully connect to their cloud-hosted virtual desktops but receive authentication failures when trying to access domain resources. Other users on the same VDI infrastructure can access domain resources normally. What is the MOST likely cause?	[{"text": "VDI licensing has expired", "isCorrect": false}, {"text": "Network connectivity issues to domain controller", "isCorrect": false}, {"text": "Computer account trust relationship broken", "isCorrect": true}, {"text": "User profile corruption", "isCorrect": false}]	Computer account trust relationship broken	When users can connect to VDI but cannot authenticate to domain resources, it typically indicates the computer account trust relationship with the domain controller has been compromised.	{"summary": "VDI domain authentication issues:", "breakdown": ["Trust relationship: Computer accounts must be trusted by domain", "Selective failure: Only affects specific virtual desktops", "Authentication chain: VDI connects  Domain auth fails", "Resolution: Rejoin affected machines to domain"], "otherOptions": "Licensing would prevent VDI connection entirely\\nNetwork issues would affect all users consistently\\nProfile corruption affects user settings, not domain authentication"}	\N	\N
104	104	241	Cloud Security - Data Governance	Analysis	Cloud Security	A financial services company must comply with multiple regulations requiring different data retention periods: PCI DSS (1 year), SOX (7 years), and GDPR (right to be forgotten). How should they implement their cloud data retention strategy?	[{"text": "Apply the longest retention period (7 years) to all data", "isCorrect": false}, {"text": "Implement data classification with automated lifecycle policies", "isCorrect": true}, {"text": "Store all data indefinitely to ensure compliance", "isCorrect": false}, {"text": "Apply the shortest retention period (1 year) to minimize risk", "isCorrect": false}]	Implement data classification with automated lifecycle policies	Data classification enables different retention periods for different data types, with automated policies ensuring compliance with multiple regulations while supporting GDPR deletion rights.	{"summary": "Multi-regulation data retention:", "breakdown": ["Data classification: Categorize by regulatory requirements", "Automated policies: Enforce retention rules consistently", "Compliance matrix: Different data types = different rules", "GDPR balance: Enable deletion while maintaining required records"], "otherOptions": "Over-retention violates GDPR right to be forgotten\\nIndefinite storage violates multiple privacy regulations\\nUnder-retention violates SOX and other compliance requirements"}	\N	\N
105	105	242	Cloud Operations - Disaster Recovery	Application	Cloud Operations and Support	Following a complete cloud region outage, an e-commerce platform needs to restore service as quickly as possible. The company has a warm standby environment in another region with data replicated every 15 minutes. What should be the FIRST action?	[{"text": "Verify the Recovery Point Objective (RPO)", "isCorrect": false}, {"text": "Update DNS records to point to backup region", "isCorrect": true}, {"text": "Restore from the most recent backup", "isCorrect": false}, {"text": "Contact the cloud provider for status updates", "isCorrect": false}]	Update DNS records to point to backup region	With a warm standby already running, the fastest way to restore service is updating DNS records to redirect traffic to the backup region, minimizing downtime.	{"summary": "Warm standby failover process:", "breakdown": ["Warm standby: System already running and updated", "DNS failover: Fastest method to redirect traffic", "Minimize RTO: Immediate service restoration", "15-minute RPO: Acceptable data loss window"], "otherOptions": "RPO verification comes after service restoration\\nWarm standby eliminates need for backup restoration\\nProvider contact does not restore immediate service"}	\N	\N
106	106	243	Cloud Security - Authentication	Comprehension	Cloud Security	A cloud-based trading application experiences intermittent authentication failures with time-sensitive security tokens. The failures occur randomly across different user sessions and geographic locations. What is the MOST likely root cause?	[{"text": "Insufficient server processing power", "isCorrect": false}, {"text": "Network latency between regions", "isCorrect": false}, {"text": "Inconsistent time synchronization across servers", "isCorrect": true}, {"text": "Database connection timeouts", "isCorrect": false}]	Inconsistent time synchronization across servers	Time-sensitive security tokens depend on synchronized clocks. When servers have different times, valid tokens may appear expired or not yet valid, causing random authentication failures.	{"summary": "Time-sensitive token requirements:", "breakdown": ["Token timestamps: Include issued-at and expiration times", "Clock synchronization: All servers must have accurate time", "Random failures: Indicates time skew between servers", "Geographic distribution: NTP essential across regions"], "otherOptions": "Processing power affects response time, not token validation\\nNetwork latency affects communication speed, not time validation\\nDatabase timeouts would show consistent patterns, not random failures"}	\N	\N
107	107	244	Cloud Security - API Management	Analysis	Cloud Security	A cloud API serves sensitive financial data and experiences varying loads throughout the trading day. Peak trading hours require 10x normal capacity, but current REST endpoints over-fetch data, causing bandwidth and latency issues. Which combination addresses BOTH performance and security concerns?	[{"text": "Implement rate limiting and API caching", "isCorrect": false}, {"text": "Deploy GraphQL with OAuth 2.0 token validation", "isCorrect": true}, {"text": "Use WebSockets with TLS encryption", "isCorrect": false}, {"text": "Implement gRPC with mutual TLS authentication", "isCorrect": false}]	Deploy GraphQL with OAuth 2.0 token validation	GraphQL reduces over-fetching by allowing precise data selection, while OAuth 2.0 provides secure token-based authentication suitable for financial APIs with varying access patterns.	{"summary": "Financial API optimization:", "breakdown": ["GraphQL: Eliminates over-fetching for better performance", "OAuth 2.0: Industry standard for secure API access", "Token validation: Suitable for high-frequency trading systems", "Bandwidth reduction: Critical during peak trading periods"], "otherOptions": "Does not solve over-fetching problem\\nWebSockets for real-time but does not address over-fetching\\ngRPC efficient but more complex than needed"}	\N	\N
108	108	245	Cloud Security - Compliance Automation	Expert	Cloud Security	A multinational corporation processes personal data under GDPR, PCI DSS, and HIPAA regulations across different cloud regions. They need automated compliance monitoring and data subject rights management. Which approach provides the MOST comprehensive solution?	[{"text": "Manual quarterly compliance audits with documentation", "isCorrect": false}, {"text": "Implement cloud security posture management (CSPM) with automated remediation", "isCorrect": true}, {"text": "Deploy separate compliance tools for each regulation", "isCorrect": false}, {"text": "Use cloud provider native compliance dashboards only", "isCorrect": false}]	Implement cloud security posture management (CSPM) with automated remediation	CSPM provides continuous compliance monitoring across multiple regulations and cloud environments, with automated remediation capabilities and centralized data subject rights management.	{"summary": "Multi-regulation compliance automation:", "breakdown": ["CSPM: Continuous monitoring across all cloud resources", "Automated remediation: Fixes compliance violations immediately", "Multi-regulation support: Handles GDPR, PCI DSS, HIPAA simultaneously", "Data subject rights: Automated GDPR deletion and reporting"], "otherOptions": "Manual audits do not provide continuous monitoring or automation\\nSeparate tools create management complexity and gaps\\nNative dashboards lack cross-regulation correlation and automation"}	\N	\N
109	109	246	Cloud Architecture - VDI Scaling	Application	Cloud Architecture and Design	A company's VDI environment supports 500 remote workers. During peak hours, users experience slow login times and desktop responsiveness issues. The VDI infrastructure shows adequate CPU and memory resources. What should be addressed FIRST?	[{"text": "Increase the number of VDI host servers", "isCorrect": false}, {"text": "Optimize storage IOPS and implement caching", "isCorrect": true}, {"text": "Upgrade network bandwidth to the data center", "isCorrect": false}, {"text": "Reduce the number of applications per desktop", "isCorrect": false}]	Optimize storage IOPS and implement caching	VDI performance issues with adequate CPU/memory typically indicate storage bottlenecks. Multiple users accessing virtual desktops simultaneously creates high IOPS demand that requires optimization.	{"summary": "VDI storage performance optimization:", "breakdown": ["IOPS bottleneck: Multiple desktops competing for storage", "Boot storms: Users logging in simultaneously", "Profile loading: User data accessed from shared storage", "Caching: Reduces storage load for common desktop images"], "otherOptions": "CPU/memory adequate, more servers won't help\\nNetwork issues would affect all operations, not just peak times\\nApplication reduction does not address underlying storage bottleneck"}	\N	\N
110	110	127	Cloud Concepts	Knowledge	Cloud Architecture and Design	Bob is accessing a self-service portal in the cloud to instantly create additional servers, storage, and database instances for his firms DevOps group. Which of the following options best describes this operation?	[{"text": "On-demand", "isCorrect": true}, {"text": "Bursting", "isCorrect": false}, {"text": "Pay-as-you-grow", "isCorrect": false}, {"text": "Multitenancy", "isCorrect": false}]	On-demand	On-demand self-service is a key characteristic of cloud computing, allowing users to provision resources automatically without requiring human interaction from the service provider.	{"summary": "This scenario describes on-demand self-service.", "breakdown": ["Users can provision resources as needed.", "The process is automated and instantaneous.", "It is one of the five essential characteristics of cloud computing defined by NIST."], "otherOptions": "Bursting refers to scaling from a private to a public cloud for peak demand.\\nPay-as-you-grow is a pricing model, not the act of provisioning.\\nMultitenancy is the architecture where a single software instance serves multiple customers."}	0	\N
111	111	128	Deployment Models	Comprehension	Cloud Architecture and Design	Jillian is working on a project to interconnect her companys private data center to a cloud company that offers email services and another that can provide burstable compute capacity. What type of cloud delivery model is she creating?	[{"text": "Public", "isCorrect": false}, {"text": "Hybrid", "isCorrect": true}, {"text": "Community", "isCorrect": false}, {"text": "Private", "isCorrect": false}]	Hybrid	A hybrid cloud model is composed of two or more distinct cloud infrastructures (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability.	{"summary": "This describes a Hybrid cloud model.", "breakdown": ["It combines a private data center (private cloud) with public cloud services.", "This model allows organizations to leverage public cloud benefits while keeping sensitive data on-premises.", "The key is the interconnection between the different environments."], "otherOptions": "A public cloud is entirely hosted by a third-party provider.\\nA community cloud is shared by several organizations with common concerns.\\nA private cloud is operated solely for a single organization."}	0	\N
112	112	129	Cloud Concepts	Knowledge	Cloud Architecture and Design	Carl is learning how cloud service providers allocate physical resources into a group. These resources are then dynamically associated with cloud services as demand requires. What best describes this?	[{"text": "On-demand virtualization", "isCorrect": false}, {"text": "Dynamic scaling", "isCorrect": false}, {"text": "Resource pooling", "isCorrect": true}, {"text": "Elasticity", "isCorrect": false}]	Resource pooling	Resource pooling is the concept where a cloud providers computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand.	{"summary": "This is the definition of resource pooling.", "breakdown": ["Providers serve multiple customers from a shared pool of physical hardware.", "Resources are dynamically assigned based on demand.", "This is what enables the efficiency and scale of public cloud services."], "otherOptions": "Dynamic scaling is a result of resource pooling, not the concept itself.\\nElasticity is the ability to scale resources up and down, which is enabled by resource pooling."}	0	\N
113	113	130	Service Models	Application	Cloud Architecture and Design	Liza is a new Cloud+ architect for BigCo Inc. She is investigating cloud services that provide server hardware, but not applications. What cloud service is she using?	[{"text": "IaaS", "isCorrect": true}, {"text": "PaaS", "isCorrect": false}, {"text": "SaaS", "isCorrect": false}, {"text": "CaaS", "isCorrect": false}]	IaaS	Infrastructure as a Service (IaaS) is the cloud service model that provides fundamental computing resources such as virtual servers, storage, and networking. The customer is responsible for the operating system and applications.	{"summary": "IaaS provides the foundational infrastructure.", "breakdown": ["The customer rents IT infrastructureservers and virtual machines (VMs), storage, networks, operating systems.", "It offers the most control over the hardware and OS.", "Examples include AWS EC2, Azure VMs, and Google Compute Engine."], "otherOptions": "PaaS provides a platform and abstracts the OS.\\nSaaS provides the entire application.\\nCaaS (Containers as a Service) is a subset of IaaS focused on containers."}	0	\N
114	114	131	Service Models	Application	Cloud Architecture and Design	Harold is investigating his options to migrate his companys time and attendance application to the cloud. He wants to be responsible only for maintaining the application and would prefer that the public cloud company manage all underlying infrastructure and servers. What would you suggest that he implement?	[{"text": "IaaS", "isCorrect": false}, {"text": "PaaS", "isCorrect": true}, {"text": "SaaS", "isCorrect": false}, {"text": "CaaS", "isCorrect": false}]	PaaS	Platform as a Service (PaaS) provides a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app.	{"summary": "PaaS is the best fit for this requirement.", "breakdown": ["The cloud provider manages the OS, middleware, and runtime.", "The customer is only responsible for their application and data.", "This model accelerates development and reduces operational overhead."], "otherOptions": "IaaS would require Harold to manage the OS and servers.\\nSaaS would involve using a pre-built application, not migrating his own.\\nCaaS focuses on container management, but PaaS is a broader and better fit."}	0	\N
115	115	132	Shared Responsibility	Comprehension	Security	Jane is a Cloud+ architect who is working to educate her staff on the shared responsibility security model. In an IaaS deployment, which of the following is Janes company responsible for securing?	[{"text": "The virtualization software", "isCorrect": false}, {"text": "The physical servers", "isCorrect": false}, {"text": "The guest operating system", "isCorrect": true}, {"text": "The storage arrays", "isCorrect": false}]	The guest operating system	In the IaaS shared responsibility model, the customer is responsible for everything from the guest operating system upwards. This includes patching the OS, configuring it securely, and managing all applications and data running on it.	{"summary": "In IaaS, the customer manages the Guest OS and above.", "breakdown": ["Cloud Provider Responsibility: Physical data center, network infrastructure, virtualization hypervisor.", "Customer Responsibility: Guest OS, middleware, runtime, applications, and data security."], "otherOptions": "A, B, The virtualization software, physical servers, and storage arrays are all part of the underlying infrastructure managed by the cloud provider."}	0	\N
116	116	133	Cloud Concepts	Comprehension	Cloud Architecture and Design	A cloud provider has three data centers in close proximity, all interconnected with low-latency, high-bandwidth links. They are designed so that a failure in one does not affect the others. What does this grouping of data centers represent?	[{"text": "A region", "isCorrect": false}, {"text": "An availability zone", "isCorrect": true}, {"text": "A storage array", "isCorrect": false}, {"text": "A server rack", "isCorrect": false}]	An availability zone	While the term can sometimes refer to a single data center, an Availability Zone (AZ) is a fault-tolerant construct typically made of one or more data centers with redundant power, networking, and cooling. They are designed to be isolated from failures in other AZs.	{"summary": "This describes an Availability Zone (AZ).", "breakdown": ["An AZ is a location with one or more data centers.", "They are designed for high availability and fault tolerance.", "Multiple AZs are grouped together to form a Region."], "otherOptions": "A region is a larger geographic area that contains multiple AZs.\\nC, Storage arrays and server racks are components within a data center."}	0	\N
117	117	134	Security	Comprehension	Security	You are migrating a sensitive application to the cloud and need to ensure that the virtual servers are not running on the same physical hardware as any other customer. Which tenancy model should you select?	[{"text": "Multitenant", "isCorrect": false}, {"text": "Dedicated host", "isCorrect": true}, {"text": "Containerized", "isCorrect": false}, {"text": "Serverless", "isCorrect": false}]	Dedicated host	A dedicated host provides a physical server that is fully dedicated for your use. This ensures complete isolation from other customers at the hardware level, which is often a requirement for compliance or licensing reasons.	{"summary": "Dedicated host tenancy provides physical isolation.", "breakdown": ["The customer gets an entire physical server.", "It helps meet compliance requirements for physical isolation.", "It can be more expensive than standard multi-tenant instances."], "otherOptions": "Multitenant is the standard model where you share hardware.\\nContainerization provides OS-level isolation but not hardware isolation.\\nServerless is an execution model and does not provide hardware isolation."}	0	\N
118	118	135	Networking	Knowledge	Cloud Architecture and Design	Your company has a hybrid cloud deployment and requires a consistent, high-bandwidth, low-latency private connection between your on-premises data center and the cloud provider. Which of the following services should be used?	[{"text": "Site-to-Site VPN", "isCorrect": false}, {"text": "Direct Connect / ExpressRoute", "isCorrect": true}, {"text": "Client VPN", "isCorrect": false}, {"text": "NAT Gateway", "isCorrect": false}]	Direct Connect / ExpressRoute	Direct Connect (AWS) and ExpressRoute (Azure) are dedicated private network connection services. They bypass the public internet to provide a more reliable, faster, and lower-latency connection between an on-premises environment and the cloud.	{"summary": "A dedicated, private connection is the best solution.", "breakdown": ["Provides a private, dedicated link, not over the public internet.", "Offers higher bandwidth and more consistent network performance than VPN.", "It is the preferred method for enterprise-grade hybrid cloud connectivity."], "otherOptions": "A site-to-site VPN runs over the public internet and has variable performance.\\nA client VPN is for individual users, not for connecting data centers.\\nA NAT Gateway is for outbound internet access from private subnets."}	0	\N
119	119	136	Cloud Concepts	Comprehension	Cloud Architecture and Design	What is the term for a cloud architecture that uses services from more than one cloud provider to leverage the best features of each?	[{"text": "Hybrid cloud", "isCorrect": false}, {"text": "Multicloud", "isCorrect": true}, {"text": "Community cloud", "isCorrect": false}, {"text": "Private cloud", "isCorrect": false}]	Multicloud	A multicloud strategy involves using two or more cloud computing services from different cloud providers. This can be done to avoid vendor lock-in, for cost savings, or to use the best-of-breed services from each provider.	{"summary": "Using multiple providers is known as multicloud.", "breakdown": ["It avoids dependency on a single vendor.", "Allows for leveraging unique services from different providers (e.g., Google for AI, AWS for serverless).", "Can improve resilience and availability."], "otherOptions": "Hybrid cloud specifically refers to a mix of on-premises and public cloud.\\nA community cloud is shared by organizations with a common goal.\\nA private cloud is a single-tenant environment."}	0	\N
120	120	137	Migration	Knowledge	Deployment	A company wants to move a legacy application to the cloud as quickly as possible with the fewest changes to the application itself. What is this migration strategy commonly called?	[{"text": "Re-architecting", "isCorrect": false}, {"text": "Replatforming", "isCorrect": false}, {"text": "Lift and shift", "isCorrect": true}, {"text": "Repurchasing", "isCorrect": false}]	Lift and shift	Lift and shift, also known as rehosting, is a migration strategy where you move an application from on-premises to the cloud with minimal or no changes. It is the fastest way to start taking advantage of cloud infrastructure.	{"summary": "This strategy is called Lift and Shift (Rehosting).", "breakdown": ["Involves moving the application with minimal modifications.", "It is the quickest migration path.", "The application may not be optimized for the cloud, but this can be done later in a phased approach."], "otherOptions": "Re-architecting involves significant changes to the application to make it cloud-native.\\nReplatforming involves minor changes to take advantage of cloud services like managed databases.\\nRepurchasing means switching to a different product, often a SaaS solution."}	0	\N
121	121	138	Service Models	Comprehension	Cloud Architecture and Design	Frank is looking for a cloud service that will allow him to deploy his custom application but does not want to manage the underlying operating system. Which of the following cloud service models would you recommend to him?	[{"text": "IaaS", "isCorrect": false}, {"text": "PaaS", "isCorrect": true}, {"text": "SaaS", "isCorrect": false}, {"text": "DaaS", "isCorrect": false}]	PaaS	Platform as a Service (PaaS) provides the platformincluding the operating system, middleware, and runtimefor developers to build and deploy applications without managing the underlying infrastructure.	{"summary": "PaaS is the ideal model for this scenario.", "breakdown": ["The provider manages the OS, patching, and server maintenance.", "The developer focuses only on the application code and data.", "This accelerates the development lifecycle."], "otherOptions": "IaaS would require Frank to manage the OS.\\nSaaS would mean using a pre-existing application, not deploying his own.\\nDaaS (Desktop as a Service) provides virtual desktops."}	0	\N
122	122	139	Storage	Knowledge	Cloud Architecture and Design	What type of cloud storage is best suited for storing and serving large, unstructured data such as videos, images, and backups?	[{"text": "Block storage", "isCorrect": false}, {"text": "File storage", "isCorrect": false}, {"text": "Object storage", "isCorrect": true}, {"text": "Ephemeral storage", "isCorrect": false}]	Object storage	Object storage is designed to store massive quantities of unstructured data. Data is stored as objects, each with its own unique identifier, metadata, and the data itself. It is highly scalable and durable.	{"summary": "Object storage is best for unstructured data.", "breakdown": ["Stores data in a flat structure, not a file hierarchy.", "Accessed via APIs (typically REST).", "Extremely scalable and cost-effective for large datasets.", "Common use cases include backups, archives, data lakes, and static website assets."], "otherOptions": "Block storage provides raw volumes for servers, like a hard drive.\\nFile storage provides a hierarchical file system (like NFS or SMB).\\nEphemeral storage is temporary and is lost when an instance stops."}	0	\N
123	123	140	Business Continuity	Comprehension	Operations and Support	A company has defined that in the event of a disaster, they can tolerate losing up to 4 hours of data. What does this metric define?	[{"text": "Recovery Time Objective (RTO)", "isCorrect": false}, {"text": "Recovery Point Objective (RPO)", "isCorrect": true}, {"text": "Mean Time Between Failures (MTBF)", "isCorrect": false}, {"text": "Service Level Agreement (SLA)", "isCorrect": false}]	Recovery Point Objective (RPO)	The Recovery Point Objective (RPO) is a disaster recovery metric that defines the maximum acceptable amount of data loss, measured in time. An RPO of 4 hours means backups must be performed at least every 4 hours.	{"summary": "This metric is the Recovery Point Objective (RPO).", "breakdown": ["RPO is about data loss tolerance.", "It dictates the minimum frequency of backups or replication.", "A lower RPO generally means a more expensive disaster recovery solution."], "otherOptions": "RTO is the target time to restore the service, i.e., the acceptable downtime.\\nMTBF is a measure of reliability, not a disaster recovery target.\\nSLA is a formal agreement on service uptime and performance."}	0	\N
124	124	141	Business Continuity	Comprehension	Operations and Support	What is the key difference between a hot site and a cold site for disaster recovery?	[{"text": "A hot site is fully operational and ready for immediate failover; a cold site has only the basic infrastructure and requires significant setup.", "isCorrect": true}, {"text": "A hot site is located in a warm climate; a cold site is in a cold climate.", "isCorrect": false}, {"text": "A hot site uses physical servers; a cold site uses virtual servers.", "isCorrect": false}, {"text": "A hot site is for short-term outages; a cold site is for long-term outages.", "isCorrect": false}]	A hot site is fully operational and ready for immediate failover; a cold site has only the basic infrastructure and requires significant setup.	A hot site is a fully redundant data center with real-time data synchronization, allowing for near-instantaneous failover. A cold site is just a space with power and cooling, requiring equipment and data to be brought in, leading to a long recovery time.	{"summary": "Hot sites are ready immediately; cold sites are not.", "breakdown": ["Hot Site: Fully equipped, data is replicated, allows for very low RTO.", "Warm Site: Has hardware but requires data restoration.", "Cold Site: Basically an empty data center, has the longest RTO."], "otherOptions": "The terms are unrelated to climate.\\nBoth can use either physical or virtual servers.\\nThe choice depends on the required RTO, not the duration of the outage."}	0	\N
125	125	142	Security	Application	Security	You are setting up a security group for a web server. Which of the following inbound rules is the most appropriate and secure configuration?	[{"text": "Allow ALL traffic from source 0.0.0.0/0", "isCorrect": false}, {"text": "Allow TCP port 22 (SSH) from source 0.0.0.0/0", "isCorrect": false}, {"text": "Allow TCP ports 80 (HTTP) and 443 (HTTPS) from source 0.0.0.0/0", "isCorrect": true}, {"text": "Allow TCP port 3389 (RDP) from source 0.0.0.0/0", "isCorrect": false}]	Allow TCP ports 80 (HTTP) and 443 (HTTPS) from source 0.0.0.0/0	A web server needs to accept incoming traffic from any IP address (0.0.0.0/0) on the standard web ports, which are TCP 80 for HTTP and TCP 443 for HTTPS. All other ports should be restricted.	{"summary": "A secure web server only exposes necessary web ports.", "breakdown": ["Port 80 is for HTTP traffic.", "Port 443 is for HTTPS (secure) traffic.", "The source 0.0.0.0/0 means 'any IP address on the internet'.", "This follows the principle of least privilege by only opening the ports required for its function."], "otherOptions": "Allowing all traffic is extremely insecure.\\nB, Opening management ports like SSH or RDP to the entire internet is a major security risk and should be restricted to specific admin IPs."}	0	\N
126	126	143	Cloud Concepts	Comprehension	Cloud Architecture and Design	What does the term "elasticity" refer to in the context of cloud computing?	[{"text": "The ability of a system to remain operational despite component failures.", "isCorrect": false}, {"text": "The ability to automatically scale computing resources up and down to match demand.", "isCorrect": true}, {"text": "The ability to access services from anywhere over the network.", "isCorrect": false}, {"text": "The pooling of provider resources to serve multiple customers.", "isCorrect": false}]	The ability to automatically scale computing resources up and down to match demand.	Elasticity is the ability of the cloud to automatically and dynamically add or remove resources (like VMs or containers) to meet the current workload demand. This is a key benefit that prevents over-provisioning and reduces cost.	{"summary": "Elasticity is the automatic scaling of resources.", "breakdown": ["Scaling up (or out) to handle increases in load.", "Scaling down (or in) to save money when demand decreases.", "This process is typically automated based on metrics like CPU utilization or request count."], "otherOptions": "This describes fault tolerance or high availability.\\nThis describes broad network access.\\nThis describes resource pooling."}	0	\N
127	127	144	Networking	Knowledge	Troubleshooting	A network administrator is troubleshooting an issue where a user cannot resolve a website's domain name, such as www.example.com, to its IP address. Which service is most likely experiencing a problem?	[{"text": "DHCP", "isCorrect": false}, {"text": "DNS", "isCorrect": true}, {"text": "NAT", "isCorrect": false}, {"text": "BGP", "isCorrect": false}]	DNS	The Domain Name System (DNS) is responsible for translating human-readable domain names into the IP addresses that computers use to connect to each other. If this translation fails, the user cannot connect to the website.	{"summary": "DNS handles domain name to IP address translation.", "breakdown": ["DNS acts like the phonebook of the internet.", "When you type a domain name, your computer queries a DNS server to get the corresponding IP address.", "Failure in this process is a common cause of connectivity issues."], "otherOptions": "DHCP assigns IP addresses to devices on a local network.\\nNAT translates private IP addresses to public ones for internet access.\\nBGP is a routing protocol used by internet service providers."}	0	\N
128	128	145	DevOps	Knowledge	Deployment	What is the primary goal of Continuous Integration (CI) in a DevOps workflow?	[{"text": "To automatically deploy every change directly to production.", "isCorrect": false}, {"text": "To frequently merge developer code changes into a central repository and run automated builds and tests.", "isCorrect": true}, {"text": "To manage and provision infrastructure using code.", "isCorrect": false}, {"text": "To monitor the application and infrastructure for performance issues.", "isCorrect": false}]	To frequently merge developer code changes into a central repository and run automated builds and tests.	Continuous Integration (CI) is a DevOps practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. The key goals are to find and address bugs quicker, improve software quality, and reduce the time it takes to validate and release new software updates.	{"summary": "CI is about frequent integration and automated testing.", "breakdown": ["Developers commit code to a shared repository multiple times a day.", "An automated system builds the application and runs a suite of tests.", "This provides rapid feedback, allowing teams to find and fix bugs early."], "otherOptions": "This describes Continuous Deployment, which is a subsequent step.\\nThis is Infrastructure as Code (IaC).\\nThis is Continuous Monitoring."}	0	\N
129	129	146	Deployment	Application	Deployment	A company wants to deploy a new version of their web application with zero downtime. The strategy involves setting up a completely new, identical environment with the new version, testing it, and then switching all user traffic from the old environment to the new one instantly. What is this deployment strategy called?	[{"text": "Canary deployment", "isCorrect": false}, {"text": "Rolling deployment", "isCorrect": false}, {"text": "Blue-green deployment", "isCorrect": true}, {"text": "In-place deployment", "isCorrect": false}]	Blue-green deployment	In a blue-green deployment, you create two separate, but identical, environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. You can then switch traffic instantly from blue to green. This provides zero downtime and a rapid way to roll back if issues are found.	{"summary": "This is a blue-green deployment strategy.", "breakdown": ["Two identical production environments are maintained.", "Traffic is routed to only one environment at a time.", "This allows for safe testing of the new version before release and provides instant rollback capability."], "otherOptions": "A canary deployment releases the new version to a small subset of users first.\\nA rolling deployment gradually replaces old instances with new ones.\\nAn in-place deployment updates the code on the existing instances, causing downtime."}	0	\N
131	131	148	Operations	Knowledge	Operations and Support	What is the term for a predefined, documented set of procedures to be followed in response to a specific event or incident?	[{"text": "A service level agreement (SLA)", "isCorrect": false}, {"text": "A baseline", "isCorrect": false}, {"text": "A runbook", "isCorrect": true}, {"text": "A metric", "isCorrect": false}]	A runbook	A runbook is a compilation of routine procedures and operations that a system administrator or operator carries out. They are designed to be followed step-by-step to ensure consistency and speed when responding to a known scenario or alert.	{"summary": "This document is known as a runbook.", "breakdown": ["It contains step-by-step instructions for routine or emergency tasks.", "It is a key part of operational readiness and incident response.", "Runbooks can be manual (documents) or automated (scripts)."], "otherOptions": "An SLA is a contract about service performance.\\nA baseline is a measurement of normal performance.\\nA metric is a specific measurement (e.g., CPU %)."}	0	\N
132	132	149	Containers	Comprehension	Cloud Architecture and Design	What is a primary benefit of using containers over traditional virtual machines?	[{"text": "Containers provide better hardware-level isolation.", "isCorrect": false}, {"text": "Containers are more lightweight and have faster startup times because they share the host OS kernel.", "isCorrect": true}, {"text": "Each container runs a full copy of the guest operating system.", "isCorrect": false}, {"text": "Containers are more difficult to manage and orchestrate.", "isCorrect": false}]	Containers are more lightweight and have faster startup times because they share the host OS kernel.	Containers virtualize the operating system, allowing multiple applications to run in isolated user spaces while sharing the same OS kernel. This makes them much more lightweight and faster to start than VMs, which must each boot a full guest OS.	{"summary": "Containers are lightweight due to sharing the host OS kernel.", "breakdown": ["VMs virtualize the hardware; containers virtualize the OS.", "This results in smaller image sizes and faster startup times (seconds vs. minutes).", "Higher density allows more containers than VMs to run on the same host."], "otherOptions": "VMs provide stronger, hardware-level isolation. Containers provide OS-level isolation.\\nThis describes a virtual machine, not a container.\\nWhile orchestration adds complexity, individual containers are generally easier to manage."}	0	\N
133	133	150	Security	Knowledge	Security	What is the purpose of a Web Application Firewall (WAF)?	[{"text": "To filter traffic at the network layer based on IP addresses and ports.", "isCorrect": false}, {"text": "To protect against application-layer attacks such as SQL injection and cross-site scripting (XSS).", "isCorrect": true}, {"text": "To provide secure remote access for employees to the corporate network.", "isCorrect": false}, {"text": "To scan virtual machine images for known vulnerabilities.", "isCorrect": false}]	To protect against application-layer attacks such as SQL injection and cross-site scripting (XSS).	A Web Application Firewall (WAoperates at Layer 7 (the application layer) to inspect HTTP traffic and protect web applications from common exploits that network firewalls cannot detect.	{"summary": "A WAF protects against Layer 7 application attacks.", "breakdown": ["It sits in front of web servers to filter malicious traffic.", "It can detect and block common attacks like SQL injection, cross-site scripting, and file inclusion.", "It helps organizations comply with regulations like PCI DSS."], "otherOptions": "This describes a network firewall or Network ACL.\\nThis describes a VPN.\\nThis describes a vulnerability scanner."}	0	\N
134	134	151	Operations	Application	Operations and Support	An administrator is trying to determine the normal performance of an application over a one-week period to set effective alerting thresholds. What is the administrator establishing?	[{"text": "A disaster recovery plan", "isCorrect": false}, {"text": "A performance baseline", "isCorrect": true}, {"text": "A security audit", "isCorrect": false}, {"text": "A capacity plan", "isCorrect": false}]	A performance baseline	A performance baseline is a standardized level of performance for a given system. It is established by collecting metrics (like CPU, memory, and latency) over a period of normal operation. This baseline is then used to identify deviations that may indicate a problem.	{"summary": "The administrator is establishing a performance baseline.", "breakdown": ["It defines what 'normal' looks like for the system.", "It is essential for effective monitoring and alerting.", "Alerts are configured to trigger when metrics deviate significantly from the established baseline."], "otherOptions": "A DR plan is for responding to disasters.\\nA security audit assesses security controls.\\nA capacity plan forecasts future resource needs."}	0	\N
135	135	152	Networking	Comprehension	Cloud Architecture and Design	What is the purpose of a load balancer in a cloud architecture?	[{"text": "To provide a private, dedicated connection to the cloud.", "isCorrect": false}, {"text": "To distribute incoming network traffic across multiple backend servers.", "isCorrect": true}, {"text": "To translate domain names into IP addresses.", "isCorrect": false}, {"text": "To cache content closer to end-users for faster delivery.", "isCorrect": false}]	To distribute incoming network traffic across multiple backend servers.	A load balancer acts as a reverse proxy and distributes network or application traffic across a number of servers. Load balancers are used to increase capacity (concurrent users) and reliability of applications.	{"summary": "Load balancers distribute traffic for scalability and availability.", "breakdown": ["Improves application scalability by spreading requests.", "Increases availability by routing traffic away from failed or unhealthy servers.", "Can perform health checks to monitor the status of backend servers."], "otherOptions": "This describes a Direct Connect or ExpressRoute.\\nThis describes DNS.\\nThis describes a Content Delivery Network (CDN)."}	0	\N
136	136	153	Storage	Knowledge	Cloud Architecture and Design	Which of the following is a characteristic of block storage?	[{"text": "It is accessed via API calls using HTTP methods.", "isCorrect": false}, {"text": "It stores data in a hierarchical structure of files and folders.", "isCorrect": false}, {"text": "It presents a raw storage volume to an operating system, which formats it with a file system.", "isCorrect": true}, {"text": "It is primarily used for long-term archival of unstructured data.", "isCorrect": false}]	It presents a raw storage volume to an operating system, which formats it with a file system.	Block storage provides raw storage volumes (blocks) that are attached to a server. The server's operating system can partition, format, and mount the volume just like a local physical hard drive (e.g., an SSD or HDD).	{"summary": "Block storage acts like a virtual hard drive for a server.", "breakdown": ["The cloud provider manages the physical hardware.", "The customer manages the volume and the file system on it (e.g., NTFS, ext4).", "It is used for performance-sensitive workloads like databases and virtual machine disks."], "otherOptions": "This describes object storage.\\nThis describes file storage.\\nThis is a use case for object storage (archive tier)."}	0	\N
137	137	154	Security	Comprehension	Security	Which security principle involves giving a user account or service only the permissions essential to perform its intended function?	[{"text": "Defense in depth", "isCorrect": false}, {"text": "Principle of least privilege", "isCorrect": true}, {"text": "Security through obscurity", "isCorrect": false}, {"text": "Separation of duties", "isCorrect": false}]	Principle of least privilege	The principle of least privilege requires that in a particular abstraction layer of a computing environment, every module (such as a process, a user, or a program) must be able to access only the information and resources that are necessary for its legitimate purpose.	{"summary": "This is the principle of least privilege.", "breakdown": ["It minimizes the potential damage from a security breach.", "If an account is compromised, the attacker only gains access to a limited set of resources.", "It is a fundamental concept in information security."], "otherOptions": "Defense in depth is about layering multiple security controls.\\nSecurity through obscurity is the ineffective practice of trying to hide vulnerabilities.\\nSeparation of duties involves splitting a critical task between multiple people."}	0	\N
138	138	155	Deployment	Knowledge	Deployment	A developer has written a script using a tool like Ansible to configure 100 web servers to have the exact same state (installed software, file permissions, etc.). What is this practice called?	[{"text": "Infrastructure as Code", "isCorrect": false}, {"text": "Configuration Management", "isCorrect": true}, {"text": "Continuous Integration", "isCorrect": false}, {"text": "Container Orchestration", "isCorrect": false}]	Configuration Management	Configuration management is the process of maintaining computer systems, servers, and software in a desired, consistent state. Tools like Ansible, Puppet, and Chef are used to automate the process of configuring and maintaining the state of servers.	{"summary": "This practice is known as configuration management.", "breakdown": ["It ensures consistency across multiple servers.", "It automates the setup and maintenance of server state.", "It helps prevent configuration drift, where servers become inconsistent over time."], "otherOptions": "Infrastructure as Code is broader and includes provisioning the servers themselves, not just configuring them.\\nContinuous Integration is about integrating and testing code.\\nContainer Orchestration is about managing the lifecycle of containers."}	0	\N
139	139	332	Deployment - Serverless	Knowledge	Cloud Deployment	A development team wants to focus all its efforts on creating and maintaining code. The team does not have\nthe resources to provision and scale the infrastructure their applications require to run. Which solution\nshould the development team use?	[{"text": "Containerize the app and deploy a container cluster service.", "isCorrect": false}, {"text": "Deploy a serverless compute subscription and upload the code", "isCorrect": true}, {"text": "Configure instance VMs and deploy app updates using a playbook", "isCorrect": false}, {"text": "Provision systems using templates and configure auto-scaling", "isCorrect": false}]	Deploy a serverless compute subscription and upload the code	The development team should deploy a serverless compute subscription and upload the code. \n\nIn serverless computing, the customer simply submits their application code, and the cloud service provider (CSP)\nprovisions and maintains the servers and infrastructure required to run an application. This includes code\nbackups, high availabilty features, and auto-scaling to meet increased workloads. \n\nContainers typically contain only the binaries and libraries to run a single app or service. However, a container is an infrastructure component, and the containers must be created, deployed, and periodically updated	{"summary": "The development team should deploy a serverless compute subscription and upload the code.", "breakdown": ["In serverless computing, the customer simply submits their application code, and the cloud service provider (CSP) provisions and maintains the servers and infrastructure required to run an application. This includes code backups, high availabilty features, and auto-scaling to meet increased workloads. ", "Containers typically contain only the binaries and libraries to run a single app or service. However, a container is an infrastructure component, and the containers must be created, deployed, and periodically updated"], "otherOptions": "VM templates can be compared to a clone. Many organizations use templates to speed the deployment of\\nfrequently used operating system configurations. Like containers, VMs are an infrastructure component\\nAn Ansible playbook is a great method for deploying automated configuration changes. However, in this\\nscenario the underlying VM would still need to be created and deployed."}	\N	\N
140	140	156	Operations	Comprehension	Troubleshooting	A cloud-hosted application has become completely unresponsive. The monitoring system shows that the virtual machine it runs on has a CPU utilization of 100%. What is the MOST likely cause of the issue?	[{"text": "A network misconfiguration", "isCorrect": false}, {"text": "A storage I/O bottleneck", "isCorrect": false}, {"text": "A runaway process or infinite loop in the application", "isCorrect": true}, {"text": "Insufficient memory (RAM)", "isCorrect": false}]	A runaway process or infinite loop in the application	When a system is unresponsive and the CPU is at 100%, it typically indicates that one or more processes are consuming all available processing power. This is often caused by a software bug, such as an infinite loop, or a process that has become stuck in a resource-intensive state.	{"summary": "100% CPU utilization points to a process-level problem.", "breakdown": ["This is a classic symptom of a software fault.", "The application is likely stuck in a loop or a high-intensity computation.", "Troubleshooting would involve identifying and terminating or debugging the offending process."], "otherOptions": "Network issues would not cause 100% CPU.\\nA storage bottleneck would show high disk I/O, not high CPU.\\nInsufficient memory would typically cause high memory usage and swapping, but not necessarily 100% CPU."}	0	\N
141	141	157	Cloud Concepts	Comprehension	Cloud Architecture and Design	Which two of the following are considered Capital Expenditure (CapEx)? (Choose TWO)	[{"text": "Paying a monthly bill for a cloud-based virtual machine.", "isCorrect": false}, {"text": "Purchasing physical servers for an on-premises data center.", "isCorrect": true}, {"text": "Paying for electricity and cooling for a data center.", "isCorrect": false}, {"text": "Buying a 5-year license for a piece of software.", "isCorrect": true}, {"text": "Paying for data transfer out of a public cloud.", "isCorrect": false}]	B, D	Capital Expenditure (CapEx) involves acquiring assets whose benefits extend beyond the current year. This includes buying physical hardware like servers and long-term software licenses. Cloud computing primarily shifts these costs to Operational Expenditure (OpEx).	{"summary": "CapEx is the upfront spending on physical or fixed assets.", "breakdown": ["Purchasing servers is a classic example of CapEx.", "A multi-year software license is also treated as a capital asset.", "Cloud computing helps companies reduce their CapEx in favor of OpEx."], "otherOptions": "A, C, Monthly cloud bills, utility costs, and data transfer fees are all examples of ongoing Operational Expenditure (OpEx)."}	1	{B,D}
142	142	158	Security	Application	Security	A company needs to provide its employees with access to a set of cloud-based virtual desktops. The solution must be centrally managed and accessible from any location. Which cloud service model should they use?	[{"text": "IaaS", "isCorrect": false}, {"text": "PaaS", "isCorrect": false}, {"text": "SaaS", "isCorrect": false}, {"text": "DaaS", "isCorrect": true}]	DaaS	Desktop as a Service (DaaS) is a cloud computing offering where a service provider delivers virtual desktops to end users over the internet, licensed with a per-user subscription. The provider takes care of the backend management for this VDI (Virtual Desktop Infrastructure) deployment.	{"summary": "Desktop as a Service (DaaS) provides virtual desktops.", "breakdown": ["It provides a full virtualized desktop experience to users.", "The infrastructure is managed by the cloud provider.", "It is a form of SaaS, specifically for delivering desktops."], "otherOptions": "A, IaaS and PaaS are for building and deploying applications, not for providing end-user desktops.\\nWhile DaaS is a type of SaaS, DaaS is the more specific and correct term for this use case."}	0	\N
143	143	159	Business Continuity	Knowledge	Operations and Support	What is the purpose of performing regular disaster recovery tests?	[{"text": "To satisfy the curiosity of the management team.", "isCorrect": false}, {"text": "To validate the effectiveness of the DR plan and identify any gaps or issues.", "isCorrect": true}, {"text": "To intentionally cause downtime to see how customers react.", "isCorrect": false}, {"text": "To reduce the overall cost of the disaster recovery solution.", "isCorrect": false}]	To validate the effectiveness of the DR plan and identify any gaps or issues.	Disaster recovery testing is a critical process that simulates a disaster scenario to ensure that the recovery plan is effective, the technical solutions work as expected, and the teams involved are prepared. It helps uncover issues that would otherwise only be found during a real disaster.	{"summary": "DR testing validates the recovery plan and team readiness.", "breakdown": ["It verifies that RPO and RTO targets can be met.", "It identifies technical problems with failover scripts or replication.", "It ensures that the operations staff are familiar with the procedures.", "It is a key requirement for many compliance frameworks."], "otherOptions": "A, The purpose is technical validation, not satisfying curiosity or testing customer reaction.\\nTesting often adds to the cost of DR but is essential for ensuring it works."}	0	\N
144	144	160	Networking	Comprehension	Troubleshooting	A user is able to connect to a web server using its IP address (e.g., 52.95.122.208) but not by using its domain name (e.g., www.example.com). What is the MOST likely problem?	[{"text": "The user's computer does not have an IP address.", "isCorrect": false}, {"text": "There is a problem with DNS resolution.", "isCorrect": true}, {"text": "The web server is down.", "isCorrect": false}, {"text": "A network firewall is blocking HTTP traffic.", "isCorrect": false}]	There is a problem with DNS resolution.	Since the user can connect via the IP address, we know that the server is running and network connectivity exists. The failure to connect via the domain name points directly to an issue with the Domain Name System (DNS), which is responsible for translating the name to the IP address.	{"summary": "This is a classic DNS resolution issue.", "breakdown": ["The ability to connect via IP proves the server is online and reachable.", "The failure of the domain name to work means the translation step is failing.", "The issue could be with the user's local DNS resolver, a corporate DNS server, or the public DNS records for the domain."], "otherOptions": "The user must have an IP address to connect to anything.\\nThe server cannot be down if a connection via IP is successful.\\nA firewall block would prevent connection to the IP address as well."}	0	\N
145	145	161	Cloud Concepts	Application	Operations and Support	A company is expecting a massive, temporary surge in traffic for a 24-hour marketing event. They need to rapidly provision a large number of servers to handle the load and then remove them immediately after the event. Which cloud characteristic is MOST critical to meet this requirement?	[{"text": "Measured service", "isCorrect": false}, {"text": "Broad network access", "isCorrect": false}, {"text": "Rapid elasticity", "isCorrect": true}, {"text": "Resource pooling", "isCorrect": false}]	Rapid elasticity	Rapid elasticity allows for the quick and automatic scaling of resources to meet demand. This is essential for handling sudden, large traffic spikes for events like this, and then scaling back down to save costs.	{"summary": "Rapid elasticity is key for handling traffic surges.", "breakdown": ["It allows for provisioning and de-provisioning resources in minutes.", "It enables companies to handle peak loads without permanently owning the required hardware.", "It is a core value proposition of cloud computing for variable workloads."], "otherOptions": "Measured service is the pay-as-you-go billing model.\\nBroad network access allows users to connect from anywhere.\\nResource pooling is the underlying mechanism that enables elasticity."}	0	\N
146	146	162	Security	Knowledge	Security	Which of the following cloud security tools is specifically designed to detect and alert on anomalous or malicious activity in your cloud account, such as an instance communicating with a known cryptocurrency mining pool?	[{"text": "A vulnerability scanner", "isCorrect": false}, {"text": "A threat detection service (e.g., AWS GuardDuty)", "isCorrect": true}, {"text": "A configuration management database (CMDB)", "isCorrect": false}, {"text": "A secrets management tool", "isCorrect": false}]	A threat detection service (e.g., AWS GuardDuty)	Cloud threat detection services use machine learning, anomaly detection, and integrated threat intelligence to continuously monitor for malicious activity and unauthorized behavior within a cloud environment.	{"summary": "This describes a cloud threat detection service.", "breakdown": ["It analyzes data sources like VPC Flow Logs, DNS logs, and CloudTrail events.", "It can detect threats like reconnaissance, instance compromise, and account compromise.", "It provides intelligent, actionable alerts on potential security issues."], "otherOptions": "A vulnerability scanner looks for known software vulnerabilities, it does not monitor live traffic.\\nA CMDB is a database of configuration items.\\nA secrets management tool stores credentials securely."}	0	\N
147	147	163	Deployment	Application	Deployment	You are deploying a new version of an application and want to ensure that it meets all security and compliance requirements before it goes live. Which of the following should be integrated into your CI/CD pipeline? (Choose TWO)	[{"text": "Static Application Security Testing (SAST)", "isCorrect": true}, {"text": "Manual deployment to production by the lead developer.", "isCorrect": false}, {"text": "Dynamic Application Security Testing (DAST)", "isCorrect": true}, {"text": "A step that emails the security team for approval before every test run.", "isCorrect": false}, {"text": "Disabling all automated tests to speed up the pipeline.", "isCorrect": false}]	A, C	Integrating security testing directly into the CI/CD pipeline is a key practice of DevSecOps. SAST analyzes the source code for vulnerabilities before the application is compiled, while DAST tests the running application for vulnerabilities, often in a staging environment.	{"summary": "Integrate both SAST and DAST into the CI/CD pipeline.", "breakdown": ["SAST (Static Testing): Analyzes code without executing it. Finds issues like SQL injection flaws or improper input validation early.", "DAST (Dynamic Testing): Tests the application while it is running. Finds runtime vulnerabilities and configuration errors.", "Automating these scans ensures security is a continuous part of the development process."], "otherOptions": "Manual deployments are error-prone and slow down the pipeline.\\nManual approvals should be for critical gates (like production release), not for every test run.\\nDisabling tests is the opposite of ensuring quality."}	1	{A,C}
149	149	169	Operations	Knowledge	Operations and Support	An organization needs to track all API calls made within their cloud account for security analysis and compliance auditing. Which service should they enable?	[{"text": "A performance monitoring tool (APM)", "isCorrect": false}, {"text": "A logging and auditing service (e.g., AWS CloudTrail)", "isCorrect": true}, {"text": "A billing dashboard", "isCorrect": false}, {"text": "A service catalog", "isCorrect": false}]	A logging and auditing service (e.g., AWS CloudTrail)	Services like AWS CloudTrail or Azure Monitor Audit Logs are specifically designed to record every API call made in an account. This provides a detailed audit trail of who did what, from where, and when, which is essential for security investigations and compliance.	{"summary": "Cloud logging and auditing services track all API activity.", "breakdown": ["Records API calls, including the user, source IP, time, and parameters.", "Provides an event history for security analysis and troubleshooting.", "Is a critical component for meeting compliance standards like PCI DSS, HIPAA, and SOC."], "otherOptions": "An APM tool monitors application performance, not account-level API calls.\\nA billing dashboard shows cost data, not API activity.\\nA service catalog is for managing approved services for deployment."}	0	\N
150	150	165	Troubleshooting	Comprehension	Troubleshooting	A user reports they are unable to access a newly deployed application in the cloud. You have verified the application is running correctly on the virtual machine. Which TWO of the following are the most likely causes of the connectivity issue? (Choose TWO)	[{"text": "The virtual machine's CPU is too small.", "isCorrect": false}, {"text": "The security group or firewall is blocking the user's traffic.", "isCorrect": true}, {"text": "The application is not compatible with the user's web browser.", "isCorrect": false}, {"text": "There is no route from the internet to the application's subnet.", "isCorrect": true}, {"text": "The user has forgotten their password for the application.", "isCorrect": false}]	B, D	When an application is confirmed to be running but is inaccessible from the outside, the problem is almost always related to networking. The two most common issues are firewalls (security groups, NACLs) blocking the traffic, or a missing route (e.g., no Internet Gateway or incorrect route table entry) that prevents traffic from reaching the subnet where the application resides.	{"summary": "Inaccessibility of a running app is usually a network issue.", "breakdown": ["Security Groups / Firewalls: These are stateful firewalls that must explicitly allow inbound traffic on the application's port.", "Routing: The VPC's route table must have a path from the source (e.g., an Internet Gateway) to the destination subnet."], "otherOptions": "A small CPU would cause performance issues, not a complete lack of access.\\nBrowser compatibility would result in a rendering error, not a connection failure.\\nA forgotten password would occur after connecting to the application's login page."}	1	{B,D}
151	151	166	Automation	Comprehension	Deployment	Which of the following best describes the difference between automation and orchestration?	[{"text": "Automation is for servers, while orchestration is for networks.", "isCorrect": false}, {"text": "Automation refers to a single task being performed without human intervention, while orchestration is the coordination of multiple automated tasks into a complete workflow.", "isCorrect": true}, {"text": "Automation requires scripting, while orchestration uses graphical user interfaces.", "isCorrect": false}, {"text": "Automation is a term for on-premises, while orchestration is used for the cloud.", "isCorrect": false}]	Automation refers to a single task being performed without human intervention, while orchestration is the coordination of multiple automated tasks into a complete workflow.	Automation focuses on making individual tasks repeatable and efficient. Orchestration takes a higher-level view, arranging and managing a sequence of automated tasks to deliver a service or complete a process, such as deploying a multi-tier application.	{"summary": "Orchestration is the automation of automation.", "breakdown": ["Automation Example: A script that installs a web server on a VM.", "Orchestration Example: A workflow that provisions a VPC, launches a database server, launches multiple web servers, and configures a load balancer to point to them."], "otherOptions": "A, C, These are false distinctions. Both can apply to any resource, use various tools, and be used in any environment."}	0	\N
152	152	167	Storage	Application	Cloud Architecture and Design	A company is designing a system to store medical images. The images must be stored with the highest level of durability, be replicated across multiple data centers automatically, and be accessible via a web-based API. Which storage type is the best fit?	[{"text": "Block storage attached to a virtual machine.", "isCorrect": false}, {"text": "File storage mounted on multiple servers.", "isCorrect": false}, {"text": "Object storage.", "isCorrect": true}, {"text": "A relational database.", "isCorrect": false}]	Object storage.	Object storage is designed for high durability and massive scalability. It inherently replicates data across multiple facilities (availability zones) and is accessed via APIs, making it perfect for storing large, unstructured data like medical images for web applications.	{"summary": "Object storage meets all the requirements.", "breakdown": ["High Durability: Cloud object storage typically offers 11 nines (99.999999999%) of durability.", "Automatic Replication: Data is automatically replicated across multiple AZs within a region.", "API Access: It is designed to be accessed programmatically via REST APIs."], "otherOptions": "Block storage is not inherently replicated across data centers and is not accessed via web APIs.\\nFile storage can be complex to scale and manage for petabytes of data.\\nRelational databases are not suitable for storing large binary files like images."}	0	\N
153	153	168	Security	Comprehension	Security	What is the purpose of using federated identity management (e.g., with SAML or OIDfor accessing cloud services?	[{"text": "To create and manage user accounts directly within each cloud service.", "isCorrect": false}, {"text": "To allow users to authenticate with their existing corporate credentials without creating new accounts in the cloud service.", "isCorrect": true}, {"text": "To encrypt all data traffic between the user and the cloud service.", "isCorrect": false}, {"text": "To enforce a single, strong password policy across all services.", "isCorrect": false}]	To allow users to authenticate with their existing corporate credentials without creating new accounts in the cloud service.	Federation establishes trust between an organization's identity provider (IdP) and the cloud service (SP). This allows users to sign in once to their corporate network and gain access to multiple trusted applications without needing to manage a separate set of credentials for each one.	{"summary": "Federation enables Single Sign-On (SSO) with existing credentials.", "breakdown": ["Users authenticate with their primary IdP (e.g., Active Directory).", "The IdP sends a secure assertion to the cloud service, verifying the user's identity.", "This improves user experience and centralizes access control."], "otherOptions": "Federation avoids the need to create separate user accounts.\\nWhile the connection is encrypted, that is not the primary purpose of federation.\\nPassword policy is enforced by the corporate IdP, not the federation itself."}	0	\N
154	154	170	Networking	Application	Troubleshooting	A web server is deployed in a public subnet, and a database server is in a private subnet. The web server cannot connect to the database. You have verified that the database is running. Which two settings should you investigate first? (Choose TWO)	[{"text": "The security group attached to the web server.", "isCorrect": false}, {"text": "The security group attached to the database server.", "isCorrect": true}, {"text": "The route table for the public subnet.", "isCorrect": false}, {"text": "The Network ACL (NACL) for the private subnet.", "isCorrect": true}, {"text": "The Internet Gateway (IGW) configuration.", "isCorrect": false}]	B, D	Connectivity issues between subnets are almost always caused by network filtering rules. The two primary filters are security groups (stateful firewalls attached to instances) and NACLs (stateless firewalls attached to subnets). You must check the DB security group to ensure it allows inbound traffic from the web server, and the private subnet's NACL to ensure it allows both inbound and outbound traffic for the database connection.	{"summary": "Check security groups and NACLs for inter-subnet connectivity.", "breakdown": ["The database security group must have an inbound rule allowing traffic from the web server's security group on the database port.", "The NACL on the private subnet must allow inbound traffic on the database port and outbound traffic on the ephemeral ports for the return connection."], "otherOptions": "The web server is initiating the connection, so its outbound rules are less likely to be the issue (they are typically permissive).\\nThe route table for the public subnet controls outbound internet traffic, not inter-subnet traffic.\\nThe IGW is for internet connectivity, not communication between subnets in the same VPC."}	1	{B,D}
155	155	171	High Availability	Comprehension	Cloud Architecture and Design	What is the primary benefit of designing an application to run across multiple availability zones?	[{"text": "It protects against the failure of an entire geographic region.", "isCorrect": false}, {"text": "It improves application performance by caching data closer to users.", "isCorrect": false}, {"text": "It provides high availability by protecting the application from the failure of a single data center.", "isCorrect": true}, {"text": "It reduces data transfer costs between services.", "isCorrect": false}]	It provides high availability by protecting the application from the failure of a single data center.	Availability Zones (AZs) are physically separate data centers within a region. By deploying an application across multiple AZs, it can continue to operate even if one of those data centers fails due to a power outage, flood, or other disaster. This is a fundamental pattern for building highly available systems in the cloud.	{"summary": "Multi-AZ architectures provide data center-level fault tolerance.", "breakdown": ["If one AZ fails, traffic is automatically routed to the healthy AZs.", "This allows the application to remain online without interruption.", "It is a standard best practice for all production workloads in the cloud."], "otherOptions": "To protect against regional failure, you need a multi-region architecture.\\nThis describes a Content Delivery Network (CDN).\\nData transfer between AZs typically incurs costs."}	0	\N
156	156	177	Business Continuity	Comprehension	Operations and Support	During a disaster recovery test, the team was able to recover the application in 2 hours, but they discovered that the recovered database was 12 hours old. Which of the following has occurred?	[{"text": "The RTO was met, but the RPO was not.", "isCorrect": true}, {"text": "The RPO was met, but the RTO was not.", "isCorrect": false}, {"text": "Both the RTO and RPO were met.", "isCorrect": false}, {"text": "Neither the RTO nor RPO were met.", "isCorrect": false}]	The RTO was met, but the RPO was not.	The Recovery Time Objective (RTO) is the time it takes to restore the service, which was 2 hours. The Recovery Point Objective (RPO) is the amount of acceptable data loss. Since the data was 12 hours old, the 12-hour data loss exceeded their likely RPO target, meaning the RPO was not met.	{"summary": "Distinguish between recovery time and data loss point.", "breakdown": ["RTO (Time): The time to recover the service. 2 hours is the time it took.", "RPO (Data Point): The age of the data upon recovery. 12 hours old means 12 hours of data was lost."], "otherOptions": "B, C, Based on the definitions, the RTO was achieved but the RPO was missed."}	0	\N
157	157	172	Cost Management	Application	Operations and Support	A company notices that their cloud bill is consistently high, even during nights and weekends when traffic is very low. They are running a fleet of virtual machines for a web application. Which of the following is the MOST effective strategy to reduce costs?	[{"text": "Purchase Reserved Instances for all virtual machines.", "isCorrect": false}, {"text": "Implement auto-scaling to automatically reduce the number of running instances during off-peak hours.", "isCorrect": true}, {"text": "Switch to a different cloud provider with a lower hourly rate.", "isCorrect": false}, {"text": "Manually shut down all servers every evening and restart them in the morning.", "isCorrect": false}]	Implement auto-scaling to automatically reduce the number of running instances during off-peak hours.	Auto-scaling is the ideal solution for workloads with variable traffic patterns. It automatically scales out (adds instances) to handle high demand and, crucially, scales in (removes instances) when demand is low, ensuring you only pay for the capacity you actually need.	{"summary": "Auto-scaling aligns cost with demand.", "breakdown": ["It avoids paying for idle resources during nights and weekends.", "The scaling can be based on a schedule for predictable traffic or on metrics for unpredictable traffic.", "This is a primary method for cost optimization in the cloud."], "otherOptions": "Reserved Instances provide a discount but do not address the problem of over-provisioning during low traffic periods.\\nMigrating providers is a major undertaking and does not solve the underlying issue of static capacity.\\nManual intervention is error-prone, can cause downtime, and is not a scalable solution."}	0	\N
216	216	334	Cloud Architecture - Service Models	Comprehension	Cloud Architecture and Models	Your company wants to deploy a web application without managing the underlying servers, operating systems, or runtime environments. Which service model best fits this requirement?	[{"text": "Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "Software as a Service (SaaS)", "isCorrect": false}, {"text": "Platform as a Service (PaaS)", "isCorrect": true}, {"text": "Function as a Service (FaaS)", "isCorrect": false}]	Platform as a Service (PaaS)	PaaS abstracts away infrastructure management while providing a platform for application development and deployment.	{"summary": "PaaS characteristics for this scenario:", "breakdown": ["Eliminates server and OS management overhead", "Provides development tools and runtime environments", "Allows focus on application code and business logic", "Examples: Azure App Service, Google App Engine, Heroku"], "otherOptions": "IaaS requires managing servers and OS\\nSaaS provides complete applications\\nFaaS is for event-driven functions"}	\N	\N
158	158	173	Security	Comprehension	Security	What is the primary function of a key management service (KMS) in the cloud?	[{"text": "To store and manage user passwords for applications.", "isCorrect": false}, {"text": "To create, manage, and control the use of cryptographic keys for data encryption.", "isCorrect": true}, {"text": "To manage SSH keys for accessing virtual machines.", "isCorrect": false}, {"text": "To store API keys for third-party services.", "isCorrect": false}]	To create, manage, and control the use of cryptographic keys for data encryption.	A Key Management Service (KMS) is a centralized system for managing the entire lifecycle of cryptographic keys. This includes key generation, storage, rotation, and deletion, as well as controlling who and what services can use the keys to encrypt and decrypt data.	{"summary": "KMS provides centralized control over encryption keys.", "breakdown": ["It simplifies the process of managing cryptographic keys at scale.", "It integrates with other cloud services to provide transparent data encryption.", "It provides detailed audit logs of all key usage, which is critical for compliance."], "otherOptions": "A, While a KMS could be used for this, a dedicated secrets management service is often a better fit for application credentials.\\nSSH key management is typically handled by other mechanisms, not a KMS."}	0	\N
159	159	174	Virtualization	Knowledge	Cloud Architecture and Design	In virtualization, what is the software that creates and runs virtual machines called?	[{"text": "A guest operating system", "isCorrect": false}, {"text": "A hypervisor", "isCorrect": true}, {"text": "A container engine", "isCorrect": false}, {"text": "A host operating system", "isCorrect": false}]	A hypervisor	The hypervisor, also known as a virtual machine monitor (VMM), is the software that creates the virtualized environment and manages the allocation of physical hardware resources (CPU, memory, storage) to the virtual machines.	{"summary": "The software is called a hypervisor.", "breakdown": ["Type 1 (Bare-Metal) hypervisors run directly on the host's hardware.", "Type 2 (Hosted) hypervisors run on top of a conventional operating system.", "It is the core component that enables virtualization."], "otherOptions": "A guest OS is the operating system running inside a VM.\\nA container engine (like Docker) runs containers, not VMs.\\nA host OS is the operating system on which a Type 2 hypervisor runs."}	0	\N
160	160	175	Networking	Knowledge	Cloud Architecture and Design	What type of network load balancer makes routing decisions based on information at Layer 7 of the OSI model, such as HTTP headers or URL paths?	[{"text": "A network load balancer", "isCorrect": false}, {"text": "An application load balancer", "isCorrect": true}, {"text": "A gateway load balancer", "isCorrect": false}, {"text": "A classic load balancer", "isCorrect": false}]	An application load balancer	Application Load Balancers (ALBs) operate at the application layer (Layer 7). This allows them to inspect application-level content and perform advanced routing, such as sending requests to different backend servers based on the URL path (e.g., /images vs. /api).	{"summary": "Application Load Balancers are Layer 7 aware.", "breakdown": ["They can make intelligent, content-based routing decisions.", "They support features like path-based routing, host-based routing, and SSL termination.", "They are ideal for modern microservices-based architectures."], "otherOptions": "A network load balancer operates at Layer 4 (Transport) and routes based on IP and port, not application content.\\nA gateway load balancer is used to deploy and scale third-party virtual network appliances.\\nA classic load balancer is a legacy type that has been largely superseded by ALBs and NLBs."}	0	\N
161	161	176	Cloud Concepts	Comprehension	Deployment	Which of the following scenarios is the BEST use case for an edge computing strategy?	[{"text": "Running a large-scale data warehousing and analytics platform.", "isCorrect": false}, {"text": "A factory that needs to process sensor data in real-time for immediate machine shutdown to prevent accidents.", "isCorrect": true}, {"text": "Storing long-term backups for regulatory compliance.", "isCorrect": false}, {"text": "Hosting a corporate employee intranet portal.", "isCorrect": false}]	A factory that needs to process sensor data in real-time for immediate machine shutdown to prevent accidents.	Edge computing is designed for use cases that require extremely low latency, real-time data processing, or operation during periods of disconnected network access. Processing IoT sensor data on-site at a factory for immediate safety responses is a classic example.	{"summary": "Edge computing is ideal for low-latency, real-time applications.", "breakdown": ["It brings computation and data storage closer to the sources of data.", "This reduces latency by avoiding a round-trip to a centralized cloud.", "It can improve security and operate even when the primary internet connection is down."], "otherOptions": "A, C, Data warehousing, long-term backups, and intranet portals are all well-suited for a centralized cloud architecture and do not have the same extreme low-latency requirements."}	0	\N
162	162	178	Security	Application	Security	An administrator needs to ensure that all data written to a cloud storage bucket is encrypted, without requiring any action from the applications or users who upload the data. Which feature should be enabled on the storage bucket?	[{"text": "Client-side encryption", "isCorrect": false}, {"text": "Server-side encryption", "isCorrect": true}, {"text": "Multi-factor authentication", "isCorrect": false}, {"text": "Access control lists (ACLs)", "isCorrect": false}]	Server-side encryption	Server-side encryption is a feature where the cloud provider automatically encrypts data as it is written to the storage service and decrypts it when it is accessed. This is transparent to the end-user or application and enforces encryption for all objects in the bucket.	{"summary": "Server-side encryption enforces encryption at rest.", "breakdown": ["The encryption and decryption are handled automatically by the service.", "It protects data from unauthorized access if the physical storage media is compromised.", "It is a standard security best practice and a requirement for many compliance frameworks."], "otherOptions": "Client-side encryption requires the application or user to encrypt the data *before* uploading it.\\nMFA is an access control measure for users, it does not encrypt data.\\nACLs are a form of access control, they do not encrypt data."}	0	\N
163	163	179	Automation	Comprehension	Deployment	Which of the following is an example of orchestration?	[{"text": "A script that automatically reboots a server every night.", "isCorrect": false}, {"text": "A workflow that provisions a network, deploys a database, deploys a set of web servers, and then configures a load balancer.", "isCorrect": true}, {"text": "A tool that ensures a specific software package is installed on all servers.", "isCorrect": false}, {"text": "A manual checklist that an operator follows to deploy an application.", "isCorrect": false}]	A workflow that provisions a network, deploys a database, deploys a set of web servers, and then configures a load balancer.	Orchestration is the coordination of multiple automated tasks to execute a larger workflow. Deploying a complete multi-tier application involves a sequence of dependent tasks, which is a perfect example of orchestration.	{"summary": "Orchestration coordinates multiple automated tasks.", "breakdown": ["It manages the entire lifecycle of a complex service.", "It handles dependencies and sequencing between different tasks.", "It is a higher level of automation than simply scripting a single action."], "otherOptions": "A, These are examples of automation (a single automated task), not orchestration.\\nThis is a manual process, not automation or orchestration."}	0	\N
164	164	180	Networking	Knowledge	Cloud Architecture and Design	What is the purpose of a VPC Peering connection?	[{"text": "To connect a VPC to the public internet.", "isCorrect": false}, {"text": "To connect a VPC to an on-premises data center.", "isCorrect": false}, {"text": "To connect two VPCs together privately, allowing them to communicate as if they were on the same network.", "isCorrect": true}, {"text": "To provide a dedicated endpoint for accessing a specific cloud service without traversing the internet.", "isCorrect": false}]	To connect two VPCs together privately, allowing them to communicate as if they were on the same network.	VPC Peering is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network.	{"summary": "VPC Peering connects two VPCs privately.", "breakdown": ["Traffic uses the cloud provider's private backbone, not the public internet.", "The connection is not a gateway or a VPN connection and does not rely on a separate piece of physical hardware.", "It has limitations, such as not being transitive (if A is peered with B, and B with C, A cannot talk to C)."], "otherOptions": "This is done with an Internet Gateway.\\nThis is done with a VPN or a Direct Connect/ExpressRoute.\\nThis describes a VPC Endpoint."}	0	\N
165	165	181	Cloud Concepts	Comprehension	Operations and Support	A company is considering moving from an on-premises data center to the cloud. Which of the following is a primary financial benefit of this move?	[{"text": "Converting Capital Expenditure (CapEx) to Operational Expenditure (OpEx).", "isCorrect": true}, {"text": "Converting Operational Expenditure (OpEx) to Capital Expenditure (CapEx).", "isCorrect": false}, {"text": "Eliminating all IT operational costs.", "isCorrect": false}, {"text": "Increasing the total cost of ownership (TCO).", "isCorrect": false}]	Converting Capital Expenditure (CapEx) to Operational Expenditure (OpEx).	One of the main financial drivers for cloud adoption is the shift from CapEx to OpEx. Instead of making large, upfront investments in hardware and data centers (CapEx), companies can pay a monthly, operational fee for the services they consume (OpEx).	{"summary": "Cloud shifts IT spending from CapEx to OpEx.", "breakdown": ["CapEx: Large, upfront investments in physical assets.", "OpEx: Ongoing, pay-as-you-go expenses.", "This shift improves cash flow and allows businesses to be more agile without large capital outlays."], "otherOptions": "This is the opposite of what happens.\\nCloud computing reduces some operational costs but does not eliminate them.\\nThe goal of moving to the cloud is typically to reduce the TCO."}	0	\N
166	166	182	Security	Application	Security	An application requires access to a database password to connect to its database. What is the MOST secure way to provide this credential to the application running on a virtual machine in the cloud?	[{"text": "Store the password in a plain text file on the virtual machine.", "isCorrect": false}, {"text": "Hardcode the password directly into the application's source code.", "isCorrect": false}, {"text": "Use a secrets management service to store the password and an IAM role to grant the application permission to retrieve it at runtime.", "isCorrect": true}, {"text": "Pass the password to the virtual machine as an environment variable during startup.", "isCorrect": false}]	Use a secrets management service to store the password and an IAM role to grant the application permission to retrieve it at runtime.	Storing secrets in a dedicated service like AWS Secrets Manager or Azure Key Vault is the security best practice. The application can then be given a specific IAM role that grants it permission to fetch only the secrets it needs, just-in-time. This avoids storing credentials on the instance or in the code.	{"summary": "Use a dedicated secrets management service with IAM roles.", "breakdown": ["It avoids hardcoding secrets, which is a major security risk.", "It allows for centralized management and rotation of secrets.", "Access to secrets can be tightly controlled and audited using IAM policies.", "This is the standard for modern, secure cloud applications."], "otherOptions": "A, B, Storing secrets in plain text, in code, or as environment variables are all insecure practices that expose the credentials."}	0	\N
167	167	183	Databases	Comprehension	Cloud Architecture and Design	What is a key benefit of using a managed database service (e.g., Amazon RDS, Azure SQL Database) compared to running a database on a self-managed virtual machine?	[{"text": "It provides full root access to the underlying operating system.", "isCorrect": false}, {"text": "It offloads operational tasks like patching, backups, and high availability to the cloud provider.", "isCorrect": true}, {"text": "It is always less expensive than a self-managed database.", "isCorrect": false}, {"text": "It allows you to use any database engine, including proprietary or custom ones.", "isCorrect": false}]	It offloads operational tasks like patching, backups, and high availability to the cloud provider.	The primary value of a managed database service is the reduction of operational burden. The cloud provider handles time-consuming administrative tasks, allowing the customer to focus on their application and data schema.	{"summary": "Managed databases reduce operational overhead.", "breakdown": ["Automated patching and software updates.", "Automated backups and point-in-time recovery.", "Simplified high availability and read replica setup.", "Built-in monitoring and alerting."], "otherOptions": "Managed services abstract the OS, so you do not get root access.\\nIt can sometimes be more expensive in terms of direct cost, but the TCO is often lower due to reduced operational effort.\\nManaged services support a specific set of popular database engines."}	0	\N
168	168	184	DevOps	Knowledge	Deployment	In a CI/CD pipeline, what is the purpose of the 'build' stage?	[{"text": "To provision the infrastructure needed for the application.", "isCorrect": false}, {"text": "To compile the source code into a deployable artifact, such as a binary, package, or container image.", "isCorrect": true}, {"text": "To run security scans on the production environment.", "isCorrect": false}, {"text": "To deploy the application to the end-users.", "isCorrect": false}]	To compile the source code into a deployable artifact, such as a binary, package, or container image.	The build stage is a fundamental part of the CI process. It takes the developer's source code, resolves dependencies, and compiles it into an executable or packaged format that can be tested and eventually deployed.	{"summary": "The build stage creates a deployable artifact.", "breakdown": ["It is typically triggered by a code commit to the repository.", "It may involve compiling code, running linters, and packaging assets.", "A successful build produces a single, versioned artifact that is used in all subsequent stages of the pipeline."], "otherOptions": "This is done during a provisioning stage, often using IaC.\\nSecurity scans can be part of the pipeline, but the build stage is specifically about compiling code.\\nThis is the deployment stage."}	0	\N
169	169	185	Business Continuity	Application	Troubleshooting	A company's disaster recovery plan states that they must have a fully functional copy of their infrastructure running in a secondary region, but it should handle no live traffic until a failover is initiated. During the failover, traffic is redirected to the secondary region. Which DR strategy is this?	[{"text": "Backup and Restore", "isCorrect": false}, {"text": "Pilot Light", "isCorrect": false}, {"text": "Warm Standby", "isCorrect": true}, {"text": "Multi-Site Active-Active", "isCorrect": false}]	Warm Standby	A warm standby strategy involves having a scaled-down but fully functional copy of your infrastructure running in the DR region. Upon failover, the system is scaled up to handle the full production load. It is faster than pilot light but less expensive than active-active.	{"summary": "This describes a warm standby DR strategy.", "breakdown": ["A scaled-down version of the full infrastructure is always running.", "Data is actively being replicated or backed up to the DR site.", "Recovery involves scaling up the resources and redirecting traffic.", "It provides a good balance between cost and recovery time (RTO)."], "otherOptions": "Backup and restore involves creating new infrastructure from backups, which is much slower.\\nPilot light only keeps the most critical core components running (like a database replica), not a full, scaled-down environment.\\nActive-active would involve the secondary site actively handling a portion of the live traffic."}	0	\N
170	170	186	Storage	Comprehension	Cloud Architecture and Design	What is a key difference between file storage and object storage?	[{"text": "File storage is for unstructured data, while object storage is for structured data.", "isCorrect": false}, {"text": "File storage presents data in a hierarchical file system, while object storage uses a flat address space.", "isCorrect": true}, {"text": "File storage is more scalable than object storage.", "isCorrect": false}, {"text": "File storage is accessed via an API, while object storage is mounted as a network drive.", "isCorrect": false}]	File storage presents data in a hierarchical file system, while object storage uses a flat address space.	This is the fundamental architectural difference. File storage (like NFS/SMuses a familiar hierarchy of directories and files. Object storage uses a flat model where each object is retrieved via a unique ID, along with its data and metadata, without a folder structure.	{"summary": "File storage is hierarchical; object storage is flat.", "breakdown": ["File Storage: Uses a file-and-folder structure, accessed via file paths.", "Object Storage: Stores objects in a single, massive pool, accessed via a unique object ID.", "This flat structure allows object storage to scale to virtually unlimited size, which is difficult for traditional file systems."], "otherOptions": "This is reversed; object storage is ideal for unstructured data.\\nThis is reversed; object storage is significantly more scalable.\\nThis is reversed; object storage is accessed via API, file storage is mounted."}	0	\N
171	171	195	Cloud Concepts	Knowledge	Cloud Architecture and Design	Which of the following are characteristics of a public cloud deployment model? (Choose TWO)	[{"text": "Resources are owned and operated by a third-party cloud provider.", "isCorrect": true}, {"text": "The infrastructure is shared among multiple organizations (multi-tenant).", "isCorrect": true}, {"text": "The infrastructure is dedicated to a single customer.", "isCorrect": false}, {"text": "It requires significant upfront capital expenditure (CapEx) from the customer.", "isCorrect": false}, {"text": "The customer has full physical control over the hardware.", "isCorrect": false}]	A, B	A public cloud is defined by two key characteristics: the infrastructure is owned and managed by a third-party provider (like AWS, Azure, or Google), and that infrastructure is shared by multiple customers in a multi-tenant model.	{"summary": "Public cloud is owned by a third party and is multi-tenant.", "breakdown": ["The provider is responsible for all hardware and data center management.", "Customers share the underlying physical resources, which is what drives the economies of scale.", "It operates on a pay-as-you-go, operational expenditure (OpEx) model."], "otherOptions": "C, Dedicated infrastructure with full physical control describes an on-premises data center or a private cloud.\\nPublic cloud is designed to reduce or eliminate customer CapEx."}	1	{A,B}
172	172	187	Security	Comprehension	Security	Which of the following statements are true about a stateless network firewall like a Network ACL (NACL)? (Choose TWO)	[{"text": "It automatically allows return traffic for an allowed inbound request.", "isCorrect": false}, {"text": "You must explicitly define both inbound and outbound rules for a request and its response to be successful.", "isCorrect": true}, {"text": "It is attached directly to virtual machine instances.", "isCorrect": false}, {"text": "It processes rules in order, starting from the lowest numbered rule, and stops when it finds a match.", "isCorrect": true}, {"text": "It can only have `allow` rules, not `deny` rules.", "isCorrect": false}]	B, D	Stateless firewalls do not track the state of connections. This means you must explicitly create rules for both the ingress (inbound) and egress (outbound) traffic. They evaluate rules in numerical order, and the first rule that matches the traffic is immediately applied.	{"summary": "NACLs are stateless and process rules in order.", "breakdown": ["Stateless: It does not remember previous packets. You must create an outbound rule to allow the return traffic for an inbound request.", "Rule Order: Rules are evaluated by number, from lowest to highest. The first matching rule is executed, and subsequent rules are ignored."], "otherOptions": "This describes a stateful firewall, like a security group.\\nNACLs are attached to subnets, not instances. Security groups are attached to instances.\\nNACLs can have both 'allow' and 'deny' rules."}	1	{B,D}
174	174	189	Operations	Comprehension	Operations and Support	An application team is consistently deploying new code that causes performance issues in production. The operations team wants to implement a process to catch these issues before they impact all users. Which of the following is the BEST strategy?	[{"text": "Require all developers to get senior management approval before committing code.", "isCorrect": false}, {"text": "Implement a blue-green deployment strategy where the new version is extensively load-tested in the 'green' environment before switching traffic.", "isCorrect": true}, {"text": "Double the amount of monitoring alerts to catch issues faster.", "isCorrect": false}, {"text": "Stop all new deployments until the application is rewritten.", "isCorrect": false}]	Implement a blue-green deployment strategy where the new version is extensively load-tested in the 'green' environment before switching traffic.	This strategy allows the new code to be deployed to a separate, identical production environment (green) where it can undergo rigorous performance and load testing without affecting any live users on the current (blue) environment. Only after it passes these tests is traffic switched over.	{"summary": "Blue-green deployment allows for safe pre-release testing.", "breakdown": ["The 'green' environment is a full-scale replica of production.", "It enables realistic load testing to identify performance regressions.", "If issues are found, the release is aborted with zero impact on live users.", "This is a key pattern for safe, high-quality releases."], "otherOptions": "This is a bureaucratic process that will slow down development without solving the technical problem.\\nMore alerts don't prevent the issues, they just report on them after they have already impacted users.\\nHalting deployments is not a sustainable business strategy."}	0	\N
175	175	191	Security	Application	Security	A company wants to ensure that all objects uploaded to a specific cloud storage bucket are automatically scanned for malware. What type of solution should they implement?	[{"text": "A security group attached to the storage bucket.", "isCorrect": false}, {"text": "An event-driven security workflow that triggers a scanning function on every object creation event.", "isCorrect": true}, {"text": "A lifecycle policy that deletes objects after 24 hours.", "isCorrect": false}, {"text": "A network ACL on the subnet where the bucket resides.", "isCorrect": false}]	An event-driven security workflow that triggers a scanning function on every object creation event.	Modern cloud platforms can generate events when actions occur, such as an object being uploaded. An event-driven architecture can listen for these 'object created' events and trigger a serverless function or container that runs a malware scanner on the newly uploaded object.	{"summary": "Use event-driven automation for real-time security scanning.", "breakdown": ["Storage services can emit events for actions like 'PutObject'.", "These events can trigger compute services (like AWS Lambda).", "The triggered function can then perform an action, such as scanning the object with an antivirus engine.", "This provides automated, real-time threat detection."], "otherOptions": "A, Security groups and NACLs are network firewalls; they cannot inspect the content of files for malware.\\nA lifecycle policy manages the storage class or deletion of objects over time; it does not scan them."}	0	\N
176	176	192	Databases	Comprehension	Cloud Architecture and Design	What is the primary use case for an in-memory database or cache (e.g., Redis, Memcached)?	[{"text": "For long-term, durable storage of relational data.", "isCorrect": false}, {"text": "To store data with extremely low latency requirements for rapid access, such as for caching database query results or user sessions.", "isCorrect": true}, {"text": "To store large binary files like videos and images.", "isCorrect": false}, {"text": "To meet strict data compliance and archival requirements.", "isCorrect": false}]	To store data with extremely low latency requirements for rapid access, such as for caching database query results or user sessions.	In-memory databases store data in RAM instead of on disk, which provides microsecond read and write latency. This makes them ideal for caching layers that absorb load from slower, disk-based databases, and for use cases that require real-time speed, like leaderboards or session stores.	{"summary": "In-memory caches provide ultra-low latency data access.", "breakdown": ["Data is stored in RAM, which is orders of magnitude faster than SSDs.", "Commonly used to cache frequently accessed data to reduce latency and database load.", "Can be used as a primary database for specific, high-performance use cases."], "otherOptions": "In-memory databases are typically not designed for long-term durability in the same way as disk-based databases.\\nWhile they can store binary data, they are not cost-effective for large files; object storage is better.\\nThis is a use case for archive-tier object storage, not an in-memory cache."}	0	\N
177	177	194	Troubleshooting	Comprehension	Troubleshooting	An administrator notices that a specific API call in their application is occasionally very slow. They want to understand the complete path of this request, including the time it spends in each downstream microservice it calls. Which observability tool would be BEST for this analysis?	[{"text": "Metrics dashboards", "isCorrect": false}, {"text": "Log aggregation", "isCorrect": false}, {"text": "Distributed tracing", "isCorrect": true}, {"text": "Uptime monitoring", "isCorrect": false}]	Distributed tracing	Distributed tracing is specifically designed to track the lifecycle of a single request as it propagates through a complex, distributed system like a microservices architecture. It provides a detailed, flame-graph view of the request path, showing the latency contributed by each service call.	{"summary": "Distributed tracing is the tool for analyzing request paths.", "breakdown": ["It captures the parent-child relationships between service calls.", "It allows you to pinpoint which specific downstream service is causing a slowdown.", "It is a key part of the 'three pillars of observability' (along with metrics and logs)."], "otherOptions": "Metrics can show you that the API is slow, but not *why* or *where* in the call stack the slowness is occurring.\\nLogs can provide details from each service, but correlating them for a single request without a trace ID is very difficult.\\nUptime monitoring only tells you if the service is up or down, not its performance."}	0	\N
178	178	196	Storage	Application	Deployment	An application needs a shared file system that can be mounted and accessed simultaneously by hundreds of Linux-based virtual machines running in different availability zones. The file system must support the NFS protocol. Which type of cloud storage solution is required?	[{"text": "Block storage", "isCorrect": false}, {"text": "Object storage", "isCorrect": false}, {"text": "A distributed file system service (e.g., Amazon EFS, Azure Files)", "isCorrect": true}, {"text": "Ephemeral storage", "isCorrect": false}]	A distributed file system service (e.g., Amazon EFS, Azure Files)	A distributed file system service is designed for this exact use case. It provides a managed, scalable file system that can be concurrently accessed by many clients using standard file protocols like NFS or SMB.	{"summary": "A managed, distributed file system is the correct solution.", "breakdown": ["It allows simultaneous access from many VMs.", "It can span multiple availability zones for high availability.", "It supports standard protocols like NFS, meaning no application changes are required.", "It scales automatically as data is added."], "otherOptions": "Block storage volumes cannot be mounted by hundreds of VMs simultaneously.\\nObject storage is accessed via API and cannot be mounted as a file system in this way.\\nEphemeral storage is temporary and not shared."}	0	\N
179	179	197	Security	Comprehension	Security	Which of the following BEST describes the concept of "defense in depth"?	[{"text": "Using only the strongest possible encryption for all data.", "isCorrect": false}, {"text": "Layering multiple, different security controls to protect an asset.", "isCorrect": true}, {"text": "Focusing all security efforts on protecting the network perimeter.", "isCorrect": false}, {"text": "Relying on a single, highly advanced security appliance.", "isCorrect": false}]	Layering multiple, different security controls to protect an asset.	Defense in depth is a security strategy that uses multiple layers of security controls. The idea is that if one layer fails, another layer is there to stop the attack. For example, protecting a database with a network firewall, host-based firewall, IAM permissions, and encryption.	{"summary": "Defense in depth is a layered security approach.", "breakdown": ["It provides redundancy in security.", "It protects against a wide variety of attack vectors.", "It acknowledges that no single security control is perfect.", "Layers can be physical, technical, and administrative."], "otherOptions": "Encryption is just one layer of a defense-in-depth strategy.\\nThis is the outdated 'perimeter security' model, which defense in depth improves upon.\\nRelying on a single control is the opposite of a layered approach."}	0	\N
180	180	198	Operations	Knowledge	Troubleshooting	An administrator is reviewing logs and sees a series of failed login attempts for a privileged account, followed by a successful login from an unfamiliar IP address. What type of incident might this indicate?	[{"text": "A denial-of-service (DoS) attack.", "isCorrect": false}, {"text": "A brute-force attack leading to an account compromise.", "isCorrect": true}, {"text": "A misconfigured network firewall.", "isCorrect": false}, {"text": "A server hardware failure.", "isCorrect": false}]	A brute-force attack leading to an account compromise.	The pattern of numerous failed logins followed by a success is the classic signature of a brute-force or dictionary attack, where an attacker tries many passwords until they guess the correct one. The login from an unfamiliar IP further suggests that the account has been compromised.	{"summary": "This pattern indicates a brute-force account compromise.", "breakdown": ["Multiple failed logins suggest an automated password guessing attack.", "The final successful login means the attack succeeded.", "The unfamiliar IP address is a strong indicator of an external attacker.", "This is a critical security incident that requires immediate response."], "otherOptions": "A DoS attack would involve overwhelming the service with traffic, not login attempts.\\nA firewall misconfiguration would block traffic, not cause failed logins.\\nA hardware failure would not manifest as login activity."}	0	\N
181	181	199	DevOps	Comprehension	Deployment	What is the relationship between containers and microservices?	[{"text": "They are the same thing.", "isCorrect": false}, {"text": "Microservices is an architectural style, and containers are a common technology used to package and deploy them.", "isCorrect": true}, {"text": "Containers are a type of microservice.", "isCorrect": false}, {"text": "You must use microservices in order to use containers.", "isCorrect": false}]	Microservices is an architectural style, and containers are a common technology used to package and deploy them.	Microservices is an approach to building an application as a collection of small, independent services. Containers (like Docker) are an ideal technology for running microservices because they provide lightweight isolation and a consistent runtime environment, making it easy to deploy and scale each service independently.	{"summary": "Microservices are an architecture; containers are a deployment technology.", "breakdown": ["You can run microservices without containers (e.g., on VMs).", "You can run monolithic applications inside containers.", "However, the two are a very popular and effective combination because containers make managing many small services much easier."], "otherOptions": "A, C, These statements represent common misconceptions about the relationship between the two concepts."}	0	\N
182	182	200	Cost Management	Comprehension	Operations and Support	A company has a number of virtual machines that are used for development and testing. These workloads are not critical and can tolerate interruptions. Which cloud pricing model would offer the greatest cost savings for these VMs?	[{"text": "On-demand", "isCorrect": false}, {"text": "Reserved instances", "isCorrect": false}, {"text": "Spot instances / Low-priority VMs", "isCorrect": true}, {"text": "Dedicated hosts", "isCorrect": false}]	Spot instances / Low-priority VMs	Spot instances (or Low-priority VMs) leverage a cloud provider's spare, unused compute capacity at a very large discount (often up to 90%) compared to on-demand prices. The trade-off is that the provider can reclaim this capacity at any time. This makes it a perfect fit for non-critical, fault-tolerant workloads like development, testing, and batch processing.	{"summary": "Spot instances offer the largest discounts for interruptible workloads.", "breakdown": ["They provide access to spare capacity at a steep discount.", "They can be terminated with very short notice.", "They are ideal for workloads that can be stopped and restarted without negative impact."], "otherOptions": "On-demand is the most expensive and flexible model.\\nReserved instances offer a discount for a long-term commitment and are for persistent workloads.\\nDedicated hosts are the most expensive option and are for workloads with specific compliance or licensing needs."}	0	\N
183	183	201	Networking	Comprehension	Cloud Architecture and Design	You need to create a logically isolated section of the public cloud where you can launch resources in a virtual network that you define. What is this isolated network environment called?	[{"text": "An availability zone", "isCorrect": false}, {"text": "A Virtual Private Cloud (VPor Virtual Network (VNet)", "isCorrect": true}, {"text": "A subnet", "isCorrect": false}, {"text": "A security group", "isCorrect": false}]	A Virtual Private Cloud (VPor Virtual Network (VNet)	A VPC (in AWS) or VNet (in Azure) is a private, isolated virtual network within the public cloud. It allows you to provision your own logically isolated section of the cloud where you can launch resources with full control over the IP address range, subnets, route tables, and network gateways.	{"summary": "This is a Virtual Private Cloud (VPor Virtual Network (VNet).", "breakdown": ["It provides network-level isolation for your cloud resources.", "You have control over the virtual networking environment.", "It is a fundamental building block for any cloud deployment."], "otherOptions": "An availability zone is a physical data center location.\\nA subnet is a segment or subdivision of a VPC/VNet.\\nA security group is a firewall for your virtual machines."}	0	\N
184	184	302	Databases	Comprehension	Cloud Architecture and Design	Which type of database is best suited for storing and querying data with complex relationships and a predefined schema, such as a customer relationship management (CRM) system?	[{"text": "A NoSQL key-value store", "isCorrect": false}, {"text": "A relational database (e.g., SQL)", "isCorrect": true}, {"text": "An in-memory cache", "isCorrect": false}, {"text": "An object storage system", "isCorrect": false}]	A relational database (e.g., SQL)	Relational databases excel at managing structured data with well-defined relationships between different data entities (e.g., customers, orders, products). They use a predefined schema to enforce data integrity and support complex queries using SQL (Structured Query Language).	{"summary": "Relational databases are for structured data with complex relationships.", "breakdown": ["Data is stored in tables with rows and columns.", "A schema defines the structure of the data.", "They enforce ACID (Atomicity, Consistency, Isolation, Durability) properties for transactions.", "Examples include MySQL, PostgreSQL, and Microsoft SQL Server."], "otherOptions": "A key-value store is a type of NoSQL database best for simple lookups, not complex relationships.\\nAn in-memory cache is for performance, not for durable, structured data storage.\\nObject storage is for unstructured binary data, not for structured, queryable data."}	0	\N
185	185	303	Automation	Comprehension	Deployment	What is a primary benefit of using version control (e.g., Git) for Infrastructure as Code (IaC)?	[{"text": "It guarantees that the infrastructure will never fail.", "isCorrect": false}, {"text": "It provides an auditable history of all changes made to the infrastructure and enables collaboration.", "isCorrect": true}, {"text": "It automatically optimizes the cost of the deployed infrastructure.", "isCorrect": false}, {"text": "It eliminates the need to test infrastructure changes.", "isCorrect": false}]	It provides an auditable history of all changes made to the infrastructure and enables collaboration.	Treating infrastructure as code allows you to store the configuration files in a version control system like Git. This provides a full, auditable log of every change, who made it, and when. It also enables collaboration through features like branching and pull requests.	{"summary": "Version control provides auditability and collaboration for IaC.", "breakdown": ["Every change to the infrastructure is tracked and can be reviewed.", "It makes it easy to roll back to a previous known-good configuration if a change causes problems.", "It allows multiple team members to work on the infrastructure configuration concurrently."], "otherOptions": "No system can guarantee zero failures.\\nCost optimization is a separate practice; IaC does not do it automatically.\\nOn the contrary, IaC makes it *easier* to test infrastructure changes before deploying them."}	0	\N
186	186	304	Security	Comprehension	Security	Which of the following cloud identity concepts allows an application to obtain temporary, limited-privilege credentials to access other cloud resources?	[{"text": "A user account with a permanent password.", "isCorrect": false}, {"text": "An IAM Role or Service Principal.", "isCorrect": true}, {"text": "An API key stored in a configuration file.", "isCorrect": false}, {"text": "A shared administrative account.", "isCorrect": false}]	An IAM Role or Service Principal.	IAM Roles (in AWS) or Service Principals (in Azure) are identities that applications or services can assume to securely obtain temporary security credentials. This is the recommended best practice for service-to-service authentication, as it avoids the use of long-lived, static credentials like API keys.	{"summary": "IAM Roles provide secure, temporary credentials for applications.", "breakdown": ["The application assumes the role at runtime to get temporary tokens.", "Permissions are defined on the role, not the application, following the principle of least privilege.", "This eliminates the risk of static, long-lived credentials being leaked."], "otherOptions": "A, C, Permanent passwords, stored API keys, and shared accounts are all insecure practices that should be avoided."}	0	\N
187	187	305	Cloud Concepts	Comprehension	Cloud Architecture and Design	A company is using a cloud provider that has multiple geographic locations around the world, such as "US East", "EU West", and "Asia Pacific (Tokyo)". What are these top-level geographic locations called?	[{"text": "Availability Zones", "isCorrect": false}, {"text": "Regions", "isCorrect": true}, {"text": "Data Centers", "isCorrect": false}, {"text": "Subnets", "isCorrect": false}]	Regions	A region is a distinct geographic area where a cloud provider has a collection of data centers. Regions are isolated from each other to provide fault tolerance and to allow customers to place resources closer to their end-users or to meet data sovereignty requirements.	{"summary": "These geographic locations are called Regions.", "breakdown": ["Regions are the highest level of geographic division in a cloud provider's infrastructure.", "Each region contains multiple, isolated Availability Zones.", "Choosing the right region is important for latency and data residency."], "otherOptions": "Availability Zones are the data centers *within* a region.\\nData center is a more generic term; Region is the specific term used by cloud providers.\\nSubnets are network segments *within* a virtual network."}	0	\N
188	188	306	Operations	Knowledge	Operations and Support	An administrator needs to automate the process of applying operating system security patches to a large fleet of virtual machines. Which type of tool is best suited for this task?	[{"text": "A monitoring tool", "isCorrect": false}, {"text": "A configuration management tool (e.g., Ansible, Puppet, Chef)", "isCorrect": true}, {"text": "An Infrastructure as Code tool (e.g., Terraform)", "isCorrect": false}, {"text": "A CI/CD tool (e.g., Jenkins)", "isCorrect": false}]	A configuration management tool (e.g., Ansible, Puppet, Chef)	Configuration management tools are designed to maintain the state of existing servers. This includes tasks like installing software, configuring services, and applying patches. They can be used to ensure that an entire fleet of servers is consistently patched and configured correctly.	{"summary": "Configuration management tools automate patching.", "breakdown": ["They can connect to a fleet of servers and execute tasks.", "They can check the current state and only apply changes if needed (idempotency).", "They are essential for managing large numbers of servers at scale."], "otherOptions": "A monitoring tool can tell you if a server needs patches, but it cannot apply them.\\nIaC tools are for provisioning the initial infrastructure, not for ongoing maintenance like patching.\\nCI/CD tools are for deploying application code, not for managing the OS."}	0	\N
189	189	307	Troubleshooting	Application	Troubleshooting	Users are reporting that a web application is extremely slow. A quick check shows that the database server has very high disk read latency and its I/O operations queue is full. Which of the following are the MOST likely solutions to this problem? (Choose TWO)	[{"text": "Increase the network bandwidth to the database server.", "isCorrect": false}, {"text": "Migrate the database to a storage volume with higher IOPS (e.g., from HDD to SSD).", "isCorrect": true}, {"text": "Implement a caching layer to reduce the number of read requests hitting the database.", "isCorrect": true}, {"text": "Increase the CPU cores of the web servers.", "isCorrect": false}, {"text": "Restart the database server.", "isCorrect": false}]	B, C	The symptomshigh read latency and a full I/O queuepoint directly to a storage performance bottleneck at the database. The two most effective solutions are to increase the underlying storage performance (by moving to a higher IOPS volume like an SSand to reduce the load on the database by implementing a cache for frequently read data.	{"summary": "Address a storage I/O bottleneck by improving storage or reducing load.", "breakdown": ["Upgrading storage from a standard HDD to a Provisioned IOPS SSD will directly increase the I/O capacity of the database.", "A caching layer (like Redis or Memcached) can handle a large percentage of the read requests, preventing them from ever reaching the overburdened database."], "otherOptions": "The problem is with disk I/O, not the network.\\nThe bottleneck is at the database, not the web servers.\\nRestarting the server will not fix an underlying performance bottleneck and will only cause downtime."}	1	{B,C}
190	190	308	Deployment	Knowledge	Deployment	Which of the following BEST describes a container image?	[{"text": "A running instance of an application with its dependencies.", "isCorrect": false}, {"text": "A lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, and settings.", "isCorrect": true}, {"text": "A type of virtual machine that includes a full guest operating system.", "isCorrect": false}, {"text": "A script used to configure a server's operating system.", "isCorrect": false}]	A lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, and settings.	A container image is a static, immutable file that contains all the necessary code, dependencies, and configurations needed to run an application. When you run the image, it becomes a container. This packaging makes the application highly portable.	{"summary": "A container image is a portable, self-contained software package.", "breakdown": ["It is a template or blueprint for creating a container.", "It includes the application code and all its dependencies.", "This ensures that the application runs consistently in any environment (development, testing, production)."], "otherOptions": "This describes a container, which is a running instance of an image.\\nThis describes a virtual machine image, which is much larger as it contains a full OS.\\nThis describes a configuration management script."}	0	\N
191	191	309	Security	Comprehension	Security	Which of the following is a key security concern specifically related to a multi-tenant cloud environment?	[{"text": "The physical security of the data center.", "isCorrect": false}, {"text": "The risk of data leakage or interference between different tenants sharing the same physical hardware.", "isCorrect": true}, {"text": "The need to patch the host operating system.", "isCorrect": false}, {"text": "The risk of a power failure in the data center.", "isCorrect": false}]	The risk of data leakage or interference between different tenants sharing the same physical hardware.	In a multi-tenant architecture, multiple customers (tenants) are running their applications on the same shared physical infrastructure. The primary security challenge is ensuring that these tenants are logically isolated and that one tenant cannot access another tenant's data or impact their performance (the "noisy neighbor" problem).	{"summary": "Tenant isolation is the primary security concern in multi-tenancy.", "breakdown": ["Cloud providers invest heavily in the hypervisor and other technologies to ensure strong logical isolation.", "Vulnerabilities in the hypervisor could potentially lead to a tenant 'escape' and compromise the host.", "This is a key area of focus for cloud security professionals."], "otherOptions": "A, C, Physical security, host patching, and power redundancy are responsibilities of the cloud provider but are not concerns specific *to the customer* in a multi-tenant model."}	0	\N
192	192	323	Cloud Concepts	Knowledge	Cloud Architecture and Design	What is the term for the cloud computing characteristic that allows a provider's resources to be shared among multiple customers, with safeguards for isolation?	[{"text": "Elasticity", "isCorrect": false}, {"text": "Multi-tenancy", "isCorrect": true}, {"text": "High availability", "isCorrect": false}, {"text": "On-demand", "isCorrect": false}]	Multi-tenancy	Multi-tenancy is an architecture in which a single instance of a software application serves multiple customers. Each customer is called a tenant. Tenants may be given the ability to customize some parts of the application, but they cannot customize the application's code. This is the model that allows public cloud providers to achieve massive economies of scale.	{"summary": "Multi-tenancy is the architecture of shared resources.", "breakdown": ["It is a core principle of public cloud computing.", "It allows for efficient resource utilization and lower costs.", "Strong logical isolation between tenants is a critical security requirement."], "otherOptions": "Elasticity is the ability to scale resources.\\nHigh availability is about resilience to failure.\\nOn-demand is the ability to self-provision resources."}	0	\N
193	193	310	Operations	Application	Troubleshooting	You are running a web application that consists of several microservices. Users report intermittent errors. You need to trace a single user's request from the initial load balancer all the way through the various microservices it interacts with to pinpoint where the error is occurring. Which of the following would you need to implement?	[{"text": "Centralized logging with correlation IDs.", "isCorrect": true}, {"text": "A network packet capture on the load balancer.", "isCorrect": false}, {"text": "CPU and memory monitoring for each microservice.", "isCorrect": false}, {"text": "An uptime monitoring service that pings the main URL.", "isCorrect": false}]	Centralized logging with correlation IDs.	To trace a single request through a distributed system, you need two things: 1) Centralized logging to bring all the logs from different services into one place, and 2) A correlation ID (or trace Ithat is generated at the start of the request and passed along to every microservice it touches. This allows you to filter the centralized logs to see the complete path of that single request. This is the foundation of distributed tracing.	{"summary": "Centralized logging with correlation IDs enables request tracing.", "breakdown": ["A correlation ID is a unique identifier attached to every log message generated by a single request.", "Centralized logging collects all logs into a searchable system.", "By searching for the correlation ID, you can reconstruct the entire journey of the user's request."], "otherOptions": "A packet capture is too low-level and difficult to analyze for application logic.\\nMetrics can show that a service is unhealthy, but not the path of the request that caused the error.\\nUptime monitoring only tells you if the entry point is available."}	0	\N
194	194	311	Storage	Knowledge	Cloud Architecture and Design	Which type of data is NOT well-suited for storage in a relational database?	[{"text": "Customer records with defined fields like name, address, and phone number.", "isCorrect": false}, {"text": "Financial transactions that must be ACID compliant.", "isCorrect": false}, {"text": "Large, unstructured binary files like high-resolution videos and audio files.", "isCorrect": true}, {"text": "An inventory system with tables for products, suppliers, and warehouses.", "isCorrect": false}]	Large, unstructured binary files like high-resolution videos and audio files.	Relational databases are optimized for structured, transactional data. They are not designed to store large binary objects (BLOBs), and doing so is inefficient, expensive, and can severely degrade database performance. This type of unstructured data is best stored in an object storage system.	{"summary": "Relational databases are poor at storing large, unstructured files.", "breakdown": ["Storing large files in a database bloats its size and slows down backups and queries.", "Object storage is designed for this use case and is much more scalable and cost-effective.", "The common pattern is to store the file in object storage and then store the *URL* or *identifier* of that object in the relational database."], "otherOptions": "A, B, Customer records, financial transactions, and inventory systems are all classic examples of structured, relational data that are a perfect fit for a relational database."}	0	\N
195	195	312	Deployment	Knowledge	Deployment	What is the purpose of a 'golden image' in cloud deployments?	[{"text": "To provide a standardized, pre-configured template for creating new virtual machine instances.", "isCorrect": true}, {"text": "To store the final, production-ready version of the application code.", "isCorrect": false}, {"text": "To serve as the primary backup for an entire cloud environment.", "isCorrect": false}, {"text": "To provide a graphical user interface for managing cloud resources.", "isCorrect": false}]	To provide a standardized, pre-configured template for creating new virtual machine instances.	A golden image is a template for a virtual machine (VM), virtual desktop, or server. It is created by an administrator to pre-install and pre-configure the operating system, software, and settings that are required for a specific purpose. This ensures that all new instances are created in a consistent and secure state.	{"summary": "A golden image is a pre-configured VM template.", "breakdown": ["It includes the base OS, security hardening configurations, and common software.", "It speeds up the process of deploying new instances.", "It ensures consistency and reduces configuration drift across the fleet of servers."], "otherOptions": "This is a deployable artifact, which is different from a VM image.\\nBackups are separate from deployment templates.\\nThis describes a cloud management console."}	0	\N
197	197	314	Troubleshooting	Comprehension	Troubleshooting	An application is experiencing intermittent errors. The administrator suspects the issue is caused by a recent change but is unsure which change is the culprit. Which of the following is the BEST first step in troubleshooting?	[{"text": "Immediately roll back all changes made in the last 24 hours.", "isCorrect": false}, {"text": "Review change logs and deployment records to correlate the start of the errors with a specific change.", "isCorrect": true}, {"text": "Increase the server capacity to see if the errors go away.", "isCorrect": false}, {"text": "Ask users to clear their browser cache.", "isCorrect": false}]	Review change logs and deployment records to correlate the start of the errors with a specific change.	When troubleshooting, it's crucial to be systematic. The most likely cause of a new problem is a recent change. Before taking any action, the administrator should investigate change management logs, version control history, and deployment records to find a correlation between a specific change and when the errors began.	{"summary": "Correlate the problem with recent changes.", "breakdown": ["Change is the most common cause of failure in IT systems.", "A systematic review of logs and records provides evidence for a theory of probable cause.", "This avoids guessing and potentially making the problem worse by taking random actions."], "otherOptions": "Rolling back all changes at once is a drastic step and may not be necessary. It's better to identify the specific problematic change first.\\nThis is a random action that doesn't address the likely root cause.\\nThis is unlikely to be the cause of server-side application errors."}	0	\N
198	198	315	Cloud Concepts	Application	Operations and Support	A start-up company wants to minimize its upfront IT costs and adopt a pay-as-you-go pricing model. They need the ability to scale their services quickly as their user base grows. Which of the following models is most suitable for them?	[{"text": "A traditional on-premises data center.", "isCorrect": false}, {"text": "A co-location facility.", "isCorrect": false}, {"text": "A public cloud service.", "isCorrect": true}, {"text": "A private cloud.", "isCorrect": false}]	A public cloud service.	Public cloud services are designed for this exact scenario. They require no upfront capital expenditure (CapEx), operate on a pay-as-you-go operational expenditure (OpEx) model, and offer rapid elasticity, allowing the company to scale its resources on-demand to match its growth.	{"summary": "Public cloud is ideal for startups needing agility and low upfront cost.", "breakdown": ["No CapEx: No need to buy expensive hardware.", "Pay-as-you-go (OpEx): Aligns cost directly with usage.", "Scalability & Elasticity: Resources can be scaled up or down in minutes.", "This allows the startup to focus its capital on its core business, not on IT infrastructure."], "otherOptions": "A, B, On-premises, co-location, and private cloud all require significant upfront capital investment and do not offer the same level of elasticity as the public cloud."}	0	\N
199	199	316	High Availability	Knowledge	Cloud Architecture and Design	Which of the following describes the ability of a system to continue functioning even if one of its components fails?	[{"text": "Scalability", "isCorrect": false}, {"text": "Elasticity", "isCorrect": false}, {"text": "Fault tolerance", "isCorrect": true}, {"text": "Agility", "isCorrect": false}]	Fault tolerance	Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of some of its components. This is typically achieved through redundancy, such as running multiple web servers, database replicas, or deploying across multiple data centers.	{"summary": "This is the definition of fault tolerance.", "breakdown": ["It is a key principle for building highly available and reliable systems.", "It is achieved by eliminating single points of failure.", "Examples include load balancers, database clustering, and multi-AZ deployments."], "otherOptions": "Scalability is the ability to handle increased load.\\nElasticity is the ability to automatically scale resources.\\nAgility is the ability to develop and deploy applications quickly."}	0	\N
200	200	317	Networking	Knowledge	Cloud Architecture and Design	You are designing a virtual network for your company. You need to divide the network into smaller, isolated segments to group related resources and apply specific security policies. What are these network segments called?	[{"text": "Regions", "isCorrect": false}, {"text": "Availability Zones", "isCorrect": false}, {"text": "Subnets", "isCorrect": true}, {"text": "Security Groups", "isCorrect": false}]	Subnets	A subnet (subnetwork) is a logical subdivision of an IP network. In a cloud VPC, you divide your network into subnets to isolate resources. For example, you typically place public-facing web servers in a public subnet and backend database servers in a private subnet with stricter security.	{"summary": "These network segments are called subnets.", "breakdown": ["Each subnet has its own CIDR block range, which is a subset of the VPC's CIDR block.", "You can associate route tables and Network ACLs with subnets to control traffic flow.", "They are a fundamental tool for network organization and security."], "otherOptions": "A, Regions and Availability Zones are physical infrastructure constructs, not logical network segments.\\nA security group is a firewall, not a network segment."}	0	\N
201	201	318	Databases	Knowledge	Cloud Architecture and Design	Which of the following database types uses a flexible, document-based data model (often using JSON-like documents) and is well-suited for applications that require a flexible schema?	[{"text": "Relational (SQL)", "isCorrect": false}, {"text": "NoSQL Document Database", "isCorrect": true}, {"text": "In-memory", "isCorrect": false}, {"text": "Data Warehouse", "isCorrect": false}]	NoSQL Document Database	Document databases are a type of NoSQL database that store data in documents, which are typically in a JSON, BSON, or XML format. This model allows for flexible schemas, meaning you don't have to define all the columns upfront, which is great for agile development and evolving applications.	{"summary": "This describes a NoSQL Document Database.", "breakdown": ["Data is stored in flexible, JSON-like documents.", "It does not require a fixed, predefined schema.", "It is horizontally scalable and good for a wide variety of modern applications.", "Popular examples include MongoDB and Amazon DynamoDB."], "otherOptions": "Relational databases use a rigid, predefined schema of tables and columns.\\nIn-memory describes where the data is stored (RAM), not the data model itself.\\nA data warehouse is a specialized database optimized for analytics, not for transactional applications."}	0	\N
202	202	319	Security	Application	Security	You need to give a third-party auditing firm read-only access to your cloud environment for a limited period of time. What is the MOST secure way to grant this access?	[{"text": "Create a new IAM user with a permanent password and add it to the 'administrators' group.", "isCorrect": false}, {"text": "Create a cross-account IAM role with a read-only permissions policy and an external ID. Grant the auditor's account permission to assume this role.", "isCorrect": true}, {"text": "Share the access key and secret key of an existing administrative user with the auditing firm.", "isCorrect": false}, {"text": "Create a new IAM user with read-only permissions and email the password to the auditing firm.", "isCorrect": false}]	Create a cross-account IAM role with a read-only permissions policy and an external ID. Grant the auditor's account permission to assume this role.	Using a cross-account IAM role is the standard, most secure method for granting third-party access. It provides temporary, limited credentials and avoids the need to create and manage permanent users or share long-lived keys. The external ID adds another layer of security to prevent the "confused deputy" problem.	{"summary": "A cross-account IAM role is the best practice for third-party access.", "breakdown": ["It provides temporary credentials, not permanent ones.", "Permissions are strictly defined in the role's policy (e.g., read-only).", "Access can be easily revoked by deleting the role or changing its trust policy.", "The external ID ensures that only the intended third party can assume the role."], "otherOptions": "A, C, Creating permanent users or sharing existing credentials are all insecure practices that violate the principle of least privilege and create unnecessary risk."}	0	\N
203	203	320	Automation	Comprehension	Deployment	Which two of the following are primary benefits of using Infrastructure as Code (IaC)? (Choose TWO)	[{"text": "It allows for consistent and repeatable environment creation.", "isCorrect": true}, {"text": "It eliminates the need for network security.", "isCorrect": false}, {"text": "It provides a version-controlled, auditable history of infrastructure changes.", "isCorrect": true}, {"text": "It reduces the cost of cloud computing by 50% or more.", "isCorrect": false}, {"text": "It removes the need for application developers to write code.", "isCorrect": false}]	A, C	The core benefits of IaC are consistency and auditability. By defining infrastructure in code, you can create identical environments every time, eliminating configuration drift. Storing this code in a version control system like Git gives you a complete history of every change made to your infrastructure.	{"summary": "IaC provides repeatability and version control for infrastructure.", "breakdown": ["Repeatability: Eliminates manual, error-prone setup processes.", "Version Control: You can see who changed what and when, and easily roll back to previous versions.", "This leads to more stable environments and faster recovery from errors."], "otherOptions": "IaC is used to *define* network security rules, not eliminate them.\\nIaC can help with cost management but does not guarantee a specific percentage of savings.\\nIaC is for infrastructure, not application code."}	1	{A,C}
204	204	321	Operations	Application	Operations and Support	An application is deployed in the cloud and has a Service Level Agreement (SLof 99.95% uptime. The operations team needs to be notified immediately if the application becomes unavailable. What should they configure?	[{"text": "A billing alarm that triggers when costs exceed the budget.", "isCorrect": false}, {"text": "An automated backup job that runs every hour.", "isCorrect": false}, {"text": "A monitoring system with a health check or uptime probe that sends an alert when it fails.", "isCorrect": true}, {"text": "A CI/CD pipeline to deploy new updates automatically.", "isCorrect": false}]	A monitoring system with a health check or uptime probe that sends an alert when it fails.	To meet an SLA, you must continuously monitor the availability of the application. An uptime probe or health check is an automated service that sends requests to your application endpoint at regular intervals. If the check fails for a specified period, it triggers an alert (e.g., email, SMS, PagerDuty) to notify the operations team.	{"summary": "Uptime monitoring and alerting are required to enforce SLAs.", "breakdown": ["The monitor constantly checks the application's availability.", "If the application goes down, an alert is triggered immediately.", "This allows the operations team to respond quickly to minimize downtime and meet the SLA."], "otherOptions": "A billing alarm is for cost management, not availability.\\nBackups are for data recovery, not for monitoring uptime.\\nA CI/CD pipeline is for deployments, not for monitoring."}	0	\N
205	205	322	Troubleshooting	Comprehension	Troubleshooting	A user is unable to log in to a web application. They are certain they are using the correct password. Which of the following is the MOST likely cause from a cloud infrastructure perspective?	[{"text": "The database server is offline.", "isCorrect": true}, {"text": "The web server has run out of disk space.", "isCorrect": false}, {"text": "The DNS records for the application are incorrect.", "isCorrect": false}, {"text": "The load balancer has failed its health check.", "isCorrect": false}]	The database server is offline.	Authentication almost always involves checking the user's credentials against a database. If the web server can't connect to the database, it can't verify the password, and the login will fail, even if the user is entering the correct credentials.	{"summary": "Authentication failure often points to a database connectivity issue.", "breakdown": ["The web server needs to communicate with the database to authenticate users.", "If the database is down or unreachable, the login process cannot be completed.", "The user-facing error might be a generic 'login failed' message."], "otherOptions": "Running out of disk space would likely cause a different error, or a total site failure.\\nIncorrect DNS records would prevent the user from reaching the application's login page in the first place.\\nIf the load balancer failed its health check on the web server, the user wouldn't even be able to load the login page."}	0	\N
206	206	324	Networking	Knowledge	Cloud Architecture and Design	What is the purpose of a subnet mask in IP networking?	[{"text": "To hide the IP address of a server from the public internet.", "isCorrect": false}, {"text": "To divide an IP network into two or more smaller networks (subnets).", "isCorrect": true}, {"text": "To encrypt the traffic flowing between two IP addresses.", "isCorrect": false}, {"text": "To assign a permanent IP address to a device.", "isCorrect": false}]	To divide an IP network into two or more smaller networks (subnets).	A subnet mask is used to determine which part of an IP address is the network portion and which part is the host portion. By using different subnet masks, a single large network can be divided into smaller, more manageable subnets.	{"summary": "A subnet mask divides a network into subnets.", "breakdown": ["It is a 32-bit number that masks an IP address and divides the IP address into network address and host address.", "It is used in conjunction with the IP address to determine the network and host IDs.", "For example, the subnet mask 255.255.255.0 divides the network at the last octet."], "otherOptions": "This is done by a NAT Gateway or by using private IP addresses.\\nThis is done using encryption protocols like TLS or IPsec.\\nThis is done via static IP assignment, not a subnet mask."}	0	\N
207	207	325	Security	Comprehension	Security	Which of the following BEST describes a "zero-day" vulnerability?	[{"text": "A vulnerability that is discovered and exploited by attackers before the software vendor is aware of it or has released a patch.", "isCorrect": true}, {"text": "A vulnerability that takes zero days to fix once it has been discovered.", "isCorrect": false}, {"text": "A type of vulnerability that only affects cloud services on their first day of release.", "isCorrect": false}, {"text": "A security audit that finds zero vulnerabilities in a system.", "isCorrect": false}]	A vulnerability that is discovered and exploited by attackers before the software vendor is aware of it or has released a patch.	A zero-day vulnerability is a security flaw that is known to attackers but not yet known to the vendor or the public. This means there is no patch available, making these vulnerabilities particularly dangerous as there is no immediate defense against an attack.	{"summary": "A zero-day is a vulnerability without a patch.", "breakdown": ["The 'zero-day' refers to the fact that the vendor has had zero days to create a fix.", "Attackers who discover these can sell them or use them in highly targeted attacks.", "Defense against zero-day attacks often relies on behavioral threat detection and intrusion prevention systems."], "otherOptions": "B, C, These are incorrect descriptions of the term."}	0	\N
208	208	326	Deployment	Comprehension	Cloud Architecture and Design	When migrating a workload to the cloud, the team decides to make a few optimizations to the application to take advantage of a managed database service, but they are not fully rebuilding the application. What is this migration strategy called?	[{"text": "Rehost (Lift and Shift)", "isCorrect": false}, {"text": "Replatform (Lift and Reshape)", "isCorrect": true}, {"text": "Rearchitect", "isCorrect": false}, {"text": "Retire", "isCorrect": false}]	Replatform (Lift and Reshape)	Replatforming is a migration strategy that involves making some modifications to an application to better leverage cloud capabilities, without changing the core architecture. Moving from a self-managed database to a managed database service (like Amazon RDS) is a classic example of replatforming.	{"summary": "This strategy is known as replatforming.", "breakdown": ["It is a middle ground between a simple lift-and-shift and a full re-architecting.", "It provides tangible benefits (like reduced operational overhead) with moderate effort.", "It allows you to start optimizing for the cloud without a major rewrite."], "otherOptions": "Rehosting would involve moving the database to a VM without changing to a managed service.\\nRearchitecting would involve a major rewrite of the application, for example, to a microservices architecture.\\nRetire means to decommission the application."}	0	\N
209	209	327	Operations - Log Management	Application	Operations and Support	A company deploys 150 Linux-based servers in the cloud. The project team is tasked with storing system logs in the cloud. Logs older than 180 days should be archived automatically. Which techniques should the project team use to create the optimal solution? (Select TWO)	[{"text": "Implement lifecycle policies to automatically move logs to cold storage after 180 days", "isCorrect": true}, {"text": "Configure centralized logging with automated log rotation and compression", "isCorrect": true}, {"text": "Store all logs in high-performance SSD storage for faster access", "isCorrect": false}, {"text": "Manually transfer logs to archive storage every 6 months", "isCorrect": false}, {"text": "Delete all logs after 180 days to save storage costs", "isCorrect": false}]	Implement lifecycle policies to automatically move logs to cold storage after 180 days, Configure centralized logging with automated log rotation and compression	The optimal solution requires lifecycle policies to automatically transition logs to cost-effective cold storage after 180 days, and centralized logging with automated rotation and compression to efficiently manage the high volume of logs from 150 servers while optimizing storage costs.	{"summary": "Optimal cloud log management strategy for large-scale deployments:", "breakdown": ["Lifecycle policies automate storage tier transitions based on age (hot  warm  cold)", "Cold storage provides cost-effective archiving for compliance and historical analysis", "Centralized logging aggregates logs from all 150 servers for unified management", "Automated log rotation prevents storage overflow and manages disk space", "Compression reduces storage requirements and associated costs", "Automated processes eliminate manual intervention and human error"], "otherOptions": "SSD storage is expensive for long-term archival of old logs\\\\nManual processes do not scale for 150 servers and introduce operational risk\\\\nDeleting logs may violate compliance requirements and eliminate valuable troubleshooting data"}	1	{"Implement lifecycle policies to automatically move logs to cold storage after 180 days","Configure centralized logging with automated log rotation and compression"}
210	210	328	Deployments -	Application	Cloud Deployment	A cloud engineer has deployed an ecommerce system consisting of multiple components. To reduce latency for customers, the engineer plans to deploy additional compute capacity on web front-ends. However, horizontal scaling options are too expensive. Which solution BEST meets the requirements?	[{"text": "Deploy additional VCPU resources on front-end VMs.", "isCorrect": true}, {"text": "Convert VM hosts from type 1 to type 2 hypervisor.", "isCorrect": false}, {"text": "Place the front-end VMs in an auto-scaling group.", "isCorrect": false}, {"text": "Deploy additional front-end VMs with faster processors.", "isCorrect": false}]	Deploy additional VCPU resources on front-end VMs.	The engineer should deploy additional virtualized CPU (vCPU) resources on front-end virtual machines (VMs).  A vCPU is a compute resource presented by a hypervisor to a guest OS.\n\n In this case, adding more vCPU capacity to the front-end VMs should reduce processing latency. Expanding the resources on a single node is known as vertical scaling.	{"summary": "Deploy additional VCPU resources on front-end VMs.", "breakdown": ["The engineer should not deploy additional front-end VMs with faster processors. Like auto-scaling, this is a horizontal scaling approach.", "The engineer should not convert VM hosts from type 1 to type 2 hypervisors. A hypervisor is hardware, software, and/or firmware that sits  between VMs and physical hardware and a type 1 hypervisor runs directly on hardware. A type 2 hypervisor runs on a full operating system such  as Linux or Microsoft Windows.", "The engineer should not place the front-end VMs in an auto-scaling group. This would increase performance and reduce latency. However, this  violates the requirement to not use horizontal scaling options."], "otherOptions": "presented by a hypervisor to a guest OS. In this case, adding more vCPU capacity to the front-end VMs should reduce processing latency.\\n\\nExpanding the resources on a single node is known as vertical scaling.\\n\\nThe engineer should not convert VM hosts from type 1 to type 2 hypervisors. A hypervisor is hardware, software, and/or firmware that sits\\n\\nbetween VMs and physical hardware and a type 1 hypervisor runs directly on hardware. A type 2 hypervisor runs on a full operating system such\\n\\nas Linux or Microsoft Windows.\\n\\nThe engineer should not place the front-end VMs in an auto-scaling group. This would increase performance and reduce latency. However, this\\n\\nviolates the requirement to not use horizontal scaling options.\\n\\nThe engineer should not deploy additional front-end VMs with faster processors. Like auto-scaling, this is a horizontal scaling approach."}	\N	\N
211	211	190	Performance - Database Query Optimization	Expert	Operations and Support	A data warehouse runs complex analytical queries that take 30+ minutes to complete. The queries scan 100TB+ datasets but only return aggregated results. Users need interactive query performance (<5 seconds). Which solution provides the BEST query acceleration?	[{"text": "Amazon Redshift with materialized views and result caching", "isCorrect": true}, {"text": "Athena with partitioned data and columnar formats", "isCorrect": false}, {"text": "Aurora with query plan caching and read replicas", "isCorrect": false}, {"text": "DynamoDB with pre-computed aggregation tables", "isCorrect": false}]	Amazon Redshift with materialized views and result caching	Redshift materialized views pre-compute complex aggregations, result caching returns identical queries instantly, and columnar storage with compression optimizes scan performance.	{"summary": "Data warehouse query acceleration:", "breakdown": ["Materialized views: Pre-computed aggregations refresh automatically", "Result caching: Identical queries return in milliseconds", "Columnar storage: Optimized for analytical query patterns", "Redshift Spectrum: Can query 100TB+ datasets efficiently"], "otherOptions": "Athena still requires scanning large datasets for complex aggregations\\nAurora optimized for OLTP, not analytical workloads\\nDynamoDB requires complete data model restructuring"}	\N	\N
1	1	3	Cloud Architecture - Service Models	Updated Knowledge Level	Cloud Architecture and Models	Which cloud service model provides virtualized computing resources over the internet, allowing users to rent servers, storage, and networking without purchasing physical hardware?	[{"text": "Infrastructure as a Service (IaaS)", "isCorrect": true}, {"text": "Platform as a Service (PaaS)", "isCorrect": false}, {"text": "Function as a Service (FaaS)", "isCorrect": false}, {"text": "Software as a Service (SaaS)", "isCorrect": false}]	Infrastructure as a Service (IaaS)	Updated explanation for testing purposes	{"summary": "IaaS characteristics include:", "breakdown": ["Virtualized computing resources over the internet", "Users rent infrastructure components rather than buying hardware", "Control over operating systems and applications", "Examples: AWS EC2, Azure VMs, Google Compute Engine"], "otherOptions": "SaaS delivers complete software applications\\nPaaS provides development platforms\\nFaaS provides serverless function execution"}	\N	\N
212	212	193	Cost Optimization - Serverless vs Container Economics	Expert	Cloud Architecture and Models	A microservices platform runs 200 services with varying traffic patterns: 50 services get constant low traffic, 100 services have predictable business-hour spikes, 50 services have unpredictable traffic. Current container costs are $300,000/month. Which architecture mix optimizes costs?	[{"text": "All services migrate to Lambda for serverless benefits", "isCorrect": false}, {"text": "Fargate for all services with auto-scaling enabled", "isCorrect": false}, {"text": "Lambda for unpredictable traffic, Fargate Spot for spiky services, ECS EC2 Reserved for constant traffic", "isCorrect": true}, {"text": "EKS with cluster auto-scaling for all services", "isCorrect": false}]	Lambda for unpredictable traffic, Fargate Spot for spiky services, ECS EC2 Reserved for constant traffic	This hybrid approach optimizes for traffic patterns: Lambda eliminates idle costs for unpredictable traffic, Fargate Spot reduces costs for spiky workloads, Reserved instances provide maximum savings for constant traffic.	{"summary": "Hybrid architecture cost optimization:", "breakdown": ["Lambda (50 services): Pay-per-request eliminates idle time costs", "Fargate Spot (100 services): 70% savings for fault-tolerant spiky workloads", "ECS Reserved (50 services): 70% savings for predictable constant traffic", "Estimated total savings: 60% reduction from current $300,000"], "otherOptions": "Lambda cold starts and execution time limits problematic for all services\\nFargate On-Demand expensive for constant traffic\\nEKS has control plane costs and doesn't optimize for traffic patterns"}	\N	\N
213	213	329	Security - Roles	Knowledge	Security	A cloud admin is having trouble managing permissions that have been assigned to individual users. The\nadmin needs to implement an access control model that allows users to be grouped and permissions\nassigned based on job type. Which solution BEST addresses this requirement?	[{"text": "Assign labels", "isCorrect": false}, {"text": "Set attributes", "isCorrect": false}, {"text": "Configure DAC", "isCorrect": false}, {"text": "Deploy RBAC", "isCorrect": true}]	Deploy RBAC	The cloud admin should deploy Role Based Access Control (RBAC). RBAC is designed to enhance security\nby streamlining the assignment of permissions and system privileges to users. Roles are typically defined\nbased on job descriptions and are then assigned to users with that job. For example, the Accountant role\ncould be created and then granted privileges to files and applications that accountants in an organization\nneed to access. The Accountant role can then be assigned to users in the Accounting department.	{"summary": "The cloud admin should deploy Role Based Access Control (RBAC).", "breakdown": ["The admin should not assign labels. This relates to Mandatory Access Control (MAC). MAC is considered the most secure access control method and is primarily used in government systems. The basis of the MAC model is the application of security labels, which are applied to all system resources.", "The admin should not configure Discretionary Access Control (DAC). In a DAC model, file owners or those with similar privileges can grant access to other groups or users ", "The admin should not set attributes. This describes Attribute-Based Access Control (ABAC). ABAC permits access based on a set of attributes, such as a user's name, the time of day, or a file's name."], "otherOptions": ""}	\N	\N
214	214	331	Cloud Architecture and Design	Knowledge	Cloud Architecture and Models	An organization has deployed an app using VMs it manages in the cloud. To extract maximum performance\nwhile minimizing costs, the organization wants to use parallel computations per core when possible. Which\nsolution BEST meets the requirements?	[{"text": "Enable the simultaneous multithreading functionality on guest VMs", "isCorrect": true}, {"text": "Add the VMs to an auto-scaling group", "isCorrect": false}, {"text": "Assign two or more vCPUs to each VM", "isCorrect": false}, {"text": "Migrate to a bare metal deployment", "isCorrect": false}]	Enable the simultaneous multithreading functionality on guest VMs	The organization should enable simultaneous multi-threading (SMT) on each guest virtual machine (VM).\nSMT is a Central Processing Unit (CPU) feature that allows a single core to perform parallel computations.\nThis should result in increased performance without requiring additional CPU cycles. SMT must be\nsupported by the CPU, the hypervisor, and the guest VAs.	{"summary": "The organization should enable simultaneous multi-threading (SMT) on each guest virtual machine (VM).", "breakdown": ["A virtualized CPU (vCPU) is a compute resource presented by a hypervisor to a guest OS. Increasing vCPU count will not facilitate parallel computations per core.", "Bare metal deployments run virtualization software directly on hardware by providing their own operating system. This alone does not facilitate parallel computations on a single core.", "An auto-scaling group is designed to provision and decommission VMs automatically, based on workload thresholds. This alone does not facilitate parallel computations on a single core.", "SMT is a Central Processing Unit (CPU) feature that allows a single core to perform parallel computations. This should result in increased performance without requiring additional CPU cycles. SMT must be supported by the CPU, the hypervisor, and the guest VAs."], "otherOptions": "A virtualized CPU (vCPU) is a compute resource presented by a hypervisor to a guest OS. Increasing vCPU count will not facilitate parallel computations per core.\\n\\nBare metal deployments run virtualization software directly on hardware by providing their own operating\\nsystem. This alone does not facilitate parallel computations on a single core.\\n\\nAn auto-scaling group is designed to provision and decommission VMs automatically, based on workload\\nthresholds. This alone does not facilitate parallel computations on a single core."}	\N	\N
215	215	333	Operations - Monitoring	Analysis	Operations and Support	A cloud admin must ensure that traffic can be captured, and session statistics can be analyzed and stored\nover time. Additionally, the admin must use the information to identify performance anomalies. Which\nsolution is the admin MOST LIKELY to implement?	[{"text": "Configure nodes to forward data to a network flow collector", "isCorrect": true}, {"text": "Place instances in a security group and configure traffic rules", "isCorrect": false}, {"text": "Deploy SNMP management and install agents on each node", "isCorrect": false}, {"text": "Configure a packet analyzer and perform a packet capture", "isCorrect": false}]	Configure nodes to forward data to a network flow collector	Network flows can be captured using a network flow connector. Once captured, the flows can be analyzed\nto identify traffic trends. In most implementations, network devices are configured with the Internet Protocol\n(IP) address of a flow collector - a dedicated system that collects network flow data. The collector may have\nadvanced analytical, reporting, and alerting functionality.	{"summary": "Network flows can be captured using a network flow connector.", "breakdown": ["Network flows can be captured using a network flow connector. Once captured, the flows can be analyzed to identify traffic trends.", "Network devices are configured with the Internet Protocol (IP) address of a flow collector which is a dedicated system that collects network flow data.", "The collector may have advanced analytical, reporting, and alerting functionality."], "otherOptions": "Simple Network Management Protocol (SNMP) is used to collect performance and event information from\\nnetwork devices and modify device configurations. However, SNMP is not used to capture traffic or perform\\ndetailed session analysis.\\n\\nA packet analyzer is used to capture and view the contents of network packets. While a packet analyzer\\ncould be used in this case, it is not designed to capture and store information over time."}	\N	\N
69	69	69	Cloud Identity Management	Application	Security	A company acquires three subsidiaries, each with different identity providers (AD, Google Workspace, Okta). They need unified cloud access for 5,000 total users while maintaining each subsidiary's existing identity system. Compliance requires MFA and privileged access management. Which identity architecture best meets these requirements?	[{"text": "Implement federated identity with SAML/OIDC, centralized MFA, and PAM solution", "isCorrect": true}, {"text": "Create cloud accounts for each subsidiary with separate identity systems", "isCorrect": false}, {"text": "Sync all identities to cloud provider's native directory service", "isCorrect": false}, {"text": "Migrate all users to a single corporate identity provider", "isCorrect": false}]	Implement federated identity with SAML/OIDC, centralized MFA, and PAM solution	Federation allows each subsidiary to maintain their identity provider while SAML/OIDC provides secure cloud access. Centralized MFA and PAM ensure consistent security controls.	{"summary": "Federated identity architecture benefits:", "breakdown": ["SAML/OIDC federation preserves existing identity investments", "Users maintain single credentials (reduced password fatigue)", "Centralized MFA policy applies regardless of source IdP", "PAM solution provides consistent privileged access controls"], "otherOptions": "Migration disrupts 5,000 users and requires retraining\\nSeparate accounts prevent unified access and compliance\\nSync creates password management and security challenges"}	\N	\N
130	130	147	Security	Comprehension	Security	Which of the following are valid authentication factors used in Multi-Factor Authentication (MFA)? (Choose THREE)	[{"text": "Something you know (e.g., a password)", "isCorrect": true}, {"text": "Something you have (e.g., a hardware token or mobile phone)", "isCorrect": true}, {"text": "Something you are (e.g., a fingerprint or face scan)", "isCorrect": true}, {"text": "Somewhere you are (e.g., a specific IP address)", "isCorrect": false}, {"text": "Something you do (e.g., a specific keystroke pattern)", "isCorrect": false}, {"text": "Something you create (e.g., a new user account)", "isCorrect": false}]	Something you know (e.g., a password), Something you have (e.g., a hardware token or mobile phone), Something you are (e.g., a fingerprint or face scan)	Multi-Factor Authentication is a security system that requires more than one method of authentication from independent categories of credentials to verify the user's identity for a login or other transaction. The three standard categories are knowledge, possession, and inherence.	{"summary": "MFA is based on knowledge, possession, and inherence.", "breakdown": ["Something you know: Password, PIN, security question answer.", "Something you have: Mobile phone (for SMS or authenticator app), USB security key, smart card.", "Something you are: Biometrics like a fingerprint, facial recognition, or iris scan."], "otherOptions": "Location and behavioral patterns are sometimes used as signals in adaptive authentication but are not considered one of the three core MFA factors.\\n\\nCreating an account is not an authentication factor."}	1	{"Something you know (e.g., a password)","Something you have (e.g., a hardware token or mobile phone)","Something you are (e.g., a fingerprint or face scan)"}
\.


--
-- TOC entry 5033 (class 0 OID 0)
-- Dependencies: 227
-- Name: aws_certified_architect_associate_questions_id_seq; Type: SEQUENCE SET; Schema: prepper; Owner: ericbo
--

SELECT pg_catalog.setval('prepper.aws_certified_architect_associate_questions_id_seq', 104, true);


--
-- TOC entry 5034 (class 0 OID 0)
-- Dependencies: 225
-- Name: aws_question_id_seq; Type: SEQUENCE SET; Schema: prepper; Owner: ericbo
--

SELECT pg_catalog.setval('prepper.aws_question_id_seq', 205, true);


--
-- TOC entry 5035 (class 0 OID 0)
-- Dependencies: 226
-- Name: aws_question_number_seq; Type: SEQUENCE SET; Schema: prepper; Owner: ericbo
--

SELECT pg_catalog.setval('prepper.aws_question_number_seq', 105, true);


--
-- TOC entry 5036 (class 0 OID 0)
-- Dependencies: 220
-- Name: id_seq; Type: SEQUENCE SET; Schema: prepper; Owner: ericbo
--

SELECT pg_catalog.setval('prepper.id_seq', 249, true);


--
-- TOC entry 5037 (class 0 OID 0)
-- Dependencies: 222
-- Name: question_id_seq; Type: SEQUENCE SET; Schema: prepper; Owner: ericbo
--

SELECT pg_catalog.setval('prepper.question_id_seq', 239, true);


--
-- TOC entry 5038 (class 0 OID 0)
-- Dependencies: 223
-- Name: question_number_seq; Type: SEQUENCE SET; Schema: prepper; Owner: postgres
--

SELECT pg_catalog.setval('prepper.question_number_seq', 235, true);


--
-- TOC entry 4870 (class 2606 OID 24637)
-- Name: aws_certified_architect_associate_questions aws_certified_architect_associate_questions_pkey; Type: CONSTRAINT; Schema: prepper; Owner: ericbo
--

ALTER TABLE ONLY prepper.aws_certified_architect_associate_questions
    ADD CONSTRAINT aws_certified_architect_associate_questions_pkey PRIMARY KEY (id);


--
-- TOC entry 4872 (class 2606 OID 24639)
-- Name: aws_certified_architect_associate_questions aws_certified_architect_associate_questions_question_id_key; Type: CONSTRAINT; Schema: prepper; Owner: ericbo
--

ALTER TABLE ONLY prepper.aws_certified_architect_associate_questions
    ADD CONSTRAINT aws_certified_architect_associate_questions_question_id_key UNIQUE (question_id);


--
-- TOC entry 4867 (class 2606 OID 24603)
-- Name: comptia_cloud_plus_questions comptia_cloud_plus_questions_pkey; Type: CONSTRAINT; Schema: prepper; Owner: ericbo
--

ALTER TABLE ONLY prepper.comptia_cloud_plus_questions
    ADD CONSTRAINT comptia_cloud_plus_questions_pkey PRIMARY KEY (id);


--
-- TOC entry 4868 (class 1259 OID 24604)
-- Name: question_id; Type: INDEX; Schema: prepper; Owner: ericbo
--

CREATE INDEX question_id ON prepper.comptia_cloud_plus_questions USING btree (question_id) INCLUDE (question_id) WITH (deduplicate_items='true');


-- Completed on 2025-10-28 05:44:41

--
-- PostgreSQL database dump complete
--

\unrestrict xcjnimaxyjShnqYYGRuKudhxf3MbLq7fJYfi9a2FWqfQ66mSAMQhYndvdWmpHtD

-- Completed on 2025-10-28 05:44:41

--
-- PostgreSQL database cluster dump complete
--

